src.rag_toolkit.core.llm
========================

.. py:module:: src.rag_toolkit.core.llm

.. autoapi-nested-parse::

   LLM clients - Core abstractions only (Protocol).



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/src/rag_toolkit/core/llm/base/index


Classes
-------

.. autoapisummary::

   src.rag_toolkit.core.llm.LLMClient


Package Contents
----------------

.. py:class:: LLMClient

   Bases: :py:obj:`Protocol`


   Protocol for Large Language Model clients.

   Uses Protocol instead of ABC for more flexible duck typing.
   Any class implementing these methods can be used as an LLMClient.


   .. py:method:: generate(prompt, **kwargs)

      Generate a completion from a prompt.

      :param prompt: The input prompt
      :param \*\*kwargs: Additional parameters (temperature, max_tokens, etc.)

      :returns: Generated text completion



   .. py:method:: generate_batch(prompts, **kwargs)

      Optional batch generation.

      Default implementation iterates generate() for each prompt.
      Implementations can override for batch optimization.

      :param prompts: Iterable of input prompts
      :param \*\*kwargs: Additional parameters

      :Yields: Generated text completions



   .. py:property:: model_name
      :type: str


      Return the underlying model name.


