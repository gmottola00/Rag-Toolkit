src.rag_toolkit.infra.llm.ollama
================================

.. py:module:: src.rag_toolkit.infra.llm.ollama

.. autoapi-nested-parse::

   Ollama-based LLM client.



Classes
-------

.. autoapisummary::

   src.rag_toolkit.infra.llm.ollama.OllamaLLMClient


Module Contents
---------------

.. py:class:: OllamaLLMClient(*, model = DEFAULT_OLLAMA_MODEL, base_url = DEFAULT_OLLAMA_URL, timeout = 120)

   Bases: :py:obj:`rag_toolkit.core.llm.base.LLMClient`


   Client for Ollama's /api/generate endpoint.


   .. py:attribute:: base_url


   .. py:attribute:: timeout
      :value: 120



   .. py:property:: model_name
      :type: str


      Return the underlying model name.


   .. py:method:: generate(prompt, **kwargs)

      Generate a completion from a prompt.

      :param prompt: The input prompt
      :param \*\*kwargs: Additional parameters (temperature, max_tokens, etc.)

      :returns: Generated text completion



   .. py:method:: generate_batch(prompts, **kwargs)

      Optional batch generation.

      Default implementation iterates generate() for each prompt.
      Implementations can override for batch optimization.

      :param prompts: Iterable of input prompts
      :param \*\*kwargs: Additional parameters

      :Yields: Generated text completions



