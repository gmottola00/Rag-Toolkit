<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html">

    <!-- Generated with Sphinx 7.4.7 and Furo 2025.12.19 -->
        <title>:material-vector-polyline: Embeddings - rag-toolkit</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=d577e810" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">rag-toolkit</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">rag-toolkit</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  
</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/gmottola00/rag-toolkit/blob/main/docs/guides/embeddings.md?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/gmottola00/rag-toolkit/edit/main/docs/guides/embeddings.md" rel="edit" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="material-vector-polyline-embeddings">
<h1>:material-vector-polyline: Embeddings<a class="headerlink" href="#material-vector-polyline-embeddings" title="Link to this heading">¶</a></h1>
<p>Embeddings are the cornerstone of semantic search and RAG systems. Master embeddings to build powerful, accurate retrieval systems.</p>
<hr class="docutils" />
<section id="material-help-circle-what-are-embeddings">
<h2>:material-help-circle: What are Embeddings?<a class="headerlink" href="#material-help-circle-what-are-embeddings" title="Link to this heading">¶</a></h2>
<p>!!! abstract “Semantic Vector Representations”
Embeddings are <strong>dense vector representations</strong> of text that capture semantic meaning. Similar texts produce similar embeddings, enabling semantic search beyond keyword matching.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;dog&quot;</span><span class="p">)</span> <span class="err">≈</span> <span class="n">embed</span><span class="p">(</span><span class="s2">&quot;puppy&quot;</span><span class="p">)</span>       <span class="c1"># High similarity score: 0.92</span>
<span class="n">embed</span><span class="p">(</span><span class="s2">&quot;dog&quot;</span><span class="p">)</span> <span class="err">≠</span> <span class="n">embed</span><span class="p">(</span><span class="s2">&quot;computer&quot;</span><span class="p">)</span>    <span class="c1"># Low similarity score: 0.15</span>
</pre></div>
</div>
<section id="material-star-key-properties">
<h3>:material-star: Key Properties<a class="headerlink" href="#material-star-key-properties" title="Link to this heading">¶</a></h3>
<div class="grid cards" markdown>
<ul>
<li><p>:material-ruler: <strong>Dense Vectors</strong></p>
<hr class="docutils" />
<p>Typically 384-4096 dimensions</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>  <span class="c1"># 768 dimensions</span>
</pre></div>
</div>
</li>
<li><p>:material-semantic-web: <strong>Semantic Similarity</strong></p>
<hr class="docutils" />
<p>Similar meaning → similar vectors</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cosine_similarity</span><span class="p">(</span>
    <span class="n">embed</span><span class="p">(</span><span class="s2">&quot;car&quot;</span><span class="p">),</span>
    <span class="n">embed</span><span class="p">(</span><span class="s2">&quot;automobile&quot;</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1"># 0.89</span>
</pre></div>
</div>
</li>
<li><p>:material-language: <strong>Language Understanding</strong></p>
<hr class="docutils" />
<p>Captures context, synonyms, relationships</p>
<ul class="simple">
<li><p>“bank” (financial) ≠ “bank” (river)</p></li>
<li><p>Context-aware representations</p></li>
</ul>
</li>
<li><p>:material-speedometer: <strong>Efficient Search</strong></p>
<hr class="docutils" />
<p>Fast vector similarity operations</p>
<ul class="simple">
<li><p>Millions of vectors in milliseconds</p></li>
<li><p>Approximate nearest neighbor (ANN)</p></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<hr class="docutils" />
<section id="material-server-supported-embedding-providers">
<h2>:material-server: Supported Embedding Providers<a class="headerlink" href="#material-server-supported-embedding-providers" title="Link to this heading">¶</a></h2>
<p>!!! info “Choose Your Provider”</p>
<section id="material-openai-openai-recommended">
<h3>:material-openai: OpenAI (Recommended)<a class="headerlink" href="#material-openai-openai-recommended" title="Link to this heading">¶</a></h3>
<p>!!! success “State-of-the-Art Quality”
OpenAI provides industry-leading embedding models with excellent quality and speed.</p>
<p><strong>Available Models:</strong></p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Cost (per 1M tokens)</p></th>
<th class="head"><p>Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code></p></td>
<td><p>1536</p></td>
<td><p>$0.02</p></td>
<td><p>:material-speedometer: Fast, cost-effective</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code></p></td>
<td><p>3072</p></td>
<td><p>$0.13</p></td>
<td><p>:material-star: Highest quality</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code></p></td>
<td><p>1536</p></td>
<td><p>$0.10</p></td>
<td><p>:material-check: Previous gen (still good)</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Installation:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>rag-toolkit<span class="o">[</span>openai<span class="o">]</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-api-key&quot;</span>
</pre></div>
</div>
<p><strong>Usage:</strong></p>
<p>=== “Basic”
```python
from rag_toolkit.infra.embedding import OpenAIEmbedding</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Initialize
embedding = OpenAIEmbedding(
    model=&quot;text-embedding-3-small&quot;,
    api_key=&quot;your-api-key&quot;,  # Or use OPENAI_API_KEY env var
)

# Embed single text
vector = await embedding.embed(&quot;Hello world&quot;)
print(f&quot;Dimension: {len(vector)}&quot;)  # 1536
```
</pre></div>
</div>
<p>=== “Batch Processing”
<code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">Embed</span> <span class="pre">multiple</span> <span class="pre">texts</span> <span class="pre">(batched</span> <span class="pre">for</span> <span class="pre">efficiency)</span>&#160;&#160;&#160;&#160; <span class="pre">vectors</span> <span class="pre">=</span> <span class="pre">await</span> <span class="pre">embedding.embed_batch([</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">&quot;First</span> <span class="pre">document&quot;,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">&quot;Second</span> <span class="pre">document&quot;,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">&quot;Third</span> <span class="pre">document&quot;</span>&#160;&#160;&#160;&#160; <span class="pre">])</span>&#160;&#160;&#160;&#160; <span class="pre">print(f&quot;Embedded</span> <span class="pre">{len(vectors)}</span> <span class="pre">documents&quot;)</span>&#160;&#160;&#160;&#160; </code></p>
<p>=== “Advanced”
<code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160; <span class="pre">embedding</span> <span class="pre">=</span> <span class="pre">OpenAIEmbedding(</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">model=&quot;text-embedding-3-large&quot;,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">api_key=&quot;your-api-key&quot;,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">batch_size=100,</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">Process</span> <span class="pre">100</span> <span class="pre">at</span> <span class="pre">a</span> <span class="pre">time</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">timeout=30.0,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">30</span> <span class="pre">second</span> <span class="pre">timeout</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">max_retries=3,</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">Retry</span> <span class="pre">failed</span> <span class="pre">requests</span>&#160;&#160;&#160;&#160; <span class="pre">)</span>&#160;&#160;&#160;&#160; </code></p>
<p>!!! tip “Pricing (as of January 2026)”
- <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code>: <strong>$0.02 / 1M tokens</strong> — Best value
- <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code>: <strong>$0.13 / 1M tokens</strong> — Best quality
- <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code>: <strong>$0.10 / 1M tokens</strong> — Legacy</p>
</section>
<section id="material-server-security-ollama-local-free">
<h3>:material-server-security: Ollama (Local, Free)<a class="headerlink" href="#material-server-security-ollama-local-free" title="Link to this heading">¶</a></h3>
<p>!!! success “Privacy &amp; Cost-Free”
Run powerful embedding models locally with Ollama — perfect for privacy-focused deployments and zero API costs.</p>
<p><strong>Popular Models:</strong></p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Speed</p></th>
<th class="head"><p>Quality</p></th>
<th class="head"><p>Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nomic-embed-text</span></code></p></td>
<td><p>768</p></td>
<td><p>:material-speedometer: Fast</p></td>
<td><p>:material-star::material-star::material-star::material-star:</p></td>
<td><p>274MB</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mxbai-embed-large</span></code></p></td>
<td><p>1024</p></td>
<td><p>:material-speedometer-medium: Medium</p></td>
<td><p>:material-star::material-star::material-star::material-star::material-star-half:</p></td>
<td><p>669MB</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">all-minilm</span></code></p></td>
<td><p>384</p></td>
<td><p>:material-speedometer: Very fast</p></td>
<td><p>:material-star::material-star::material-star:</p></td>
<td><p>46MB</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Installation:</strong></p>
<p>=== “macOS/Linux”
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Pull embedding model
ollama pull nomic-embed-text

# Install RAG Toolkit with Ollama support
pip install rag-toolkit[ollama]
```
</pre></div>
</div>
<p>=== “Docker”
```bash
# Run Ollama in Docker
docker run -d -p 11434:11434 ollama/ollama</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Pull model
docker exec ollama ollama pull nomic-embed-text
```
</pre></div>
</div>
<p><strong>Usage:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaEmbedding</span>

<span class="c1"># Initialize</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;nomic-embed-text&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:11434&quot;</span><span class="p">,</span>  <span class="c1"># Default Ollama URL</span>
<span class="p">)</span>

<span class="c1"># Embed text</span>
<span class="n">vector</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dimension: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 768</span>

<span class="c1"># Batch embedding</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">([</span>
    <span class="s2">&quot;First document&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Second document&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Third document&quot;</span>
<span class="p">])</span>
</pre></div>
</div>
<p><strong>Model Comparison:</strong></p>
<div class="grid cards" markdown>
<ul>
<li><p>:material-star: <strong>nomic-embed-text</strong></p>
<hr class="docutils" />
<p>768 dimensions | 274MB</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>nomic-embed-text
</pre></div>
</div>
<p><strong>Best for</strong>: General purpose, balanced quality/speed</p>
</li>
<li><p>:material-star-plus: <strong>mxbai-embed-large</strong></p>
<hr class="docutils" />
<p>1024 dimensions | 669MB</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>mxbai-embed-large
</pre></div>
</div>
<p><strong>Best for</strong>: High quality requirements</p>
</li>
<li><p>:material-speedometer: <strong>all-minilm</strong></p>
<hr class="docutils" />
<p>384 dimensions | 46MB</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>all-minilm
</pre></div>
</div>
<p><strong>Best for</strong>: Speed-critical applications, low memory</p>
</li>
</ul>
</div>
<p>!!! tip “Choosing an Ollama Model”
- <strong>General use</strong>: <code class="docutils literal notranslate"><span class="pre">nomic-embed-text</span></code> — excellent balance
- <strong>High quality</strong>: <code class="docutils literal notranslate"><span class="pre">mxbai-embed-large</span></code> — best results
- <strong>Fast &amp; lightweight</strong>: <code class="docutils literal notranslate"><span class="pre">all-minilm</span></code> — minimal resources</p>
</section>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">¶</a></h2>
<section id="batch-size">
<h3>Batch Size<a class="headerlink" href="#batch-size" title="Link to this heading">¶</a></h3>
<p>Control how many texts are embedded at once:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># OpenAI (handles batching automatically)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># Default: 100</span>
<span class="p">)</span>

<span class="c1"># Ollama</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;nomic-embed-text&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># Smaller batches for local GPU</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="timeouts">
<h3>Timeouts<a class="headerlink" href="#timeouts" title="Link to this heading">¶</a></h3>
<p>Set timeouts for embedding requests:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mf">30.0</span><span class="p">,</span>  <span class="c1"># 30 seconds (default: 60)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="retry-logic">
<h3>Retry Logic<a class="headerlink" href="#retry-logic" title="Link to this heading">¶</a></h3>
<p>Handle transient failures with retries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># Default: 3</span>
    <span class="n">retry_delay</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># Seconds between retries</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Link to this heading">¶</a></h2>
<section id="dimension-reduction">
<h3>Dimension Reduction<a class="headerlink" href="#dimension-reduction" title="Link to this heading">¶</a></h3>
<p>Reduce embedding dimensions for memory efficiency (OpenAI only):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># text-embedding-3-* models support dimension reduction</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-large&quot;</span><span class="p">,</span>
    <span class="n">dimensions</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>  <span class="c1"># Reduce from 3072 to 1024</span>
<span class="p">)</span>

<span class="c1"># Maintains ~98% of quality at ~33% of dimensions</span>
</pre></div>
</div>
</section>
<section id="custom-prefixes">
<h3>Custom Prefixes<a class="headerlink" href="#custom-prefixes" title="Link to this heading">¶</a></h3>
<p>Add prefixes for retrieval vs document embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For asymmetric search (query ≠ documents)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;nomic-embed-text&quot;</span><span class="p">,</span>
    <span class="n">query_prefix</span><span class="o">=</span><span class="s2">&quot;search_query: &quot;</span><span class="p">,</span>
    <span class="n">document_prefix</span><span class="o">=</span><span class="s2">&quot;search_document: &quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Query embedding (with prefix)</span>
<span class="n">query_vector</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span>
    <span class="s2">&quot;What is machine learning?&quot;</span><span class="p">,</span>
    <span class="n">is_query</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Document embedding (with prefix)</span>
<span class="n">doc_vector</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span>
    <span class="s2">&quot;Machine learning is a subset of AI...&quot;</span><span class="p">,</span>
    <span class="n">is_query</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="normalize-embeddings">
<h3>Normalize Embeddings<a class="headerlink" href="#normalize-embeddings" title="Link to this heading">¶</a></h3>
<p>Normalize vectors for cosine similarity:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># L2 normalization (default: True)</span>
<span class="p">)</span>

<span class="c1"># With normalization: cosine similarity = dot product</span>
<span class="c1"># Without: need to compute cosine explicitly</span>
</pre></div>
</div>
</section>
</section>
<section id="batch-processing">
<h2>Batch Processing<a class="headerlink" href="#batch-processing" title="Link to this heading">¶</a></h2>
<section id="basic-batch-embedding">
<h3>Basic Batch Embedding<a class="headerlink" href="#basic-batch-embedding" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Embed multiple documents efficiently</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;First document text&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Second document text&quot;</span><span class="p">,</span>
    <span class="c1"># ... thousands more</span>
<span class="p">]</span>

<span class="c1"># Automatically batched</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="progress-tracking">
<h3>Progress Tracking<a class="headerlink" href="#progress-tracking" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># With progress bar</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">all_embeddings</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
    <span class="n">batch_embeddings</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">all_embeddings</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batch_embeddings</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="error-handling">
<h3>Error Handling<a class="headerlink" href="#error-handling" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.core.embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">EmbeddingError</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="k">except</span> <span class="n">EmbeddingError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Handle error (retry, skip, etc.)</span>
</pre></div>
</div>
</section>
</section>
<section id="model-selection-guide">
<h2>Model Selection Guide<a class="headerlink" href="#model-selection-guide" title="Link to this heading">¶</a></h2>
<section id="by-quality">
<h3>By Quality<a class="headerlink" href="#by-quality" title="Link to this heading">¶</a></h3>
<p><strong>Best Quality (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-large&quot;</span><span class="p">)</span>
<span class="c1"># 3072 dimensions, highest quality</span>
<span class="c1"># Use for: Production systems, critical applications</span>
</pre></div>
</div>
<p><strong>Balanced Quality (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>
<span class="c1"># 1536 dimensions, excellent quality/cost ratio</span>
<span class="c1"># Use for: Most applications (recommended default)</span>
</pre></div>
</div>
<p><strong>Good Quality (Ollama, Free):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;nomic-embed-text&quot;</span><span class="p">)</span>
<span class="c1"># 768 dimensions, no API costs</span>
<span class="c1"># Use for: Privacy-sensitive, development, offline</span>
</pre></div>
</div>
</section>
<section id="by-speed">
<h3>By Speed<a class="headerlink" href="#by-speed" title="Link to this heading">¶</a></h3>
<p><strong>Fastest (Ollama, Local):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;all-minilm&quot;</span><span class="p">)</span>
<span class="c1"># 384 dimensions, very fast</span>
<span class="c1"># Use for: Real-time applications, large datasets</span>
</pre></div>
</div>
<p><strong>Fast (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>
<span class="c1"># 1536 dimensions, fast API</span>
<span class="c1"># Use for: Most applications</span>
</pre></div>
</div>
</section>
<section id="by-cost">
<h3>By Cost<a class="headerlink" href="#by-cost" title="Link to this heading">¶</a></h3>
<p><strong>Free (Ollama):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;nomic-embed-text&quot;</span><span class="p">)</span>
<span class="c1"># Zero API costs, requires local compute</span>
<span class="c1"># Cost: GPU/CPU time only</span>
</pre></div>
</div>
<p><strong>Cost-Effective (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
    <span class="n">dimensions</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>  <span class="c1"># Reduce dimensions = lower cost</span>
<span class="p">)</span>
<span class="c1"># $0.02 / 1M tokens</span>
<span class="c1"># Use for: Budget-conscious applications</span>
</pre></div>
</div>
</section>
</section>
<section id="integration-with-rag">
<h2>Integration with RAG<a class="headerlink" href="#integration-with-rag" title="Link to this heading">¶</a></h2>
<section id="basic-rag-pipeline">
<h3>Basic RAG Pipeline<a class="headerlink" href="#basic-rag-pipeline" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit</span><span class="w"> </span><span class="kn">import</span> <span class="n">RagPipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbedding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.vectorstores.milvus</span><span class="w"> </span><span class="kn">import</span> <span class="n">MilvusVectorStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>

<span class="c1"># Setup embedding</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>

<span class="c1"># Create RAG pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">RagPipeline</span><span class="p">(</span>
    <span class="n">embedding_client</span><span class="o">=</span><span class="n">embedding</span><span class="p">,</span>
    <span class="n">vector_store</span><span class="o">=</span><span class="n">MilvusVectorStore</span><span class="p">(</span>
        <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;documents&quot;</span><span class="p">,</span>
        <span class="n">embedding_client</span><span class="o">=</span><span class="n">embedding</span><span class="p">,</span>
        <span class="n">dimension</span><span class="o">=</span><span class="mi">1536</span><span class="p">,</span>  <span class="c1"># Match embedding dimension</span>
    <span class="p">),</span>
    <span class="n">llm_client</span><span class="o">=</span><span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Index documents</span>
<span class="k">await</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">index</span><span class="p">(</span>
    <span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Document 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Document 2&quot;</span><span class="p">],</span>
    <span class="n">metadatas</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;doc1&quot;</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;doc2&quot;</span><span class="p">}]</span>
<span class="p">)</span>

<span class="c1"># Query</span>
<span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;What is in the documents?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hybrid-embedding-strategy">
<h3>Hybrid Embedding Strategy<a class="headerlink" href="#hybrid-embedding-strategy" title="Link to this heading">¶</a></h3>
<p>Use different embeddings for different purposes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fast embedding for initial retrieval</span>
<span class="n">fast_embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;all-minilm&quot;</span><span class="p">)</span>

<span class="c1"># High-quality embedding for reranking</span>
<span class="n">quality_embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-large&quot;</span><span class="p">)</span>

<span class="c1"># Two-stage retrieval</span>
<span class="c1"># Stage 1: Fast search with all-minilm (1000 candidates)</span>
<span class="n">fast_vector_store</span> <span class="o">=</span> <span class="n">MilvusVectorStore</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;fast_search&quot;</span><span class="p">,</span>
    <span class="n">embedding_client</span><span class="o">=</span><span class="n">fast_embedding</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Stage 2: Rerank with text-embedding-3-large (top 10)</span>
<span class="n">quality_vector_store</span> <span class="o">=</span> <span class="n">MilvusVectorStore</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;quality_rerank&quot;</span><span class="p">,</span>
    <span class="n">embedding_client</span><span class="o">=</span><span class="n">quality_embedding</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="custom-embedding-clients">
<h2>Custom Embedding Clients<a class="headerlink" href="#custom-embedding-clients" title="Link to this heading">¶</a></h2>
<p>Implement your own embedding provider following the protocol:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Protocol</span><span class="p">,</span> <span class="n">runtime_checkable</span>

<span class="nd">@runtime_checkable</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EmbeddingClient</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Protocol for embedding clients.&quot;&quot;&quot;</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dimension</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return embedding dimension.&quot;&quot;&quot;</span>
        <span class="o">...</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed a single text.&quot;&quot;&quot;</span>
        <span class="o">...</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed multiple texts (batched).&quot;&quot;&quot;</span>
        <span class="o">...</span>
</pre></div>
</div>
<section id="example-huggingface-embeddings">
<h3>Example: HuggingFace Embeddings<a class="headerlink" href="#example-huggingface-embeddings" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="k">class</span><span class="w"> </span><span class="nc">HuggingFaceEmbedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;HuggingFace sentence-transformers embedding client.&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_sentence_embedding_dimension</span><span class="p">()</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dimension</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dimension</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed single text.&quot;&quot;&quot;</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embedding</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed multiple texts.&quot;&quot;&quot;</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
            <span class="n">texts</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
            <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># Usage</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">HuggingFaceEmbedding</span><span class="p">(</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="../examples/custom_vectorstore.html#custom-embeddings"><span class="std std-ref">Custom Vector Store Example</span></a> for more details.</p>
</section>
</section>
<section id="performance-optimization">
<h2>Performance Optimization<a class="headerlink" href="#performance-optimization" title="Link to this heading">¶</a></h2>
<section id="caching-embeddings">
<h3>Caching Embeddings<a class="headerlink" href="#caching-embeddings" title="Link to this heading">¶</a></h3>
<p>Cache embeddings to avoid recomputing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">lru_cache</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CachedEmbedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Embedding client with caching.&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_client</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">embedding_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_cached</span> <span class="o">=</span> <span class="n">lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">10000</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">_embed_single</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_embed_single</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cached embedding (must return tuple for hashability).&quot;&quot;&quot;</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Embed with caching.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_cached</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dimension</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">dimension</span>

<span class="c1"># Usage</span>
<span class="n">base_embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>
<span class="n">cached_embedding</span> <span class="o">=</span> <span class="n">CachedEmbedding</span><span class="p">(</span><span class="n">base_embedding</span><span class="p">)</span>

<span class="c1"># First call: computes embedding</span>
<span class="n">v1</span> <span class="o">=</span> <span class="k">await</span> <span class="n">cached_embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)</span>  <span class="c1"># API call</span>

<span class="c1"># Second call: uses cache</span>
<span class="n">v2</span> <span class="o">=</span> <span class="k">await</span> <span class="n">cached_embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)</span>  <span class="c1"># No API call</span>
</pre></div>
</div>
</section>
<section id="parallel-processing">
<h3>Parallel Processing<a class="headerlink" href="#parallel-processing" title="Link to this heading">¶</a></h3>
<p>Process documents in parallel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed_documents_parallel</span><span class="p">(</span>
    <span class="n">documents</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">embedding_client</span><span class="p">,</span>
    <span class="n">max_concurrent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Embed documents with concurrency limit.&quot;&quot;&quot;</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="n">max_concurrent</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed_with_limit</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">await</span> <span class="n">embedding_client</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">embed_with_limit</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
    <span class="k">return</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embed_documents_parallel</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">large_document_list</span><span class="p">,</span>
    <span class="n">embedding_client</span><span class="o">=</span><span class="n">embedding</span><span class="p">,</span>
    <span class="n">max_concurrent</span><span class="o">=</span><span class="mi">20</span>  <span class="c1"># 20 concurrent requests</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="batch-size-tuning">
<h3>Batch Size Tuning<a class="headerlink" href="#batch-size-tuning" title="Link to this heading">¶</a></h3>
<p>Find optimal batch size for your use case:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">benchmark_batch_size</span><span class="p">(</span>
    <span class="n">documents</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">embedding_client</span><span class="p">,</span>
    <span class="n">batch_sizes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark different batch sizes.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">batch_size</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">:</span>
        <span class="n">embedding_client</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">await</span> <span class="n">embedding_client</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        
        <span class="n">docs_per_sec</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span> <span class="o">/</span> <span class="n">duration</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch size </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">docs_per_sec</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> docs/sec&quot;</span><span class="p">)</span>

<span class="c1"># Find best batch size</span>
<span class="k">await</span> <span class="n">benchmark_batch_size</span><span class="p">(</span><span class="n">test_documents</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="monitoring-and-debugging">
<h2>Monitoring and Debugging<a class="headerlink" href="#monitoring-and-debugging" title="Link to this heading">¶</a></h2>
<section id="token-usage-tracking">
<h3>Token Usage Tracking<a class="headerlink" href="#token-usage-tracking" title="Link to this heading">¶</a></h3>
<p>Track embedding costs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TokenTrackedEmbedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper to track token usage.&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_client</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">embedding_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="c1"># Rough estimate: 1 token ≈ 4 characters</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">tokens</span>
        <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">embed_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">//</span> <span class="mi">4</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">tokens</span>
        <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dimension</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">dimension</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Estimate cost in USD.&quot;&quot;&quot;</span>
        <span class="c1"># Pricing per 1M tokens</span>
        <span class="n">prices</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
            <span class="s2">&quot;text-embedding-3-large&quot;</span><span class="p">:</span> <span class="mf">0.13</span><span class="p">,</span>
            <span class="s2">&quot;text-embedding-ada-002&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">price_per_million</span> <span class="o">=</span> <span class="n">prices</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_tokens</span> <span class="o">/</span> <span class="mi">1_000_000</span><span class="p">)</span> <span class="o">*</span> <span class="n">price_per_million</span>

<span class="c1"># Usage</span>
<span class="n">tracked</span> <span class="o">=</span> <span class="n">TokenTrackedEmbedding</span><span class="p">(</span><span class="n">OpenAIEmbedding</span><span class="p">())</span>
<span class="k">await</span> <span class="n">tracked</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens used: </span><span class="si">{</span><span class="n">tracked</span><span class="o">.</span><span class="n">total_tokens</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated cost: $</span><span class="si">{</span><span class="n">tracked</span><span class="o">.</span><span class="n">get_cost</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="embedding-quality-check">
<h3>Embedding Quality Check<a class="headerlink" href="#embedding-quality-check" title="Link to this heading">¶</a></h3>
<p>Verify embedding similarity:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">v2</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute cosine similarity between vectors.&quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="c1"># Test similarity</span>
<span class="n">v1</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;dog&quot;</span><span class="p">)</span>
<span class="n">v2</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;puppy&quot;</span><span class="p">)</span>
<span class="n">v3</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;computer&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dog &lt;-&gt; puppy: </span><span class="si">{</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="w"> </span><span class="n">v2</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ~0.8-0.9</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dog &lt;-&gt; computer: </span><span class="si">{</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="w"> </span><span class="n">v3</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ~0.2-0.3</span>
</pre></div>
</div>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading">¶</a></h2>
<section id="api-key-issues">
<h3>API Key Issues<a class="headerlink" href="#api-key-issues" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbedding</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">()</span>
    <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">&quot;api_key&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ Invalid or missing API key&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Set OPENAI_API_KEY environment variable&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="ollama-not-running">
<h3>Ollama Not Running<a class="headerlink" href="#ollama-not-running" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaEmbedding</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">()</span>
    <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">&quot;connection&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ Ollama not running&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start Ollama: ollama serve&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-not-found">
<h3>Model Not Found<a class="headerlink" href="#model-not-found" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;nomic-embed-text&quot;</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">&quot;not found&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ Model not found&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pull model: ollama pull nomic-embed-text&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="dimension-mismatch">
<h3>Dimension Mismatch<a class="headerlink" href="#dimension-mismatch" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ensure embedding and vector store dimensions match</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding dimension: </span><span class="si">{</span><span class="n">embedding</span><span class="o">.</span><span class="n">dimension</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 1536</span>

<span class="n">vector_store</span> <span class="o">=</span> <span class="n">MilvusVectorStore</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;docs&quot;</span><span class="p">,</span>
    <span class="n">dimension</span><span class="o">=</span><span class="n">embedding</span><span class="o">.</span><span class="n">dimension</span><span class="p">,</span>  <span class="c1"># Must match!</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p><strong>Choose the Right Model</strong></p>
<ul class="simple">
<li><p>Production: <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code> (balanced quality/cost)</p></li>
<li><p>High quality: <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code></p></li>
<li><p>Privacy/offline: <code class="docutils literal notranslate"><span class="pre">nomic-embed-text</span></code> (Ollama)</p></li>
</ul>
</li>
<li><p><strong>Batch Processing</strong></p>
<ul class="simple">
<li><p>Always use <code class="docutils literal notranslate"><span class="pre">embed_batch()</span></code> for multiple texts</p></li>
<li><p>Tune batch size based on your infrastructure</p></li>
<li><p>Use async for parallel requests</p></li>
</ul>
</li>
<li><p><strong>Error Handling</strong></p>
<ul class="simple">
<li><p>Implement retries for transient failures</p></li>
<li><p>Log failed embeddings for debugging</p></li>
<li><p>Have fallback embedding strategy</p></li>
</ul>
</li>
<li><p><strong>Cost Optimization</strong></p>
<ul class="simple">
<li><p>Cache frequently embedded texts</p></li>
<li><p>Use dimension reduction when possible</p></li>
<li><p>Consider Ollama for development</p></li>
</ul>
</li>
<li><p><strong>Quality Assurance</strong></p>
<ul class="simple">
<li><p>Test embeddings with known similar/dissimilar texts</p></li>
<li><p>Monitor embedding quality over time</p></li>
<li><p>Validate dimension consistency</p></li>
</ul>
</li>
<li><p><strong>Performance</strong></p>
<ul class="simple">
<li><p>Use appropriate batch sizes</p></li>
<li><p>Implement connection pooling</p></li>
<li><p>Consider caching layer</p></li>
</ul>
</li>
</ol>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="vector_stores.html"><span class="std std-doc">Vector Stores Guide</span></a> - Store and search embeddings</p></li>
<li><p><a class="reference internal" href="rag_pipeline.html"><span class="std std-doc">RAG Pipeline</span></a> - Build complete RAG systems</p></li>
<li><p><a class="reference internal" href="../examples/custom_vectorstore.html#custom-embeddings"><span class="std std-ref">Custom Vector Store Example</span></a></p></li>
<li><p><a class="reference internal" href="../examples/production_setup.html"><span class="std std-doc">Production Setup</span></a> - Deploy to production</p></li>
</ul>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://platform.openai.com/docs/guides/embeddings">OpenAI Embeddings Documentation</a></p></li>
<li><p><a class="reference external" href="https://ollama.com/docs">Ollama Documentation</a></p></li>
<li><p><a class="reference internal" href="protocols.html#embeddingclient"><span class="std std-ref">Embedding Protocol</span></a></p></li>
</ul>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2026, Gianmarco Mottola
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/gmottola00/rag-toolkit" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">:material-vector-polyline: Embeddings</a><ul>
<li><a class="reference internal" href="#material-help-circle-what-are-embeddings">:material-help-circle: What are Embeddings?</a><ul>
<li><a class="reference internal" href="#material-star-key-properties">:material-star: Key Properties</a></li>
</ul>
</li>
<li><a class="reference internal" href="#material-server-supported-embedding-providers">:material-server: Supported Embedding Providers</a><ul>
<li><a class="reference internal" href="#material-openai-openai-recommended">:material-openai: OpenAI (Recommended)</a></li>
<li><a class="reference internal" href="#material-server-security-ollama-local-free">:material-server-security: Ollama (Local, Free)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuration">Configuration</a><ul>
<li><a class="reference internal" href="#batch-size">Batch Size</a></li>
<li><a class="reference internal" href="#timeouts">Timeouts</a></li>
<li><a class="reference internal" href="#retry-logic">Retry Logic</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li><a class="reference internal" href="#dimension-reduction">Dimension Reduction</a></li>
<li><a class="reference internal" href="#custom-prefixes">Custom Prefixes</a></li>
<li><a class="reference internal" href="#normalize-embeddings">Normalize Embeddings</a></li>
</ul>
</li>
<li><a class="reference internal" href="#batch-processing">Batch Processing</a><ul>
<li><a class="reference internal" href="#basic-batch-embedding">Basic Batch Embedding</a></li>
<li><a class="reference internal" href="#progress-tracking">Progress Tracking</a></li>
<li><a class="reference internal" href="#error-handling">Error Handling</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-selection-guide">Model Selection Guide</a><ul>
<li><a class="reference internal" href="#by-quality">By Quality</a></li>
<li><a class="reference internal" href="#by-speed">By Speed</a></li>
<li><a class="reference internal" href="#by-cost">By Cost</a></li>
</ul>
</li>
<li><a class="reference internal" href="#integration-with-rag">Integration with RAG</a><ul>
<li><a class="reference internal" href="#basic-rag-pipeline">Basic RAG Pipeline</a></li>
<li><a class="reference internal" href="#hybrid-embedding-strategy">Hybrid Embedding Strategy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#custom-embedding-clients">Custom Embedding Clients</a><ul>
<li><a class="reference internal" href="#example-huggingface-embeddings">Example: HuggingFace Embeddings</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-optimization">Performance Optimization</a><ul>
<li><a class="reference internal" href="#caching-embeddings">Caching Embeddings</a></li>
<li><a class="reference internal" href="#parallel-processing">Parallel Processing</a></li>
<li><a class="reference internal" href="#batch-size-tuning">Batch Size Tuning</a></li>
</ul>
</li>
<li><a class="reference internal" href="#monitoring-and-debugging">Monitoring and Debugging</a><ul>
<li><a class="reference internal" href="#token-usage-tracking">Token Usage Tracking</a></li>
<li><a class="reference internal" href="#embedding-quality-check">Embedding Quality Check</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#api-key-issues">API Key Issues</a></li>
<li><a class="reference internal" href="#ollama-not-running">Ollama Not Running</a></li>
<li><a class="reference internal" href="#model-not-found">Model Not Found</a></li>
<li><a class="reference internal" href="#dimension-mismatch">Dimension Mismatch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#best-practices">Best Practices</a></li>
<li><a class="reference internal" href="#next-steps">Next Steps</a></li>
<li><a class="reference internal" href="#see-also">See Also</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=38b66d78"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    </body>
</html>