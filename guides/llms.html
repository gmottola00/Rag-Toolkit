<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html">

    <!-- Generated with Sphinx 7.4.7 and Furo 2025.12.19 -->
        <title>:material-robot: LLM Clients - rag-toolkit</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=d577e810" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">rag-toolkit</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">rag-toolkit</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  
</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/gmottola00/rag-toolkit/blob/main/docs/guides/llms.md?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/gmottola00/rag-toolkit/edit/main/docs/guides/llms.md" rel="edit" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="material-robot-llm-clients">
<h1>:material-robot: LLM Clients<a class="headerlink" href="#material-robot-llm-clients" title="Link to this heading">¶</a></h1>
<p>Large Language Models (LLMs) power the generation phase of RAG systems. Master LLM integration for high-quality, contextual responses.</p>
<hr class="docutils" />
<section id="material-information-overview">
<h2>:material-information: Overview<a class="headerlink" href="#material-information-overview" title="Link to this heading">¶</a></h2>
<p>!!! abstract “What LLM Clients Do”
LLM clients in RAG Toolkit handle the entire text generation pipeline.</p>
<p><strong>Capabilities:</strong></p>
<div class="grid cards" markdown>
<ul>
<li><p>:material-text: <strong>Text Generation</strong></p>
<hr class="docutils" />
<p>Generate natural language answers from prompts</p>
</li>
<li><p>:material-file-document-multiple: <strong>Context Integration</strong></p>
<hr class="docutils" />
<p>Combine retrieved documents with user queries</p>
</li>
<li><p>:material-wave: <strong>Streaming</strong></p>
<hr class="docutils" />
<p>Real-time response generation</p>
</li>
<li><p>:material-shield-alert: <strong>Error Handling</strong></p>
<hr class="docutils" />
<p>Automatic retries, rate limiting, fallbacks</p>
</li>
</ul>
</div>
</section>
<hr class="docutils" />
<section id="material-server-supported-llm-providers">
<h2>:material-server: Supported LLM Providers<a class="headerlink" href="#material-server-supported-llm-providers" title="Link to this heading">¶</a></h2>
<p>!!! info “Choose Your Provider”</p>
<section id="openai-recommended">
<h3>OpenAI (Recommended)<a class="headerlink" href="#openai-recommended" title="Link to this heading">¶</a></h3>
<p>OpenAI provides state-of-the-art models with excellent quality and reliability.</p>
<p><strong>Models:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code>: Latest GPT-4, best quality, 128k context</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4</span></code>: Standard GPT-4, 8k context</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code>: Fast and cost-effective, 16k context</p></li>
</ul>
<p><strong>Installation:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>rag-toolkit<span class="o">[</span>openai<span class="o">]</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-api-key&quot;</span>
</pre></div>
</div>
<p><strong>Usage:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>

<span class="c1"># Initialize</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;your-api-key&quot;</span><span class="p">,</span>  <span class="c1"># Or set OPENAI_API_KEY env var</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Generate response</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is machine learning?&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="c1"># Generate with system message</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">,</span>
    <span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a helpful physics teacher.&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Pricing</strong> (as of Dec 2024):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code>: $10 / 1M input tokens, $30 / 1M output tokens</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4</span></code>: $30 / 1M input tokens, $60 / 1M output tokens</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code>: $0.50 / 1M input tokens, $1.50 / 1M output tokens</p></li>
</ul>
</section>
<section id="ollama-local-free">
<h3>Ollama (Local, Free)<a class="headerlink" href="#ollama-local-free" title="Link to this heading">¶</a></h3>
<p>Run powerful LLMs locally with Ollama for privacy and zero API costs.</p>
<p><strong>Popular Models:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">llama3</span></code>: Meta’s Llama 3, excellent quality</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mistral</span></code>: Mistral 7B, fast and capable</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">phi3</span></code>: Microsoft Phi-3, efficient small model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gemma</span></code>: Google Gemma, strong performance</p></li>
</ul>
<p><strong>Installation:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Ollama</span>
curl<span class="w"> </span>-fsSL<span class="w"> </span>https://ollama.com/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh

<span class="c1"># Pull a model</span>
ollama<span class="w"> </span>pull<span class="w"> </span>llama3

<span class="c1"># Install rag-toolkit with Ollama support</span>
pip<span class="w"> </span>install<span class="w"> </span>rag-toolkit<span class="o">[</span>ollama<span class="o">]</span>
</pre></div>
</div>
<p><strong>Usage:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaLLM</span>

<span class="c1"># Initialize</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:11434&quot;</span><span class="p">,</span>  <span class="c1"># Default Ollama URL</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Generate response</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is machine learning?&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="c1"># With system message</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Explain neural networks&quot;</span><span class="p">,</span>
    <span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a helpful AI teacher.&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Model Comparison:</strong></p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Size</p></th>
<th class="head"><p>Speed</p></th>
<th class="head"><p>Quality</p></th>
<th class="head"><p>Context</p></th>
<th class="head"><p>Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">llama3</span></code></p></td>
<td><p>8B</p></td>
<td><p>Medium</p></td>
<td><p>Excellent</p></td>
<td><p>8k</p></td>
<td><p>General purpose</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mistral</span></code></p></td>
<td><p>7B</p></td>
<td><p>Fast</p></td>
<td><p>Very good</p></td>
<td><p>32k</p></td>
<td><p>Long context</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">phi3</span></code></p></td>
<td><p>3.8B</p></td>
<td><p>Very fast</p></td>
<td><p>Good</p></td>
<td><p>4k</p></td>
<td><p>Speed critical</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gemma</span></code></p></td>
<td><p>7B</p></td>
<td><p>Medium</p></td>
<td><p>Very good</p></td>
<td><p>8k</p></td>
<td><p>Balanced</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">¶</a></h2>
<section id="temperature">
<h3>Temperature<a class="headerlink" href="#temperature" title="Link to this heading">¶</a></h3>
<p>Control randomness in responses:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Deterministic (factual responses)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># No randomness</span>
<span class="p">)</span>

<span class="c1"># Balanced (default)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>  <span class="c1"># Some creativity</span>
<span class="p">)</span>

<span class="c1"># Creative</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># Maximum creativity</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="max-tokens">
<h3>Max Tokens<a class="headerlink" href="#max-tokens" title="Link to this heading">¶</a></h3>
<p>Limit response length:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>  <span class="c1"># Maximum 500 tokens in response</span>
<span class="p">)</span>

<span class="c1"># For summaries</span>
<span class="n">llm_summary</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># For detailed explanations</span>
<span class="n">llm_detailed</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="timeout">
<h3>Timeout<a class="headerlink" href="#timeout" title="Link to this heading">¶</a></h3>
<p>Set request timeouts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mf">60.0</span><span class="p">,</span>  <span class="c1"># 60 seconds (default: 120)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="retry-logic">
<h3>Retry Logic<a class="headerlink" href="#retry-logic" title="Link to this heading">¶</a></h3>
<p>Handle failures gracefully:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># Retry failed requests</span>
    <span class="n">retry_delay</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># Wait 1 second between retries</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Link to this heading">¶</a></h2>
<section id="streaming-responses">
<h3>Streaming Responses<a class="headerlink" href="#streaming-responses" title="Link to this heading">¶</a></h3>
<p>Stream responses for real-time display:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stream response tokens as they&#39;re generated</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_stream</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Explain quantum mechanics in detail&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>  <span class="c1"># New line after streaming</span>
</pre></div>
</div>
</section>
<section id="chat-history">
<h3>Chat History<a class="headerlink" href="#chat-history" title="Link to this heading">¶</a></h3>
<p>Maintain conversation context:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.core.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Message</span>

<span class="c1"># Build chat history</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">),</span>
    <span class="n">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;What is Python?&quot;</span><span class="p">),</span>
    <span class="n">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Python is a programming language.&quot;</span><span class="p">),</span>
    <span class="n">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Give me an example.&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="c1"># Generate with history</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_with_history</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="function-calling">
<h3>Function Calling<a class="headerlink" href="#function-calling" title="Link to this heading">¶</a></h3>
<p>Use structured outputs (OpenAI only):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define function</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;function&quot;</span><span class="p">,</span>
        <span class="s2">&quot;function&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;search_documents&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Search for relevant documents&quot;</span><span class="p">,</span>
            <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
                <span class="s2">&quot;properties&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Search query&quot;</span>
                    <span class="p">},</span>
                    <span class="s2">&quot;limit&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;integer&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Number of results&quot;</span>
                    <span class="p">}</span>
                <span class="p">},</span>
                <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Generate with function calling</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Find documents about machine learning&quot;</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">tool_choice</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># Check if function was called</span>
<span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Function: </span><span class="si">{</span><span class="n">tool_call</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Arguments: </span><span class="si">{</span><span class="n">tool_call</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">arguments</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="json-mode">
<h3>JSON Mode<a class="headerlink" href="#json-mode" title="Link to this heading">¶</a></h3>
<p>Force structured JSON output (OpenAI only):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">response_format</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;json_object&quot;</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Extract information from this text as JSON:</span>
<span class="s2">    &quot;John Smith is 30 years old and works as a software engineer in San Francisco.&quot;</span>
<span class="s2">    </span>
<span class="s2">    Return JSON with fields: name, age, occupation, location</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># {&quot;name&quot;: &quot;John Smith&quot;, &quot;age&quot;: 30, ...}</span>
</pre></div>
</div>
</section>
</section>
<section id="integration-with-rag">
<h2>Integration with RAG<a class="headerlink" href="#integration-with-rag" title="Link to this heading">¶</a></h2>
<section id="basic-rag-query">
<h3>Basic RAG Query<a class="headerlink" href="#basic-rag-query" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit</span><span class="w"> </span><span class="kn">import</span> <span class="n">RagPipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbedding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.vectorstores.milvus</span><span class="w"> </span><span class="kn">import</span> <span class="n">MilvusVectorStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>

<span class="c1"># Setup LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create RAG pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">RagPipeline</span><span class="p">(</span>
    <span class="n">embedding_client</span><span class="o">=</span><span class="n">OpenAIEmbedding</span><span class="p">(),</span>
    <span class="n">vector_store</span><span class="o">=</span><span class="n">MilvusVectorStore</span><span class="p">(</span>
        <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;documents&quot;</span><span class="p">,</span>
        <span class="n">embedding_client</span><span class="o">=</span><span class="n">OpenAIEmbedding</span><span class="p">(),</span>
    <span class="p">),</span>
    <span class="n">llm_client</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Query with automatic context retrieval</span>
<span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">query</span><span class="p">(</span>
    <span class="s2">&quot;What are the key findings in the research papers?&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Answer: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">answer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sources: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">sources</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents used&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="custom-prompts">
<h3>Custom Prompts<a class="headerlink" href="#custom-prompts" title="Link to this heading">¶</a></h3>
<p>Customize how context is presented to the LLM:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Custom prompt template</span>
<span class="n">custom_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a research assistant analyzing scientific papers.</span>

<span class="s2">Context from papers:</span>
<span class="si">{context}</span>

<span class="s2">Question: </span><span class="si">{question}</span>

<span class="s2">Provide a detailed answer with citations.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">RagPipeline</span><span class="p">(</span>
    <span class="n">llm_client</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">prompt_template</span><span class="o">=</span><span class="n">custom_prompt</span><span class="p">,</span>
    <span class="c1"># ... other config</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multi-step-reasoning">
<h3>Multi-Step Reasoning<a class="headerlink" href="#multi-step-reasoning" title="Link to this heading">¶</a></h3>
<p>Break complex queries into steps:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1: Decompose query</span>
<span class="n">decomposition_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Break this complex question into simpler sub-questions:</span>
<span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">sub_questions</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">decomposition_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">query</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Step 2: Answer each sub-question</span>
<span class="n">answers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sub_q</span> <span class="ow">in</span> <span class="n">sub_questions</span><span class="p">:</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="k">await</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">sub_q</span><span class="p">)</span>
    <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>

<span class="c1"># Step 3: Synthesize final answer</span>
<span class="n">synthesis_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Synthesize a final answer from these sub-answers:</span>
<span class="si">{answers}</span>

<span class="s2">Original question: </span><span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">final_answer</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">synthesis_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">answers</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">answers</span><span class="p">),</span>
        <span class="n">question</span><span class="o">=</span><span class="n">query</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="model-selection-guide">
<h2>Model Selection Guide<a class="headerlink" href="#model-selection-guide" title="Link to this heading">¶</a></h2>
<section id="by-quality">
<h3>By Quality<a class="headerlink" href="#by-quality" title="Link to this heading">¶</a></h3>
<p><strong>Best Quality (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">)</span>
<span class="c1"># 128k context, best reasoning</span>
<span class="c1"># Use for: Complex tasks, production systems</span>
</pre></div>
</div>
<p><strong>Good Quality (Ollama, Free):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">)</span>
<span class="c1"># 8k context, excellent for most tasks</span>
<span class="c1"># Use for: General purpose, privacy-sensitive</span>
</pre></div>
</div>
</section>
<section id="by-speed">
<h3>By Speed<a class="headerlink" href="#by-speed" title="Link to this heading">¶</a></h3>
<p><strong>Fastest (Ollama, Local):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;phi3&quot;</span><span class="p">)</span>
<span class="c1"># Very fast, good quality</span>
<span class="c1"># Use for: Real-time applications</span>
</pre></div>
</div>
<p><strong>Fast (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>
<span class="c1"># Fast API, good quality</span>
<span class="c1"># Use for: Most applications</span>
</pre></div>
</div>
</section>
<section id="by-cost">
<h3>By Cost<a class="headerlink" href="#by-cost" title="Link to this heading">¶</a></h3>
<p><strong>Free (Ollama):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">)</span>
<span class="c1"># Zero API costs</span>
<span class="c1"># Cost: GPU/CPU time only</span>
</pre></div>
</div>
<p><strong>Cost-Effective (OpenAI):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>
<span class="c1"># $0.50 / 1M input tokens</span>
<span class="c1"># Use for: Budget-conscious applications</span>
</pre></div>
</div>
</section>
<section id="by-context-window">
<h3>By Context Window<a class="headerlink" href="#by-context-window" title="Link to this heading">¶</a></h3>
<p><strong>Longest Context:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># OpenAI GPT-4-turbo: 128k tokens</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">)</span>

<span class="c1"># Ollama Mistral: 32k tokens</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="custom-llm-clients">
<h2>Custom LLM Clients<a class="headerlink" href="#custom-llm-clients" title="Link to this heading">¶</a></h2>
<p>Implement your own LLM provider following the protocol:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Protocol</span><span class="p">,</span> <span class="n">runtime_checkable</span><span class="p">,</span> <span class="n">AsyncIterator</span>

<span class="nd">@runtime_checkable</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LLMClient</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Protocol for LLM clients.&quot;&quot;&quot;</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text completion.&quot;&quot;&quot;</span>
        <span class="o">...</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text completion with streaming.&quot;&quot;&quot;</span>
        <span class="o">...</span>
</pre></div>
</div>
<section id="example-anthropic-claude">
<h3>Example: Anthropic Claude<a class="headerlink" href="#example-anthropic-claude" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncAnthropic</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AnthropicLLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Anthropic Claude LLM client.&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;claude-3-opus-20240229&quot;</span><span class="p">,</span>
        <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">AsyncAnthropic</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span> <span class="o">=</span> <span class="n">max_tokens</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate completion.&quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">messages</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">system</span><span class="o">=</span><span class="n">system_message</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate with streaming.&quot;&quot;&quot;</span>
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">messages</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">system</span><span class="o">=</span><span class="n">system_message</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="o">.</span><span class="n">text_stream</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">chunk</span>

<span class="c1"># Usage</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">AnthropicLLM</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-optimization">
<h2>Performance Optimization<a class="headerlink" href="#performance-optimization" title="Link to this heading">¶</a></h2>
<section id="caching-responses">
<h3>Caching Responses<a class="headerlink" href="#caching-responses" title="Link to this heading">¶</a></h3>
<p>Cache common queries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">lru_cache</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CachedLLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLM client with response caching.&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm_client</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">llm_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_generate_cached</span> <span class="o">=</span> <span class="n">lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">1000</span><span class="p">)(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_generate_sync</span>
        <span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Synchronous wrapper for caching.&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
        <span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loop</span><span class="o">.</span><span class="n">run_until_complete</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate with caching.&quot;&quot;&quot;</span>
        <span class="c1"># Only cache without extra parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_cached</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">base_llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">)</span>
<span class="n">cached_llm</span> <span class="o">=</span> <span class="n">CachedLLM</span><span class="p">(</span><span class="n">base_llm</span><span class="p">)</span>

<span class="c1"># First call: API request</span>
<span class="n">r1</span> <span class="o">=</span> <span class="k">await</span> <span class="n">cached_llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;What is AI?&quot;</span><span class="p">)</span>  <span class="c1"># API call</span>

<span class="c1"># Second call: from cache</span>
<span class="n">r2</span> <span class="o">=</span> <span class="k">await</span> <span class="n">cached_llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;What is AI?&quot;</span><span class="p">)</span>  <span class="c1"># No API call</span>
</pre></div>
</div>
</section>
<section id="batch-processing">
<h3>Batch Processing<a class="headerlink" href="#batch-processing" title="Link to this heading">¶</a></h3>
<p>Process multiple prompts efficiently:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">batch_generate</span><span class="p">(</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">llm_client</span><span class="p">,</span>
    <span class="n">max_concurrent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate responses in parallel with concurrency limit.&quot;&quot;&quot;</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="n">max_concurrent</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate_with_limit</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">await</span> <span class="n">llm_client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_with_limit</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="k">return</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;What is machine learning?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Explain neural networks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Describe gradient descent&quot;</span>
<span class="p">]</span>

<span class="n">responses</span> <span class="o">=</span> <span class="k">await</span> <span class="n">batch_generate</span><span class="p">(</span>
    <span class="n">prompts</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
    <span class="n">llm_client</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">max_concurrent</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="token-estimation">
<h3>Token Estimation<a class="headerlink" href="#token-estimation" title="Link to this heading">¶</a></h3>
<p>Estimate costs before calling API:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_tokens</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gpt-4&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate token count for text.&quot;&quot;&quot;</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_cost</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">expected_response_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gpt-4-turbo&quot;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate API cost in USD.&quot;&quot;&quot;</span>
    <span class="c1"># Pricing per 1M tokens</span>
    <span class="n">prices</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="mf">10.0</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="mf">30.0</span><span class="p">},</span>
        <span class="s2">&quot;gpt-4&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="mf">30.0</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="mf">60.0</span><span class="p">},</span>
        <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">},</span>
    <span class="p">}</span>
    
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">estimate_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">expected_response_length</span>
    
    <span class="n">price</span> <span class="o">=</span> <span class="n">prices</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prices</span><span class="p">[</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">])</span>
    <span class="n">input_cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_tokens</span> <span class="o">/</span> <span class="mi">1_000_000</span><span class="p">)</span> <span class="o">*</span> <span class="n">price</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
    <span class="n">output_cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_tokens</span> <span class="o">/</span> <span class="mi">1_000_000</span><span class="p">)</span> <span class="o">*</span> <span class="n">price</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">input_cost</span> <span class="o">+</span> <span class="n">output_cost</span>

<span class="c1"># Usage</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explain quantum computing in detail...&quot;</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">estimate_cost</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">expected_response_length</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated cost: $</span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="monitoring-and-debugging">
<h2>Monitoring and Debugging<a class="headerlink" href="#monitoring-and-debugging" title="Link to this heading">¶</a></h2>
<section id="response-tracking">
<h3>Response Tracking<a class="headerlink" href="#response-tracking" title="Link to this heading">¶</a></h3>
<p>Track API usage and costs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TrackedLLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLM client with usage tracking.&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm_client</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">llm_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_calls</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate with tracking.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_calls</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="c1"># Estimate input tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">//</span> <span class="mi">4</span>
        
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Estimate output tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">)</span> <span class="o">//</span> <span class="mi">4</span>
        
        <span class="k">return</span> <span class="n">response</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">get_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get usage statistics.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;total_calls&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_calls</span><span class="p">,</span>
            <span class="s2">&quot;input_tokens&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span><span class="p">,</span>
            <span class="s2">&quot;output_tokens&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span><span class="p">,</span>
            <span class="s2">&quot;total_tokens&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span><span class="p">,</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">estimate_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Estimate total cost.&quot;&quot;&quot;</span>
        <span class="n">prices</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="mf">10.0</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="mf">30.0</span><span class="p">},</span>
        <span class="p">}</span>
        <span class="n">price</span> <span class="o">=</span> <span class="n">prices</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prices</span><span class="p">[</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">])</span>
        
        <span class="n">input_cost</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span> <span class="o">/</span> <span class="mi">1_000_000</span><span class="p">)</span> <span class="o">*</span> <span class="n">price</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
        <span class="n">output_cost</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span> <span class="o">/</span> <span class="mi">1_000_000</span><span class="p">)</span> <span class="o">*</span> <span class="n">price</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">input_cost</span> <span class="o">+</span> <span class="n">output_cost</span>

<span class="c1"># Usage</span>
<span class="n">tracked</span> <span class="o">=</span> <span class="n">TrackedLLM</span><span class="p">(</span><span class="n">OpenAILLM</span><span class="p">())</span>
<span class="k">await</span> <span class="n">tracked</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;What is AI?&quot;</span><span class="p">)</span>
<span class="k">await</span> <span class="n">tracked</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Explain ML&quot;</span><span class="p">)</span>

<span class="n">stats</span> <span class="o">=</span> <span class="n">tracked</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total calls: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;total_calls&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total tokens: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;total_tokens&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated cost: $</span><span class="si">{</span><span class="n">tracked</span><span class="o">.</span><span class="n">estimate_cost</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="response-quality-check">
<h3>Response Quality Check<a class="headerlink" href="#response-quality-check" title="Link to this heading">¶</a></h3>
<p>Validate response quality:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate_with_validation</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">llm_client</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate with quality validation.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span><span class="p">):</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm_client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        
        <span class="c1"># Validation checks</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempt </span><span class="si">{</span><span class="n">attempt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Response too short, retrying...&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        
        <span class="k">if</span> <span class="s2">&quot;I don&#39;t know&quot;</span> <span class="ow">in</span> <span class="n">response</span> <span class="ow">and</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_retries</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempt </span><span class="si">{</span><span class="n">attempt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Uncertain response, retrying...&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        
        <span class="k">return</span> <span class="n">response</span>
    
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Failed to generate valid response&quot;</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">generate_with_validation</span><span class="p">(</span>
    <span class="s2">&quot;What is quantum computing?&quot;</span><span class="p">,</span>
    <span class="n">llm_client</span><span class="o">=</span><span class="n">llm</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading">¶</a></h2>
<section id="api-key-issues">
<h3>API Key Issues<a class="headerlink" href="#api-key-issues" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rag_toolkit.infra.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">()</span>
    <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">&quot;api_key&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ Invalid or missing API key&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Set OPENAI_API_KEY environment variable&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="rate-limiting">
<h3>Rate Limiting<a class="headerlink" href="#rate-limiting" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tenacity</span><span class="w"> </span><span class="kn">import</span> <span class="n">retry</span><span class="p">,</span> <span class="n">wait_exponential</span><span class="p">,</span> <span class="n">stop_after_attempt</span>

<span class="nd">@retry</span><span class="p">(</span>
    <span class="n">wait</span><span class="o">=</span><span class="n">wait_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span>
    <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="p">)</span>
<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate_with_retry</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">llm_client</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate with exponential backoff for rate limits.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="k">await</span> <span class="n">llm_client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;rate limit&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Rate limited, backing off...&quot;</span><span class="p">)</span>
            <span class="k">raise</span>
        <span class="k">raise</span>

<span class="c1"># Usage</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">generate_with_retry</span><span class="p">(</span><span class="s2">&quot;What is AI?&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="context-length-errors">
<h3>Context Length Errors<a class="headerlink" href="#context-length-errors" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">truncate_to_context_limit</span><span class="p">(</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8000</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gpt-4&quot;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Truncate text to fit context window.&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
    
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_tokens</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">text</span>
    
    <span class="c1"># Truncate and decode</span>
    <span class="n">truncated_tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">max_tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">encoding</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">truncated_tokens</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">long_prompt</span> <span class="o">=</span> <span class="s2">&quot;...&quot;</span> <span class="o">*</span> <span class="mi">10000</span>  <span class="c1"># Very long prompt</span>
<span class="n">safe_prompt</span> <span class="o">=</span> <span class="n">truncate_to_context_limit</span><span class="p">(</span><span class="n">long_prompt</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">7000</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">safe_prompt</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p><strong>Choose the Right Model</strong></p>
<ul class="simple">
<li><p>Production: <code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code> (best quality)</p></li>
<li><p>Cost-effective: <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code></p></li>
<li><p>Privacy/offline: <code class="docutils literal notranslate"><span class="pre">llama3</span></code> (Ollama)</p></li>
</ul>
</li>
<li><p><strong>Prompt Engineering</strong></p>
<ul class="simple">
<li><p>Be specific and clear</p></li>
<li><p>Provide examples (few-shot)</p></li>
<li><p>Use system messages for role/context</p></li>
<li><p>Structure with clear sections</p></li>
</ul>
</li>
<li><p><strong>Error Handling</strong></p>
<ul class="simple">
<li><p>Implement retries with exponential backoff</p></li>
<li><p>Handle rate limits gracefully</p></li>
<li><p>Validate response quality</p></li>
<li><p>Have fallback strategies</p></li>
</ul>
</li>
<li><p><strong>Cost Optimization</strong></p>
<ul class="simple">
<li><p>Cache common queries</p></li>
<li><p>Use cheaper models when appropriate</p></li>
<li><p>Estimate costs before making calls</p></li>
<li><p>Monitor usage regularly</p></li>
</ul>
</li>
<li><p><strong>Performance</strong></p>
<ul class="simple">
<li><p>Use streaming for better UX</p></li>
<li><p>Batch requests when possible</p></li>
<li><p>Set appropriate timeouts</p></li>
<li><p>Implement concurrency limits</p></li>
</ul>
</li>
<li><p><strong>Quality Assurance</strong></p>
<ul class="simple">
<li><p>Use temperature=0 for consistency</p></li>
<li><p>Validate responses</p></li>
<li><p>A/B test different models</p></li>
<li><p>Monitor response quality</p></li>
</ul>
</li>
</ol>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="rag_pipeline.html"><span class="std std-doc">RAG Pipeline</span></a> - Build complete RAG systems</p></li>
<li><p><a class="reference internal" href="embeddings.html"><span class="std std-doc">Embeddings Guide</span></a> - Learn about embeddings</p></li>
<li><p><a class="reference internal" href="../examples/advanced_pipeline.html"><span class="std std-doc">Advanced Pipeline Example</span></a></p></li>
<li><p><a class="reference internal" href="../examples/production_setup.html"><span class="std std-doc">Production Setup</span></a></p></li>
</ul>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://platform.openai.com/docs">OpenAI API Documentation</a></p></li>
<li><p><a class="reference external" href="https://ollama.com/docs">Ollama Documentation</a></p></li>
<li><p><a class="reference internal" href="protocols.html#llmclient"><span class="std std-ref">LLM Protocol</span></a></p></li>
</ul>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2026, Gianmarco Mottola
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/gmottola00/rag-toolkit" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">:material-robot: LLM Clients</a><ul>
<li><a class="reference internal" href="#material-information-overview">:material-information: Overview</a></li>
<li><a class="reference internal" href="#material-server-supported-llm-providers">:material-server: Supported LLM Providers</a><ul>
<li><a class="reference internal" href="#openai-recommended">OpenAI (Recommended)</a></li>
<li><a class="reference internal" href="#ollama-local-free">Ollama (Local, Free)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuration">Configuration</a><ul>
<li><a class="reference internal" href="#temperature">Temperature</a></li>
<li><a class="reference internal" href="#max-tokens">Max Tokens</a></li>
<li><a class="reference internal" href="#timeout">Timeout</a></li>
<li><a class="reference internal" href="#retry-logic">Retry Logic</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li><a class="reference internal" href="#streaming-responses">Streaming Responses</a></li>
<li><a class="reference internal" href="#chat-history">Chat History</a></li>
<li><a class="reference internal" href="#function-calling">Function Calling</a></li>
<li><a class="reference internal" href="#json-mode">JSON Mode</a></li>
</ul>
</li>
<li><a class="reference internal" href="#integration-with-rag">Integration with RAG</a><ul>
<li><a class="reference internal" href="#basic-rag-query">Basic RAG Query</a></li>
<li><a class="reference internal" href="#custom-prompts">Custom Prompts</a></li>
<li><a class="reference internal" href="#multi-step-reasoning">Multi-Step Reasoning</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-selection-guide">Model Selection Guide</a><ul>
<li><a class="reference internal" href="#by-quality">By Quality</a></li>
<li><a class="reference internal" href="#by-speed">By Speed</a></li>
<li><a class="reference internal" href="#by-cost">By Cost</a></li>
<li><a class="reference internal" href="#by-context-window">By Context Window</a></li>
</ul>
</li>
<li><a class="reference internal" href="#custom-llm-clients">Custom LLM Clients</a><ul>
<li><a class="reference internal" href="#example-anthropic-claude">Example: Anthropic Claude</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-optimization">Performance Optimization</a><ul>
<li><a class="reference internal" href="#caching-responses">Caching Responses</a></li>
<li><a class="reference internal" href="#batch-processing">Batch Processing</a></li>
<li><a class="reference internal" href="#token-estimation">Token Estimation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#monitoring-and-debugging">Monitoring and Debugging</a><ul>
<li><a class="reference internal" href="#response-tracking">Response Tracking</a></li>
<li><a class="reference internal" href="#response-quality-check">Response Quality Check</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#api-key-issues">API Key Issues</a></li>
<li><a class="reference internal" href="#rate-limiting">Rate Limiting</a></li>
<li><a class="reference internal" href="#context-length-errors">Context Length Errors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#best-practices">Best Practices</a></li>
<li><a class="reference internal" href="#next-steps">Next Steps</a></li>
<li><a class="reference internal" href="#see-also">See Also</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=38b66d78"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    </body>
</html>