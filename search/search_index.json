{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#rag-toolkit","title":"RAG Toolkit","text":""},{"location":"#production-ready-rag-library-with-multi-vectorstore-support","title":"Production-ready RAG library with multi-vectorstore support","text":"<p>Get Started View on GitHub</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Protocol-Based Architecture</p> <p>Clean abstractions using Python Protocols (PEP 544). No inheritance required, just implement the interface.</p> Python<pre><code>class MyEmbedding(EmbeddingClient):\n    def embed(self, texts: List[str]) -&gt; List[List[float]]:\n        return your_implementation()\n</code></pre> </li> <li> <p> Multi-VectorStore Support</p> <p>Unified interface for Milvus, Qdrant, and ChromaDB. Switch stores with zero code changes.</p> Python<pre><code>store = get_qdrant_service()  # Or Milvus, ChromaDB\nstore.add_vectors(vectors, texts, metadatas)\n</code></pre> </li> <li> <p> Multiple LLM Providers</p> <p>Built-in support for Ollama and OpenAI with easy extensibility for custom providers.</p> Python<pre><code>llm = get_ollama_llm(model=\"llama3.2\")\nresponse = llm.generate(prompt)\n</code></pre> </li> <li> <p> Modular Installation</p> <p>Optional dependencies let you install only what you need. Keep your environment lean.</p> Bash<pre><code># Minimal install\npip install rag-toolkit\n\n# With Qdrant support\npip install rag-toolkit[qdrant]\n</code></pre> </li> <li> <p> Production Ready</p> <p>Type hints, comprehensive tests, and professional code quality. Battle-tested in production.</p> Type SafetyTesting Python<pre><code>def process(chunks: List[ChunkLike]) -&gt; RagResponse:\n    # Full type checking support\n    ...\n</code></pre> Python<pre><code># 60+ tests with 100% pass rate\npytest tests/\n</code></pre> </li> <li> <p> Migration Tools</p> <p>Seamlessly migrate vector data between stores with validation, retry logic, and progress tracking.</p> Python<pre><code>migrator = VectorStoreMigrator(source, target)\nresult = migrator.migrate(\n    source_collection=\"docs\",\n    filter={\"status\": \"published\"},\n    dry_run=True  # Test first!\n)\n</code></pre> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"basic_rag.py<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra import get_ollama_embedding, get_ollama_llm\nfrom rag_toolkit.infra.vectorstores import get_qdrant_service\n\n# Initialize components\nembedding = get_ollama_embedding(model=\"nomic-embed-text\")\nllm = get_ollama_llm(model=\"llama3.2\")\nvector_store = get_qdrant_service(\n    host=\"localhost\",\n    port=6333,\n    collection_name=\"my_docs\"\n)\n\n# Create RAG pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n)\n\n# Index your documents\npipeline.index_documents([\n    \"RAG combines retrieval and generation for better answers.\",\n    \"Vector stores enable semantic search over embeddings.\",\n    \"Chunking strategies impact retrieval quality.\",\n])\n\n# Query with context\nresponse = pipeline.query(\"What is RAG?\")\nprint(response.answer)\n# Output: \"RAG (Retrieval-Augmented Generation) is a technique that combines...\"\n\nprint(f\"Sources: {len(response.sources)}\")\n# Output: Sources: 2\n</code></pre>"},{"location":"#performance","title":"Performance","text":"<p>Real-world benchmarks across vector stores:</p> Insert PerformanceSearch PerformanceScale (10K vectors) Store Single Insert Batch (100) Batch (1K) Qdrant 1.2ms 25.5ms 260ms ChromaDB 1.6ms 6.5ms 46.5ms Milvus 11086ms 11082ms 22110ms <p>Winner: Qdrant for single inserts, ChromaDB for batches</p> Store Top-1 Top-10 Top-100 ChromaDB 0.64ms 0.90ms 2.99ms Qdrant 1.41ms 2.75ms 8.78ms Milvus 2.27ms 2.30ms 2.16ms <p>Winner: ChromaDB for low latency searches</p> Store Insert Time Search Latency ChromaDB 601ms 1.02ms Qdrant 3178ms 3.29ms Milvus 222734ms 2.40ms <p>Winner: ChromaDB for rapid prototyping</p> <p>View Full Benchmarks </p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Documents] --&gt; B[Parser]\n    B --&gt; C[Chunker]\n    C --&gt; D[Embeddings]\n    D --&gt; E[Vector Store]\n\n    F[Query] --&gt; G[Rewriter]\n    G --&gt; H[Retriever]\n    E --&gt; H\n    H --&gt; I[Reranker]\n    I --&gt; J[LLM]\n    J --&gt; K[Response]\n\n    style A fill:#e1f5ff\n    style E fill:#fff3e0\n    style K fill:#e8f5e9</code></pre> <p>Protocol-based design means any component can be swapped:</p> <ul> <li>Embeddings: Ollama, OpenAI, HuggingFace, Custom</li> <li>Vector Stores: Milvus, Qdrant, ChromaDB, Pinecone, Weaviate</li> <li>LLMs: Ollama, OpenAI, Anthropic, Custom</li> <li>Parsers: PDF, DOCX, TXT, HTML, Custom</li> </ul>"},{"location":"#why-rag-toolkit","title":"Why RAG Toolkit?","text":"<p>Clean Architecture</p> <p>Protocol-based design means no inheritance requirements. Any class matching the protocol signature works seamlessly.</p> <p>Production Ready</p> <p>Built with best practices: type hints, comprehensive docstrings, modular design, and extensive testing (60+ tests).</p> <p>Extensible</p> <p>Add new vector stores, LLM providers, or embedding models without touching core code. Plugin-friendly architecture.</p> <p>Developer Friendly</p> <p>Clear documentation, working examples, and intuitive APIs make development fast and enjoyable.</p>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li> <p> Getting Started</p> <p>Install RAG Toolkit and build your first application in 5 minutes.</p> </li> <li> <p> User Guide</p> <p>Learn core concepts, protocols, and best practices for production RAG systems.</p> </li> <li> <p> API Reference</p> <p>Complete API documentation with all classes, functions, and protocols.</p> </li> <li> <p> Examples</p> <p>Real-world examples from basic RAG to production deployments.</p> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Check out our Contributing Guide to get started.</p> <p> </p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>Complete API documentation for RAG Toolkit.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The API is organized into several modules:</p> <ul> <li> <p> Core Protocols</p> <p>Base protocols for embeddings, LLMs, and vector stores</p> </li> <li> <p> Core Types</p> <p>Type definitions and data models</p> </li> <li> <p>:material-pipeline: RAG Pipeline</p> <p>Complete RAG pipeline implementation</p> </li> <li> <p> Migration</p> <p>Vector store migration tools</p> </li> </ul>"},{"location":"api/#using-the-api-reference","title":"Using the API Reference","text":"<p>Each page includes:</p> <ul> <li>Class/Function signatures with type hints</li> <li>Parameters with descriptions and types</li> <li>Return values and exceptions</li> <li>Examples showing usage</li> <li>Source code links</li> </ul>"},{"location":"api/#quick-navigation","title":"Quick Navigation","text":""},{"location":"api/#core-module","title":"Core Module","text":"Python<pre><code>from rag_toolkit.core import (\n    EmbeddingClient,  # Protocol for embeddings\n    LLMClient,        # Protocol for LLMs\n    VectorStoreClient # Protocol for vector stores\n)\n</code></pre>"},{"location":"api/#rag-module","title":"RAG Module","text":"Python<pre><code>from rag_toolkit.rag import (\n    RagPipeline,      # Main RAG pipeline\n    RagResponse,      # Response model\n    RetrievedChunk    # Retrieved document chunk\n)\n</code></pre>"},{"location":"api/#migration-module","title":"Migration Module","text":"Python<pre><code>from rag_toolkit.migration import (\n    VectorStoreMigrator,  # Migration engine\n    MigrationResult,      # Result model\n    MigrationError        # Exception types\n)\n</code></pre>"},{"location":"api/#convention-notes","title":"Convention Notes","text":"<ul> <li>Protocols: Duck-typed interfaces (no inheritance needed)</li> <li>Type Hints: All public APIs are fully typed</li> <li>Async Support: Currently synchronous (async coming in v0.2.0)</li> <li>Exceptions: All raise descriptive exceptions with context</li> </ul>"},{"location":"api/core/protocols/","title":"Protocols","text":""},{"location":"api/core/protocols/#core-protocols","title":"Core Protocols","text":"<p>Core protocol definitions for RAG Toolkit components.</p>"},{"location":"api/core/protocols/#overview","title":"Overview","text":"<p>RAG Toolkit uses Python Protocols (PEP 544) for type safety without inheritance requirements. Any class implementing the protocol interface works seamlessly.</p>"},{"location":"api/core/protocols/#embedding-protocols","title":"Embedding Protocols","text":""},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient","title":"EmbeddingClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for embedding providers.</p> <p>Uses Protocol instead of ABC for more flexible duck typing. Any class implementing these methods can be used as an EmbeddingClient.</p>"},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient-attributes","title":"Attributes","text":""},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient.model_name","title":"model_name  <code>property</code>","text":"Python<pre><code>model_name: str\n</code></pre> <p>Return the underlying embedding model name.</p>"},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient.dimension","title":"dimension  <code>property</code>","text":"Python<pre><code>dimension: int | None\n</code></pre> <p>Optionally return embedding dimension if known.</p>"},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient-functions","title":"Functions","text":""},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient.embed","title":"embed","text":"Python<pre><code>embed(text: str) -&gt; List[float]\n</code></pre> <p>Embed a single text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>Vector representation as list of floats</p> Source code in <code>src/rag_toolkit/core/embedding/base.py</code> Python<pre><code>def embed(self, text: str) -&gt; List[float]:\n    \"\"\"Embed a single text.\n\n    Args:\n        text: The text to embed\n\n    Returns:\n        Vector representation as list of floats\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.embedding.base.EmbeddingClient.embed_batch","title":"embed_batch","text":"Python<pre><code>embed_batch(texts: Sequence[str]) -&gt; List[List[float]]\n</code></pre> <p>Embed a batch of texts.</p> <p>Default implementation iterates embed() for each text. Implementations can override for batch optimization.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of texts to embed</p> required <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List of vector representations</p> Source code in <code>src/rag_toolkit/core/embedding/base.py</code> Python<pre><code>def embed_batch(self, texts: Sequence[str]) -&gt; List[List[float]]:\n    \"\"\"Embed a batch of texts.\n\n    Default implementation iterates embed() for each text.\n    Implementations can override for batch optimization.\n\n    Args:\n        texts: Sequence of texts to embed\n\n    Returns:\n        List of vector representations\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#llm-protocols","title":"LLM Protocols","text":""},{"location":"api/core/protocols/#rag_toolkit.core.llm.base.LLMClient","title":"LLMClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for Large Language Model clients.</p> <p>Uses Protocol instead of ABC for more flexible duck typing. Any class implementing these methods can be used as an LLMClient.</p>"},{"location":"api/core/protocols/#rag_toolkit.core.llm.base.LLMClient-attributes","title":"Attributes","text":""},{"location":"api/core/protocols/#rag_toolkit.core.llm.base.LLMClient.model_name","title":"model_name  <code>property</code>","text":"Python<pre><code>model_name: str\n</code></pre> <p>Return the underlying model name.</p>"},{"location":"api/core/protocols/#rag_toolkit.core.llm.base.LLMClient-functions","title":"Functions","text":""},{"location":"api/core/protocols/#rag_toolkit.core.llm.base.LLMClient.generate","title":"generate","text":"Python<pre><code>generate(prompt: str, **kwargs: Any) -&gt; str\n</code></pre> <p>Generate a completion from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters (temperature, max_tokens, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text completion</p> Source code in <code>src/rag_toolkit/core/llm/base.py</code> Python<pre><code>def generate(self, prompt: str, **kwargs: Any) -&gt; str:\n    \"\"\"Generate a completion from a prompt.\n\n    Args:\n        prompt: The input prompt\n        **kwargs: Additional parameters (temperature, max_tokens, etc.)\n\n    Returns:\n        Generated text completion\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.llm.base.LLMClient.generate_batch","title":"generate_batch","text":"Python<pre><code>generate_batch(prompts: Iterable[str], **kwargs: Any) -&gt; Iterable[str]\n</code></pre> <p>Optional batch generation.</p> <p>Default implementation iterates generate() for each prompt. Implementations can override for batch optimization.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Iterable[str]</code> <p>Iterable of input prompts</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Yields:</p> Type Description <code>Iterable[str]</code> <p>Generated text completions</p> Source code in <code>src/rag_toolkit/core/llm/base.py</code> Python<pre><code>def generate_batch(self, prompts: Iterable[str], **kwargs: Any) -&gt; Iterable[str]:\n    \"\"\"Optional batch generation.\n\n    Default implementation iterates generate() for each prompt.\n    Implementations can override for batch optimization.\n\n    Args:\n        prompts: Iterable of input prompts\n        **kwargs: Additional parameters\n\n    Yields:\n        Generated text completions\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#vector-store-protocols","title":"Vector Store Protocols","text":""},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient","title":"VectorStoreClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for vector store operations.</p> <p>This defines the interface that all vector store implementations must provide. It abstracts common operations across different vector databases.</p> Implementations <ul> <li>MilvusVectorStore: Milvus 2.x implementation</li> <li>PineconeVectorStore: Pinecone cloud implementation (future)</li> <li>QdrantVectorStore: Qdrant implementation (future)</li> </ul> Example <p>store: VectorStoreClient = MilvusVectorStore(host=\"localhost\") store.create_collection(\"docs\", dimension=384) ids = store.insert( ...     collection_name=\"docs\", ...     vectors=[[0.1, 0.2, ...], ...], ...     texts=[\"doc 1\", \"doc 2\"], ...     metadata=[{\"source\": \"a\"}, {\"source\": \"b\"}] ... ) results = store.search( ...     collection_name=\"docs\", ...     query_vector=[0.1, 0.2, ...], ...     top_k=5 ... )</p>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient-functions","title":"Functions","text":""},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.create_collection","title":"create_collection","text":"Python<pre><code>create_collection(name: str, dimension: int, *, metric: str = 'IP', description: str | None = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Create a new collection/index for storing vectors.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Collection name (unique identifier)</p> required <code>dimension</code> <code>int</code> <p>Vector dimension (e.g., 384, 768, 1536)</p> required <code>metric</code> <code>str</code> <p>Distance metric (\"IP\"=inner product, \"L2\"=euclidean, \"COSINE\")</p> <code>'IP'</code> <code>description</code> <code>str | None</code> <p>Optional collection description</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Provider-specific parameters</p> <code>{}</code> <p>Raises:</p> Type Description <code>CollectionExistsError</code> <p>If collection already exists</p> <code>ValueError</code> <p>If dimension &lt;= 0 or invalid metric</p> Example <p>store.create_collection( ...     name=\"my_docs\", ...     dimension=384, ...     metric=\"IP\", ...     description=\"Product documentation embeddings\" ... )</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def create_collection(\n    self,\n    name: str,\n    dimension: int,\n    *,\n    metric: str = \"IP\",\n    description: str | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Create a new collection/index for storing vectors.\n\n    Args:\n        name: Collection name (unique identifier)\n        dimension: Vector dimension (e.g., 384, 768, 1536)\n        metric: Distance metric (\"IP\"=inner product, \"L2\"=euclidean, \"COSINE\")\n        description: Optional collection description\n        **kwargs: Provider-specific parameters\n\n    Raises:\n        CollectionExistsError: If collection already exists\n        ValueError: If dimension &lt;= 0 or invalid metric\n\n    Example:\n        &gt;&gt;&gt; store.create_collection(\n        ...     name=\"my_docs\",\n        ...     dimension=384,\n        ...     metric=\"IP\",\n        ...     description=\"Product documentation embeddings\"\n        ... )\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.collection_exists","title":"collection_exists","text":"Python<pre><code>collection_exists(name: str) -&gt; bool\n</code></pre> <p>Check if a collection exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Collection name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if collection exists, False otherwise</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def collection_exists(self, name: str) -&gt; bool:\n    \"\"\"\n    Check if a collection exists.\n\n    Args:\n        name: Collection name\n\n    Returns:\n        True if collection exists, False otherwise\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.delete_collection","title":"delete_collection","text":"Python<pre><code>delete_collection(name: str) -&gt; None\n</code></pre> <p>Delete a collection and all its data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Collection name</p> required <p>Raises:</p> Type Description <code>CollectionNotFoundError</code> <p>If collection doesn't exist</p> Warning <p>This operation is irreversible!</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def delete_collection(self, name: str) -&gt; None:\n    \"\"\"\n    Delete a collection and all its data.\n\n    Args:\n        name: Collection name\n\n    Raises:\n        CollectionNotFoundError: If collection doesn't exist\n\n    Warning:\n        This operation is irreversible!\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.insert","title":"insert","text":"Python<pre><code>insert(collection_name: str, vectors: list[list[float]], texts: list[str], metadata: list[VectorMetadata], *, ids: list[str] | None = None, **kwargs: Any) -&gt; list[str]\n</code></pre> <p>Insert vectors with associated text and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Target collection</p> required <code>vectors</code> <code>list[list[float]]</code> <p>List of embedding vectors (must match collection dimension)</p> required <code>texts</code> <code>list[str]</code> <p>Source texts for each vector</p> required <code>metadata</code> <code>list[VectorMetadata]</code> <p>Metadata dict for each vector (for filtering)</p> required <code>ids</code> <code>list[str] | None</code> <p>Optional custom IDs (auto-generated if None)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of assigned IDs (in same order as input)</p> <p>Raises:</p> Type Description <code>CollectionNotFoundError</code> <p>If collection doesn't exist</p> <code>ValueError</code> <p>If vectors/texts/metadata lengths don't match</p> <code>DimensionMismatchError</code> <p>If vector dimension doesn't match collection</p> Example <p>ids = store.insert( ...     collection_name=\"docs\", ...     vectors=[[0.1] * 384, [0.2] * 384], ...     texts=[\"First doc\", \"Second doc\"], ...     metadata=[ ...         {\"source\": \"manual\", \"page\": 1}, ...         {\"source\": \"api\", \"page\": 5} ...     ] ... ) print(ids)  # [\"id_001\", \"id_002\"]</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def insert(\n    self,\n    collection_name: str,\n    vectors: list[list[float]],\n    texts: list[str],\n    metadata: list[VectorMetadata],\n    *,\n    ids: list[str] | None = None,\n    **kwargs: Any,\n) -&gt; list[str]:\n    \"\"\"\n    Insert vectors with associated text and metadata.\n\n    Args:\n        collection_name: Target collection\n        vectors: List of embedding vectors (must match collection dimension)\n        texts: Source texts for each vector\n        metadata: Metadata dict for each vector (for filtering)\n        ids: Optional custom IDs (auto-generated if None)\n        **kwargs: Provider-specific parameters\n\n    Returns:\n        List of assigned IDs (in same order as input)\n\n    Raises:\n        CollectionNotFoundError: If collection doesn't exist\n        ValueError: If vectors/texts/metadata lengths don't match\n        DimensionMismatchError: If vector dimension doesn't match collection\n\n    Example:\n        &gt;&gt;&gt; ids = store.insert(\n        ...     collection_name=\"docs\",\n        ...     vectors=[[0.1] * 384, [0.2] * 384],\n        ...     texts=[\"First doc\", \"Second doc\"],\n        ...     metadata=[\n        ...         {\"source\": \"manual\", \"page\": 1},\n        ...         {\"source\": \"api\", \"page\": 5}\n        ...     ]\n        ... )\n        &gt;&gt;&gt; print(ids)  # [\"id_001\", \"id_002\"]\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.search","title":"search","text":"Python<pre><code>search(collection_name: str, query_vector: list[float], top_k: int = 10, *, filters: VectorMetadata | None = None, **kwargs: Any) -&gt; list[SearchResult]\n</code></pre> <p>Search for similar vectors using ANN (approximate nearest neighbors).</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Collection to search</p> required <code>query_vector</code> <code>list[float]</code> <p>Query embedding vector</p> required <code>top_k</code> <code>int</code> <p>Number of results to return</p> <code>10</code> <code>filters</code> <code>VectorMetadata | None</code> <p>Metadata filters (e.g., {\"source\": \"manual\"})</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Provider-specific parameters (e.g., nprobe, ef)</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[SearchResult]</code> <p>List of search results ordered by similarity (best first)</p> <p>Raises:</p> Type Description <code>CollectionNotFoundError</code> <p>If collection doesn't exist</p> <code>DimensionMismatchError</code> <p>If query dimension doesn't match collection</p> Example <p>results = store.search( ...     collection_name=\"docs\", ...     query_vector=[0.15] * 384, ...     top_k=5, ...     filters={\"source\": \"manual\"} ... ) for result in results: ...     print(f\"ID: {result.id}, Score: {result.score:.4f}\") ...     print(f\"Text: {result.text[:100]}...\")</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def search(\n    self,\n    collection_name: str,\n    query_vector: list[float],\n    top_k: int = 10,\n    *,\n    filters: VectorMetadata | None = None,\n    **kwargs: Any,\n) -&gt; list[SearchResult]:\n    \"\"\"\n    Search for similar vectors using ANN (approximate nearest neighbors).\n\n    Args:\n        collection_name: Collection to search\n        query_vector: Query embedding vector\n        top_k: Number of results to return\n        filters: Metadata filters (e.g., {\"source\": \"manual\"})\n        **kwargs: Provider-specific parameters (e.g., nprobe, ef)\n\n    Returns:\n        List of search results ordered by similarity (best first)\n\n    Raises:\n        CollectionNotFoundError: If collection doesn't exist\n        DimensionMismatchError: If query dimension doesn't match collection\n\n    Example:\n        &gt;&gt;&gt; results = store.search(\n        ...     collection_name=\"docs\",\n        ...     query_vector=[0.15] * 384,\n        ...     top_k=5,\n        ...     filters={\"source\": \"manual\"}\n        ... )\n        &gt;&gt;&gt; for result in results:\n        ...     print(f\"ID: {result.id}, Score: {result.score:.4f}\")\n        ...     print(f\"Text: {result.text[:100]}...\")\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.hybrid_search","title":"hybrid_search","text":"Python<pre><code>hybrid_search(collection_name: str, query_vector: list[float], query_text: str, top_k: int = 10, *, alpha: float = 0.5, filters: VectorMetadata | None = None, **kwargs: Any) -&gt; list[SearchResult]\n</code></pre> <p>Hybrid search combining vector similarity and keyword matching.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Collection to search</p> required <code>query_vector</code> <code>list[float]</code> <p>Query embedding vector</p> required <code>query_text</code> <code>str</code> <p>Query text for keyword search</p> required <code>top_k</code> <code>int</code> <p>Number of results to return</p> <code>10</code> <code>alpha</code> <code>float</code> <p>Weight for vector vs keyword (0=keyword only, 1=vector only)</p> <code>0.5</code> <code>filters</code> <code>VectorMetadata | None</code> <p>Metadata filters</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[SearchResult]</code> <p>List of search results with combined scores</p> Note <p>Not all vector stores support hybrid search natively. Default implementation may combine separate vector + keyword searches.</p> Example <p>results = store.hybrid_search( ...     collection_name=\"docs\", ...     query_vector=[0.15] * 384, ...     query_text=\"installation guide\", ...     top_k=5, ...     alpha=0.7  # Favor vector search ... )</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def hybrid_search(\n    self,\n    collection_name: str,\n    query_vector: list[float],\n    query_text: str,\n    top_k: int = 10,\n    *,\n    alpha: float = 0.5,\n    filters: VectorMetadata | None = None,\n    **kwargs: Any,\n) -&gt; list[SearchResult]:\n    \"\"\"\n    Hybrid search combining vector similarity and keyword matching.\n\n    Args:\n        collection_name: Collection to search\n        query_vector: Query embedding vector\n        query_text: Query text for keyword search\n        top_k: Number of results to return\n        alpha: Weight for vector vs keyword (0=keyword only, 1=vector only)\n        filters: Metadata filters\n        **kwargs: Provider-specific parameters\n\n    Returns:\n        List of search results with combined scores\n\n    Note:\n        Not all vector stores support hybrid search natively.\n        Default implementation may combine separate vector + keyword searches.\n\n    Example:\n        &gt;&gt;&gt; results = store.hybrid_search(\n        ...     collection_name=\"docs\",\n        ...     query_vector=[0.15] * 384,\n        ...     query_text=\"installation guide\",\n        ...     top_k=5,\n        ...     alpha=0.7  # Favor vector search\n        ... )\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.delete","title":"delete","text":"Python<pre><code>delete(collection_name: str, ids: list[str]) -&gt; None\n</code></pre> <p>Delete vectors by IDs.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Collection name</p> required <code>ids</code> <code>list[str]</code> <p>List of vector IDs to delete</p> required <p>Raises:</p> Type Description <code>CollectionNotFoundError</code> <p>If collection doesn't exist</p> Example <p>store.delete(\"docs\", ids=[\"id_001\", \"id_003\"])</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def delete(\n    self,\n    collection_name: str,\n    ids: list[str],\n) -&gt; None:\n    \"\"\"\n    Delete vectors by IDs.\n\n    Args:\n        collection_name: Collection name\n        ids: List of vector IDs to delete\n\n    Raises:\n        CollectionNotFoundError: If collection doesn't exist\n\n    Example:\n        &gt;&gt;&gt; store.delete(\"docs\", ids=[\"id_001\", \"id_003\"])\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.vectorstore.VectorStoreClient.get_stats","title":"get_stats","text":"Python<pre><code>get_stats(collection_name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get collection statistics.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Collection name</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with stats: - \"count\": Number of vectors - \"dimension\": Vector dimension - \"metric\": Distance metric - Additional provider-specific stats</p> Example <p>stats = store.get_stats(\"docs\") print(f\"Collection has {stats['count']} vectors\")</p> Source code in <code>src/rag_toolkit/core/vectorstore.py</code> Python<pre><code>def get_stats(self, collection_name: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Get collection statistics.\n\n    Args:\n        collection_name: Collection name\n\n    Returns:\n        Dictionary with stats:\n            - \"count\": Number of vectors\n            - \"dimension\": Vector dimension\n            - \"metric\": Distance metric\n            - Additional provider-specific stats\n\n    Example:\n        &gt;&gt;&gt; stats = store.get_stats(\"docs\")\n        &gt;&gt;&gt; print(f\"Collection has {stats['count']} vectors\")\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#chunking-protocols","title":"Chunking Protocols","text":""},{"location":"api/core/protocols/#rag_toolkit.core.chunking.types.ChunkLike","title":"ChunkLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for document chunks.</p> <p>This Protocol allows domain layers to implement their own chunk types with additional fields while maintaining compatibility with core chunking logic.</p> Example domain implementation Python<pre><code>@dataclass\nclass TenderChunk:\n    id: str\n    title: str\n    heading_level: int\n    text: str\n    blocks: List[Dict[str, Any]]\n    page_numbers: List[int]\n    # Domain-specific fields\n    tender_id: str\n    lot_id: Optional[str]\n    section_type: str\n\n    def to_dict(self, *, include_blocks: bool = True) -&gt; Dict[str, Any]:\n        data = {\n            \"id\": self.id,\n            \"title\": self.title,\n            \"heading_level\": self.heading_level,\n            \"text\": self.text,\n            \"page_numbers\": self.page_numbers,\n            \"tender_id\": self.tender_id,\n            \"lot_id\": self.lot_id,\n            \"section_type\": self.section_type,\n        }\n        if include_blocks:\n            data[\"blocks\"] = self.blocks\n        return data\n</code></pre> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the chunk</p> <code>title</code> <code>str</code> <p>Section title or heading</p> <code>heading_level</code> <code>int</code> <p>Hierarchical level of the heading (e.g., h1=1, h2=2)</p> <code>text</code> <code>str</code> <p>The actual text content of the chunk</p> <code>blocks</code> <code>List[Dict[str, Any]]</code> <p>List of structured text blocks within this chunk</p> <code>page_numbers</code> <code>List[int]</code> <p>List of page numbers where this chunk appears</p>"},{"location":"api/core/protocols/#rag_toolkit.core.chunking.types.ChunkLike-functions","title":"Functions","text":""},{"location":"api/core/protocols/#rag_toolkit.core.chunking.types.ChunkLike.to_dict","title":"to_dict","text":"Python<pre><code>to_dict(*, include_blocks: bool = True) -&gt; Dict[str, Any]\n</code></pre> <p>Convert chunk to dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>include_blocks</code> <code>bool</code> <p>Whether to include the blocks field in the output</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing at minimum: id, title, heading_level, text,</p> <code>Dict[str, Any]</code> <p>page_numbers. Implementations may include additional fields.</p> Source code in <code>src/rag_toolkit/core/chunking/types.py</code> Python<pre><code>def to_dict(self, *, include_blocks: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"Convert chunk to dictionary representation.\n\n    Args:\n        include_blocks: Whether to include the blocks field in the output\n\n    Returns:\n        Dictionary containing at minimum: id, title, heading_level, text,\n        page_numbers. Implementations may include additional fields.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#rag_toolkit.core.chunking.types.TokenChunkLike","title":"TokenChunkLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for token-optimized chunks.</p> <p>Token chunks are optimized for embedding and retrieval operations, typically derived from larger document chunks with enhanced metadata.</p> Example domain implementation Python<pre><code>@dataclass\nclass TenderTokenChunk:\n    id: str\n    text: str\n    section_path: str\n    metadata: Dict[str, str]\n    page_numbers: List[int]\n    source_chunk_id: str\n    # Domain-specific fields\n    tender_id: str\n    lot_id: Optional[str]\n    section_type: str\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"id\": self.id,\n            \"text\": self.text,\n            \"section_path\": self.section_path,\n            \"metadata\": self.metadata,\n            \"page_numbers\": self.page_numbers,\n            \"source_chunk_id\": self.source_chunk_id,\n            \"tender_id\": self.tender_id,\n            \"lot_id\": self.lot_id,\n            \"section_type\": self.section_type,\n        }\n</code></pre> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the token chunk</p> <code>text</code> <code>str</code> <p>The token-optimized text content</p> <code>section_path</code> <code>str</code> <p>Hierarchical path to the section (e.g., \"Section 1 &gt; Subsection A\")</p> <code>page_numbers</code> <code>List[int]</code> <p>List of page numbers where this chunk appears</p> <code>metadata</code> <code>Dict[str, str]</code> <p>Additional metadata as key-value pairs</p> <code>source_chunk_id</code> <code>str</code> <p>Reference to the original Chunk this was derived from</p>"},{"location":"api/core/protocols/#rag_toolkit.core.chunking.types.TokenChunkLike-functions","title":"Functions","text":""},{"location":"api/core/protocols/#rag_toolkit.core.chunking.types.TokenChunkLike.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert token chunk to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing at minimum: id, text, section_path, metadata,</p> <code>Dict[str, Any]</code> <p>page_numbers, source_chunk_id. Implementations may include additional fields.</p> Source code in <code>src/rag_toolkit/core/chunking/types.py</code> Python<pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert token chunk to dictionary representation.\n\n    Returns:\n        Dictionary containing at minimum: id, text, section_path, metadata,\n        page_numbers, source_chunk_id. Implementations may include additional fields.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/protocols/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/protocols/#implementing-embeddingclient","title":"Implementing EmbeddingClient","text":"Python<pre><code>from typing import List\nfrom rag_toolkit.core import EmbeddingClient\n\nclass MyEmbedding(EmbeddingClient):\n    \"\"\"Custom embedding implementation.\"\"\"\n\n    def embed(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Generate embeddings for texts.\"\"\"\n        # Your implementation here\n        return [[0.1, 0.2, ...] for _ in texts]\n\n    @property\n    def dimension(self) -&gt; int:\n        \"\"\"Return embedding dimension.\"\"\"\n        return 384\n\n# Use it anywhere EmbeddingClient is expected\nembedding = MyEmbedding()\nvectors = embedding.embed([\"Hello\", \"World\"])\n</code></pre>"},{"location":"api/core/protocols/#implementing-vectorstoreclient","title":"Implementing VectorStoreClient","text":"Python<pre><code>from typing import List, Dict, Any, Optional\nfrom rag_toolkit.core import VectorStoreClient\n\nclass MyVectorStore(VectorStoreClient):\n    \"\"\"Custom vector store implementation.\"\"\"\n\n    def add_vectors(\n        self,\n        collection_name: str,\n        vectors: List[List[float]],\n        texts: List[str],\n        metadatas: Optional[List[Dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"Add vectors to collection.\"\"\"\n        # Your implementation\n        pass\n\n    def search(\n        self,\n        collection_name: str,\n        query_vector: List[float],\n        top_k: int = 10,\n        filter_dict: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Search for similar vectors.\"\"\"\n        # Your implementation\n        return []\n\n# Use it with any RAG Toolkit component\nstore = MyVectorStore()\npipeline = RagPipeline(vector_store=store, ...)\n</code></pre>"},{"location":"api/core/protocols/#benefits-of-protocols","title":"Benefits of Protocols","text":"<p>No Inheritance Required</p> <p>Your classes don't need to inherit from base classes. Just implement the interface.</p> <p>Type Safety</p> <p>Full type checking support with mypy, pyright, and IDE autocomplete.</p> <p>Testing Friendly</p> <p>Easy to mock for testing - just implement the protocol methods you need.</p> <p>Extensible</p> <p>Add new implementations without modifying core code or dealing with complex inheritance hierarchies.</p>"},{"location":"api/core/types/","title":"Types","text":""},{"location":"api/core/types/#core-types","title":"Core Types","text":"<p>Type definitions and data models used throughout RAG Toolkit.</p>"},{"location":"api/core/types/#data-models","title":"Data Models","text":""},{"location":"api/core/types/#rag_toolkit.core.chunking.models.Chunk","title":"Chunk  <code>dataclass</code>","text":"Python<pre><code>Chunk(id: str, title: str, heading_level: int, text: str, blocks: List[Dict[str, Any]] = list(), page_numbers: List[int] = list())\n</code></pre> <p>Standard implementation of ChunkLike protocol.</p>"},{"location":"api/core/types/#rag_toolkit.core.chunking.models.Chunk-functions","title":"Functions","text":""},{"location":"api/core/types/#rag_toolkit.core.chunking.models.Chunk.to_dict","title":"to_dict","text":"Python<pre><code>to_dict(*, include_blocks: bool = True) -&gt; Dict[str, Any]\n</code></pre> <p>Convert chunk to dictionary representation.</p> Source code in <code>src/rag_toolkit/core/chunking/models.py</code> Python<pre><code>def to_dict(self, *, include_blocks: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"Convert chunk to dictionary representation.\"\"\"\n    data = {\n        \"id\": self.id,\n        \"title\": self.title,\n        \"heading_level\": self.heading_level,\n        \"text\": self.text,\n        \"page_numbers\": self.page_numbers,\n    }\n    if include_blocks:\n        data[\"blocks\"] = self.blocks\n    return data\n</code></pre>"},{"location":"api/core/types/#rag_toolkit.core.chunking.models.TokenChunk","title":"TokenChunk  <code>dataclass</code>","text":"Python<pre><code>TokenChunk(id: str, text: str, section_path: str, metadata: Dict[str, str] = dict(), page_numbers: List[int] = list(), source_chunk_id: str = '')\n</code></pre> <p>Standard implementation of TokenChunkLike protocol.</p>"},{"location":"api/core/types/#rag_toolkit.core.chunking.models.TokenChunk-functions","title":"Functions","text":""},{"location":"api/core/types/#rag_toolkit.core.chunking.models.TokenChunk.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert token chunk to dictionary representation.</p> Source code in <code>src/rag_toolkit/core/chunking/models.py</code> Python<pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert token chunk to dictionary representation.\"\"\"\n    return {\n        \"id\": self.id,\n        \"text\": self.text,\n        \"section_path\": self.section_path,\n        \"metadata\": self.metadata,\n        \"page_numbers\": self.page_numbers,\n        \"source_chunk_id\": self.source_chunk_id,\n    }\n</code></pre>"},{"location":"api/core/types/#type-aliases","title":"Type Aliases","text":"<p>Common type aliases used in the codebase:</p> Python<pre><code>from typing import List, Dict, Any, Optional\n\n# Vector types\nVector = List[float]\nVectors = List[Vector]\n\n# Metadata types\nMetadata = Dict[str, Any]\nMetadatas = List[Metadata]\n\n# Document types\nDocument = str\nDocuments = List[Document]\n</code></pre>"},{"location":"api/core/types/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/types/#creating-chunks","title":"Creating Chunks","text":"Python<pre><code>from rag_toolkit.core.chunking.models import Chunk\n\nchunk = Chunk(\n    text=\"RAG combines retrieval and generation.\",\n    metadata={\n        \"source\": \"docs/intro.md\",\n        \"page\": 1,\n        \"section\": \"Overview\"\n    }\n)\n\nprint(chunk.text)       # Access text\nprint(chunk.metadata)   # Access metadata\n</code></pre>"},{"location":"api/core/types/#working-with-tokenchunks","title":"Working with TokenChunks","text":"Python<pre><code>from rag_toolkit.core.chunking.models import TokenChunk\n\ntoken_chunk = TokenChunk(\n    text=\"Long document text...\",\n    metadata={\"doc_id\": \"123\"},\n    token_count=150,\n    char_count=750\n)\n\n# Check token limits\nif token_chunk.token_count &gt; 512:\n    print(\"Chunk exceeds token limit\")\n</code></pre>"},{"location":"api/migration/migrator/","title":"Migration","text":""},{"location":"api/migration/migrator/#migration-tools","title":"Migration Tools","text":"<p>Vector store migration engine with validation, retry logic, and progress tracking.</p>"},{"location":"api/migration/migrator/#vectorstoremigrator","title":"VectorStoreMigrator","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.migrator.VectorStoreMigrator","title":"VectorStoreMigrator","text":"Python<pre><code>VectorStoreMigrator(source: VectorStoreClient, target: VectorStoreClient, on_progress: Optional[Callable[[MigrationProgress], None]] = None, validate: bool = True, max_retries: int = DEFAULT_MAX_RETRIES, retry_delay: float = DEFAULT_RETRY_DELAY, retry_backoff: float = DEFAULT_RETRY_BACKOFF)\n</code></pre> <p>Migrates vector data between different vector store implementations.</p> <p>This class provides functionality to migrate vectors, embeddings, and metadata from one vector store to another with validation, progress tracking, and error handling.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>VectorStoreClient</code> <p>Source vector store client</p> required <code>target</code> <code>VectorStoreClient</code> <p>Target vector store client</p> required <code>on_progress</code> <code>Optional[Callable[[MigrationProgress], None]]</code> <p>Optional callback function called with MigrationProgress</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Whether to validate data after migration (default: True)</p> <code>True</code> Example Python<pre><code>from rag_toolkit.migration import VectorStoreMigrator\n\nmigrator = VectorStoreMigrator(\n    source=chroma_service,\n    target=qdrant_service,\n    on_progress=lambda p: print(f\"{p.percentage:.1f}% complete\"),\n)\n\nresult = migrator.migrate(\n    collection_name=\"my_docs\",\n    batch_size=1000,\n)\n\nprint(f\"Migrated {result.vectors_migrated} vectors in {result.duration_seconds}s\")\n</code></pre> Source code in <code>src/rag_toolkit/migration/migrator.py</code> Python<pre><code>def __init__(\n    self,\n    source: VectorStoreClient,\n    target: VectorStoreClient,\n    on_progress: Optional[Callable[[MigrationProgress], None]] = None,\n    validate: bool = True,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    retry_delay: float = DEFAULT_RETRY_DELAY,\n    retry_backoff: float = DEFAULT_RETRY_BACKOFF,\n):\n    self.source = source\n    self.target = target\n    self.on_progress = on_progress\n    self.validate = validate\n    self.max_retries = max_retries\n    self.retry_delay = retry_delay\n    self.retry_backoff = retry_backoff\n    self._id_mapping: Dict[str, str] = {}\n</code></pre>"},{"location":"api/migration/migrator/#rag_toolkit.migration.migrator.VectorStoreMigrator-functions","title":"Functions","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.migrator.VectorStoreMigrator.migrate","title":"migrate","text":"Python<pre><code>migrate(source_collection: str, target_collection: Optional[str] = None, batch_size: int = 1000, validate: Optional[bool] = None, filter: Optional[Dict[str, Any]] = None, dry_run: bool = False) -&gt; MigrationResult\n</code></pre> <p>Migrate all vectors from source to target collection.</p> <p>Parameters:</p> Name Type Description Default <code>source_collection</code> <code>str</code> <p>Name of the collection in source store</p> required <code>target_collection</code> <code>Optional[str]</code> <p>Name of the collection in target store (defaults to source name)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of vectors to process per batch</p> <code>1000</code> <code>validate</code> <code>Optional[bool]</code> <p>Override instance validation setting</p> <code>None</code> <code>filter</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata filter to apply (only migrate matching vectors)</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, simulate migration without writing to target</p> <code>False</code> <p>Returns:</p> Type Description <code>MigrationResult</code> <p>MigrationResult with detailed migration statistics</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If migration fails</p> <code>CollectionNotFoundError</code> <p>If source collection doesn't exist</p> Source code in <code>src/rag_toolkit/migration/migrator.py</code> Python<pre><code>def migrate(\n    self,\n    source_collection: str,\n    target_collection: Optional[str] = None,\n    batch_size: int = 1000,\n    validate: Optional[bool] = None,\n    filter: Optional[Dict[str, Any]] = None,\n    dry_run: bool = False,\n) -&gt; MigrationResult:\n    \"\"\"Migrate all vectors from source to target collection.\n\n    Args:\n        source_collection: Name of the collection in source store\n        target_collection: Name of the collection in target store (defaults to source name)\n        batch_size: Number of vectors to process per batch\n        validate: Override instance validation setting\n        filter: Optional metadata filter to apply (only migrate matching vectors)\n        dry_run: If True, simulate migration without writing to target\n\n    Returns:\n        MigrationResult with detailed migration statistics\n\n    Raises:\n        MigrationError: If migration fails\n        CollectionNotFoundError: If source collection doesn't exist\n    \"\"\"\n    target_collection = target_collection or source_collection\n    should_validate = validate if validate is not None else self.validate\n\n    started_at = datetime.now()\n    start_time = time.time()\n\n    vectors_migrated = 0\n    vectors_failed = 0\n    errors: List[str] = []\n\n    try:\n        # Get total count for progress tracking\n        total_vectors = self.source.count(source_collection)\n        if total_vectors == 0:\n            raise CollectionNotFoundError(\n                f\"Collection '{source_collection}' not found or empty\"\n            )\n\n        mode_info = []\n        if dry_run:\n            mode_info.append(\"DRY-RUN\")\n        if filter:\n            mode_info.append(f\"FILTERED: {filter}\")\n        mode_str = f\" [{', '.join(mode_info)}]\" if mode_info else \"\"\n\n        logger.info(\n            f\"Starting migration of {total_vectors} vectors from '{source_collection}' \"\n            f\"to '{target_collection}'{mode_str}\"\n        )\n\n        # Calculate number of batches\n        total_batches = (total_vectors + batch_size - 1) // batch_size\n\n        # Migrate in batches\n        offset = 0\n        current_batch = 0\n\n        while offset &lt; total_vectors:\n            current_batch += 1\n            batch_start = time.time()\n\n            try:\n                # Fetch batch from source with retry\n                batch_data = self._fetch_batch_with_retry(\n                    source_collection, offset, batch_size, filter\n                )\n\n                if not batch_data:\n                    break\n\n                # Apply metadata filter if specified\n                if filter:\n                    batch_data = self._apply_filter(batch_data, filter)\n                    if not batch_data:\n                        offset += batch_size\n                        continue\n\n                # Insert batch into target (skip if dry-run)\n                if not dry_run:\n                    self._insert_batch_with_retry(target_collection, batch_data)\n\n                batch_size_actual = len(batch_data)\n                vectors_migrated += batch_size_actual\n                offset += batch_size_actual\n\n                # Report progress\n                if self.on_progress:\n                    elapsed = time.time() - start_time\n                    progress = MigrationProgress(\n                        vectors_processed=vectors_migrated,\n                        total_vectors=total_vectors,\n                        current_batch=current_batch,\n                        total_batches=total_batches,\n                        elapsed_seconds=elapsed,\n                        errors=vectors_failed,\n                    )\n                    self.on_progress(progress)\n\n                logger.debug(\n                    f\"Batch {current_batch}/{total_batches} completed: \"\n                    f\"{batch_size_actual} vectors in {time.time() - batch_start:.2f}s\"\n                )\n\n            except Exception as e:\n                error_msg = f\"Error in batch {current_batch}: {e}\"\n                logger.error(error_msg)\n                errors.append(error_msg)\n                vectors_failed += batch_size\n                offset += batch_size\n\n        # Validation (skip in dry-run mode)\n        if should_validate and vectors_migrated &gt; 0 and not dry_run:\n            logger.info(\"Validating migration...\")\n            try:\n                self._validate_migration(source_collection, target_collection, vectors_migrated)\n            except ValidationError as e:\n                errors.append(f\"Validation failed: {e}\")\n                logger.warning(f\"Validation failed: {e}\")\n\n        completed_at = datetime.now()\n        duration = time.time() - start_time\n\n        success = vectors_failed == 0 and len(errors) == 0\n\n        result = MigrationResult(\n            success=success,\n            vectors_migrated=vectors_migrated,\n            vectors_failed=vectors_failed,\n            duration_seconds=duration,\n            source_collection=source_collection,\n            target_collection=target_collection,\n            started_at=started_at,\n            completed_at=completed_at,\n            errors=errors,\n            metadata={\n                \"batch_size\": batch_size,\n                \"total_batches\": current_batch,\n                \"validated\": should_validate and not dry_run,\n                \"dry_run\": dry_run,\n                \"filter\": filter,\n            },\n        )\n\n        if dry_run:\n            logger.info(\n                f\"DRY-RUN completed: {vectors_migrated} vectors would be migrated \"\n                f\"in {duration:.2f}s\"\n            )\n        else:\n            logger.info(\n                f\"Migration completed: {vectors_migrated} vectors migrated, \"\n                f\"{vectors_failed} failed in {duration:.2f}s \"\n                f\"({result.success_rate:.1f}% success rate)\"\n            )\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Migration failed: {e}\")\n        completed_at = datetime.now()\n        duration = time.time() - start_time\n\n        return MigrationResult(\n            success=False,\n            vectors_migrated=vectors_migrated,\n            vectors_failed=vectors_failed,\n            duration_seconds=duration,\n            source_collection=source_collection,\n            target_collection=target_collection,\n            started_at=started_at,\n            completed_at=completed_at,\n            errors=errors + [str(e)],\n        )\n</code></pre>"},{"location":"api/migration/migrator/#rag_toolkit.migration.migrator.VectorStoreMigrator.estimate","title":"estimate","text":"Python<pre><code>estimate(collection_name: str, batch_size: int = 1000) -&gt; MigrationEstimate\n</code></pre> <p>Estimate migration time and resource requirements.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to migrate</p> required <code>batch_size</code> <code>int</code> <p>Number of vectors per batch</p> <code>1000</code> <p>Returns:</p> Type Description <code>MigrationEstimate</code> <p>MigrationEstimate with estimated metrics</p> <p>Raises:</p> Type Description <code>CollectionNotFoundError</code> <p>If collection doesn't exist in source</p> Source code in <code>src/rag_toolkit/migration/migrator.py</code> Python<pre><code>def estimate(self, collection_name: str, batch_size: int = 1000) -&gt; MigrationEstimate:\n    \"\"\"Estimate migration time and resource requirements.\n\n    Args:\n        collection_name: Name of the collection to migrate\n        batch_size: Number of vectors per batch\n\n    Returns:\n        MigrationEstimate with estimated metrics\n\n    Raises:\n        CollectionNotFoundError: If collection doesn't exist in source\n    \"\"\"\n    try:\n        # Get collection info from source\n        source_count = self.source.count(collection_name)\n        if source_count == 0:\n            raise CollectionNotFoundError(\n                f\"Collection '{collection_name}' not found or empty in source store\"\n            )\n\n        # Get dimension info (search for a sample vector)\n        sample_results = self.source.search(\n            collection_name=collection_name,\n            query_vector=[0.0] * 384,  # Dummy vector for dimension check\n            top_k=1,\n        )\n\n        source_dimension = 384  # Default, will be detected from actual data\n        estimated_batches = (source_count + batch_size - 1) // batch_size\n\n        # Rough estimate: 100-500 vectors/second depending on store\n        estimated_rate = 200  # Conservative estimate\n        estimated_duration = source_count / estimated_rate\n\n        return MigrationEstimate(\n            total_vectors=source_count,\n            estimated_duration_seconds=estimated_duration,\n            estimated_batches=estimated_batches,\n            source_dimension=source_dimension,\n            compatible=True,\n        )\n\n    except CollectionNotFoundError:\n        # Re-raise CollectionNotFoundError directly\n        raise\n    except Exception as e:\n        logger.error(f\"Error estimating migration: {e}\")\n        raise MigrationError(f\"Failed to estimate migration: {e}\") from e\n</code></pre>"},{"location":"api/migration/migrator/#models","title":"Models","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationResult","title":"MigrationResult  <code>dataclass</code>","text":"Python<pre><code>MigrationResult(success: bool, vectors_migrated: int, vectors_failed: int, duration_seconds: float, source_collection: str, target_collection: str, started_at: datetime, completed_at: datetime, errors: List[str] = list(), metadata: Dict[str, Any] = dict())\n</code></pre> <p>Result of a migration operation.</p> <p>Attributes:</p> Name Type Description <code>success</code> <code>bool</code> <p>Whether the migration completed successfully</p> <code>vectors_migrated</code> <code>int</code> <p>Total number of vectors successfully migrated</p> <code>vectors_failed</code> <code>int</code> <p>Number of vectors that failed to migrate</p> <code>duration_seconds</code> <code>float</code> <p>Total time taken for migration</p> <code>source_collection</code> <code>str</code> <p>Name of the source collection</p> <code>target_collection</code> <code>str</code> <p>Name of the target collection</p> <code>started_at</code> <code>datetime</code> <p>Timestamp when migration started</p> <code>completed_at</code> <code>datetime</code> <p>Timestamp when migration completed</p> <code>errors</code> <code>List[str]</code> <p>List of error messages encountered during migration</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional metadata about the migration</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationResult-attributes","title":"Attributes","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationResult.total_vectors","title":"total_vectors  <code>property</code>","text":"Python<pre><code>total_vectors: int\n</code></pre> <p>Total number of vectors processed.</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationResult.success_rate","title":"success_rate  <code>property</code>","text":"Python<pre><code>success_rate: float\n</code></pre> <p>Percentage of successfully migrated vectors.</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationEstimate","title":"MigrationEstimate  <code>dataclass</code>","text":"Python<pre><code>MigrationEstimate(total_vectors: int, estimated_duration_seconds: float, estimated_batches: int, source_dimension: int, target_dimension: Optional[int] = None, compatible: bool = True, warnings: List[str] = list())\n</code></pre> <p>Estimated metrics for a migration operation.</p> <p>Attributes:</p> Name Type Description <code>total_vectors</code> <code>int</code> <p>Total number of vectors to migrate</p> <code>estimated_duration_seconds</code> <code>float</code> <p>Estimated time to complete</p> <code>estimated_batches</code> <code>int</code> <p>Number of batches required</p> <code>source_dimension</code> <code>int</code> <p>Vector dimension in source store</p> <code>target_dimension</code> <code>Optional[int]</code> <p>Vector dimension in target store (if different)</p> <code>compatible</code> <code>bool</code> <p>Whether schemas are compatible</p> <code>warnings</code> <code>List[str]</code> <p>List of potential issues or warnings</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationEstimate-attributes","title":"Attributes","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationEstimate.vectors_per_second","title":"vectors_per_second  <code>property</code>","text":"Python<pre><code>vectors_per_second: float\n</code></pre> <p>Estimated migration throughput.</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationProgress","title":"MigrationProgress  <code>dataclass</code>","text":"Python<pre><code>MigrationProgress(vectors_processed: int, total_vectors: int, current_batch: int, total_batches: int, elapsed_seconds: float, errors: int = 0)\n</code></pre> <p>Current progress of an ongoing migration.</p> <p>Attributes:</p> Name Type Description <code>vectors_processed</code> <code>int</code> <p>Number of vectors processed so far</p> <code>total_vectors</code> <code>int</code> <p>Total number of vectors to migrate</p> <code>current_batch</code> <code>int</code> <p>Current batch number</p> <code>total_batches</code> <code>int</code> <p>Total number of batches</p> <code>elapsed_seconds</code> <code>float</code> <p>Time elapsed since start</p> <code>errors</code> <code>int</code> <p>Number of errors encountered</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationProgress-attributes","title":"Attributes","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationProgress.percentage","title":"percentage  <code>property</code>","text":"Python<pre><code>percentage: float\n</code></pre> <p>Completion percentage.</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.models.MigrationProgress.eta_seconds","title":"eta_seconds  <code>property</code>","text":"Python<pre><code>eta_seconds: float\n</code></pre> <p>Estimated time remaining in seconds.</p>"},{"location":"api/migration/migrator/#exceptions","title":"Exceptions","text":""},{"location":"api/migration/migrator/#rag_toolkit.migration.exceptions.MigrationError","title":"MigrationError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for migration-related errors.</p>"},{"location":"api/migration/migrator/#rag_toolkit.migration.exceptions.ValidationError","title":"ValidationError","text":"<p>               Bases: <code>MigrationError</code></p> <p>Raised when migration validation fails.</p>"},{"location":"api/migration/migrator/#usage-examples","title":"Usage Examples","text":""},{"location":"api/migration/migrator/#basic-migration","title":"Basic Migration","text":"Python<pre><code>from rag_toolkit.migration import VectorStoreMigrator\nfrom rag_toolkit.infra.vectorstores import get_qdrant_service, get_chromadb_service\n\n# Initialize stores\nsource = get_qdrant_service(host=\"localhost\", port=6333)\ntarget = get_chromadb_service(host=\"localhost\", port=8000)\n\n# Create migrator\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    validate=True\n)\n\n# Run migration\nresult = migrator.migrate(\n    source_collection=\"docs\",\n    target_collection=\"docs_backup\"\n)\n\nprint(f\"Migrated: {result.vectors_migrated}\")\nprint(f\"Success rate: {result.success_rate}%\")\n</code></pre>"},{"location":"api/migration/migrator/#migration-with-filtering","title":"Migration with Filtering","text":"Python<pre><code># Migrate only published documents from 2025\nresult = migrator.migrate(\n    source_collection=\"documents\",\n    target_collection=\"documents_2025\",\n    filter={\n        \"status\": \"published\",\n        \"year\": 2025\n    }\n)\n</code></pre>"},{"location":"api/migration/migrator/#dry-run-mode","title":"Dry-Run Mode","text":"Python<pre><code># Test migration without writing\nresult = migrator.migrate(\n    source_collection=\"large_dataset\",\n    target_collection=\"large_dataset_backup\",\n    dry_run=True  # No writes to target\n)\n\nprint(f\"Would migrate {result.vectors_migrated} vectors\")\nprint(f\"Estimated duration: {result.duration_seconds:.1f}s\")\n\n# If looks good, run for real\nif result.vectors_migrated &lt; 1_000_000:\n    result = migrator.migrate(\n        source_collection=\"large_dataset\",\n        target_collection=\"large_dataset_backup\",\n        dry_run=False\n    )\n</code></pre>"},{"location":"api/migration/migrator/#migration-with-retry-logic","title":"Migration with Retry Logic","text":"Python<pre><code># Configure retry behavior\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    max_retries=5,\n    retry_delay=2.0,\n    retry_backoff=1.5\n)\n\n# Migration will automatically retry on failures\nresult = migrator.migrate(source_collection=\"docs\")\n</code></pre>"},{"location":"api/migration/migrator/#progress-tracking","title":"Progress Tracking","text":"Python<pre><code>def on_progress(progress):\n    print(\n        f\"Progress: {progress.percentage:.1f}% \"\n        f\"({progress.vectors_processed}/{progress.total_vectors}) \"\n        f\"ETA: {progress.eta_seconds:.0f}s\"\n    )\n\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    on_progress=on_progress  # Callback for progress\n)\n\nresult = migrator.migrate(source_collection=\"large_collection\")\n</code></pre>"},{"location":"api/migration/migrator/#estimation","title":"Estimation","text":"Python<pre><code># Estimate before migrating\nestimate = migrator.estimate(\n    collection_name=\"huge_collection\",\n    batch_size=1000\n)\n\nprint(f\"Total vectors: {estimate.total_vectors:,}\")\nprint(f\"Estimated duration: {estimate.estimated_duration_seconds:.0f}s\")\nprint(f\"Estimated batches: {estimate.estimated_batches}\")\nprint(f\"Throughput: {estimate.vectors_per_second:.1f} vectors/sec\")\n\n# Decide based on estimate\nif estimate.estimated_duration_seconds &gt; 3600:  # &gt; 1 hour\n    print(\"Migration will take long, consider splitting\")\nelse:\n    result = migrator.migrate(collection_name=\"huge_collection\")\n</code></pre>"},{"location":"api/migration/migrator/#see-also","title":"See Also","text":"<ul> <li>User Guide: Migration - Comprehensive guide</li> <li>Examples: Production Setup - Production patterns</li> </ul>"},{"location":"api/rag/pipeline/","title":"RAG Pipeline","text":""},{"location":"api/rag/pipeline/#rag-pipeline","title":"RAG Pipeline","text":"<p>Complete RAG pipeline implementation with query rewriting, retrieval, reranking, and generation.</p>"},{"location":"api/rag/pipeline/#ragpipeline","title":"RagPipeline","text":""},{"location":"api/rag/pipeline/#rag_toolkit.rag.pipeline.RagPipeline","title":"RagPipeline","text":"Python<pre><code>RagPipeline(*, vector_searcher: SearchStrategy, rewriter: QueryRewriter, reranker: LLMReranker, assembler: ContextAssembler, generator_llm: LLMClient)\n</code></pre> <p>RAG pipeline with query rewriting, vector retrieval, reranking, assembly, generation.</p> Source code in <code>src/rag_toolkit/rag/pipeline.py</code> Python<pre><code>def __init__(\n    self,\n    *,\n    vector_searcher: SearchStrategy,\n    rewriter: QueryRewriter,\n    reranker: LLMReranker,\n    assembler: ContextAssembler,\n    generator_llm: LLMClient,\n) -&gt; None:\n    self.vector_searcher = vector_searcher\n    self.rewriter = rewriter\n    self.reranker = reranker\n    self.assembler = assembler\n    self.generator_llm = generator_llm\n</code></pre>"},{"location":"api/rag/pipeline/#rag_toolkit.rag.pipeline.RagPipeline-functions","title":"Functions","text":""},{"location":"api/rag/pipeline/#response-models","title":"Response Models","text":""},{"location":"api/rag/pipeline/#rag_toolkit.rag.models.RagResponse","title":"RagResponse  <code>dataclass</code>","text":"Python<pre><code>RagResponse(answer: str, citations: List[RetrievedChunk])\n</code></pre> <p>Structured RAG response with citations.</p>"},{"location":"api/rag/pipeline/#rag_toolkit.rag.models.RetrievedChunk","title":"RetrievedChunk  <code>dataclass</code>","text":"Python<pre><code>RetrievedChunk(id: str, text: str, section_path: Optional[str], metadata: Dict[str, str], page_numbers: List[int], source_chunk_id: Optional[str], score: float | None = None)\n</code></pre> <p>Chunk retrieved from the vector/keyword store.</p>"},{"location":"api/rag/pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"api/rag/pipeline/#basic-pipeline","title":"Basic Pipeline","text":"Python<pre><code>from rag_toolkit.rag import RagPipeline\nfrom rag_toolkit.infra import get_ollama_embedding, get_ollama_llm\nfrom rag_toolkit.infra.vectorstores import get_qdrant_service\n\n# Initialize components\nembedding = get_ollama_embedding(model=\"nomic-embed-text\")\nllm = get_ollama_llm(model=\"llama3.2\")\nvector_store = get_qdrant_service(\n    host=\"localhost\",\n    collection_name=\"docs\"\n)\n\n# Create pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n)\n\n# Index documents\npipeline.index_documents([\n    \"RAG combines retrieval and generation.\",\n    \"Vector stores enable semantic search.\",\n])\n\n# Query\nresponse = pipeline.query(\"What is RAG?\")\nprint(response.answer)\nprint(f\"Sources: {len(response.sources)}\")\n</code></pre>"},{"location":"api/rag/pipeline/#advanced-pipeline-with-reranking","title":"Advanced Pipeline with Reranking","text":"Python<pre><code>from rag_toolkit.rag import RagPipeline\nfrom rag_toolkit.rag.rerankers import LLMReranker\n\n# Create reranker\nreranker = LLMReranker(llm_client=llm)\n\n# Create pipeline with reranking\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n    reranker=reranker,  # Add reranking\n    top_k=20,           # Retrieve more candidates\n    rerank_top_k=5,     # Rerank to top 5\n)\n\nresponse = pipeline.query(\"Complex query requiring reranking\")\n</code></pre>"},{"location":"api/rag/pipeline/#custom-query-processing","title":"Custom Query Processing","text":"Python<pre><code># Query with metadata filtering\nresponse = pipeline.query(\n    query=\"What is RAG?\",\n    filter_dict={\"category\": \"tutorial\", \"level\": \"beginner\"}\n)\n\n# Access response details\nfor i, source in enumerate(response.sources, 1):\n    print(f\"Source {i}:\")\n    print(f\"  Text: {source.text[:100]}...\")\n    print(f\"  Score: {source.score:.3f}\")\n    print(f\"  Metadata: {source.metadata}\")\n</code></pre>"},{"location":"api/rag/pipeline/#see-also","title":"See Also","text":"<ul> <li>User Guide: RAG Pipeline - Detailed explanation</li> <li>Examples: Basic RAG - Working example</li> <li>Examples: Advanced Pipeline - Production setup</li> </ul>"},{"location":"development/changelog/","title":"Changelog","text":""},{"location":"development/changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"development/changelog/#010-2025-12-22","title":"0.1.0 - 2025-12-22","text":""},{"location":"development/changelog/#initial-release","title":"\ud83c\udf89 Initial Release","text":"<p>This is the first official release of <code>rag-toolkit</code>, a production-ready library for building Retrieval-Augmented Generation (RAG) systems with Protocol-based architecture.</p>"},{"location":"development/changelog/#added","title":"Added","text":""},{"location":"development/changelog/#core-components","title":"Core Components","text":"<ul> <li>Protocol-based Architecture: Clean interface definitions using Python Protocols (PEP 544)</li> <li><code>EmbeddingClient</code>: Protocol for embedding providers</li> <li><code>LLMClient</code>: Protocol for language model providers</li> <li><code>VectorStoreClient</code>: Protocol for vector database operations</li> <li><code>ChunkLike</code> &amp; <code>TokenChunkLike</code>: Protocols for document chunks</li> </ul>"},{"location":"development/changelog/#chunking-system","title":"Chunking System","text":"<ul> <li>DynamicChunker: Structure-aware chunking based on document heading hierarchy</li> <li>Splits documents at level-1 headings</li> <li>Preserves nested structure (subsections, paragraphs, lists, tables)</li> <li>Configurable heading levels and table inclusion</li> <li> <p>Preamble handling for content before first heading</p> </li> <li> <p>TokenChunker: Token-based chunking with overlap</p> </li> <li>Configurable token limits (max, min, overlap)</li> <li>Pluggable tokenizer support</li> <li>Metadata extraction (tender codes, lot IDs, document types)</li> <li> <p>Two-stage pipeline compatibility (DynamicChunker \u2192 TokenChunker)</p> </li> <li> <p>Concrete Implementations: Dataclass implementations of chunking protocols</p> </li> <li><code>Chunk</code>: Standard document chunk</li> <li><code>TokenChunk</code>: Token-optimized chunk with metadata</li> </ul>"},{"location":"development/changelog/#rag-pipeline","title":"RAG Pipeline","text":"<ul> <li>RagPipeline: End-to-end RAG workflow</li> <li>Query rewriting for better retrieval</li> <li>Vector-based search integration</li> <li>LLM-based reranking</li> <li>Context assembly</li> <li> <p>Answer generation with citations</p> </li> <li> <p>Models: Structured data models</p> </li> <li><code>RagResponse</code>: Generated answer with source citations</li> <li><code>RetrievedChunk</code>: Retrieved document chunk with metadata and scores</li> </ul>"},{"location":"development/changelog/#infrastructure","title":"Infrastructure","text":"<ul> <li>Multi-provider Support (via lazy loading):</li> <li>Ollama (embeddings and LLM)</li> <li>OpenAI (embeddings and LLM)</li> <li> <p>Extensible for custom providers</p> </li> <li> <p>Vector Store Abstraction:</p> </li> <li>Milvus integration (built-in)</li> <li>Protocol-based design for easy provider switching</li> <li>Support for metadata filtering and hybrid search</li> </ul>"},{"location":"development/changelog/#documentation","title":"Documentation","text":"<ul> <li>Comprehensive Sphinx Documentation:</li> <li>User guides for all core concepts</li> <li>API reference with auto-generated docs</li> <li>Practical examples and tutorials</li> <li>Production deployment guides</li> <li>Published at: https://gmottola00.github.io/rag-toolkit/</li> </ul>"},{"location":"development/changelog/#development-tools","title":"Development Tools","text":"<ul> <li>Test Suite: 28 tests with 19% initial coverage</li> <li>Core protocol compliance tests</li> <li>Chunking strategy tests</li> <li>RAG pipeline integration tests</li> <li> <p>Mock implementations for testing</p> </li> <li> <p>CI/CD: GitHub Actions workflows</p> </li> <li>Automated testing across Python 3.11, 3.12, 3.13</li> <li>Documentation builds and deployment</li> <li>Code quality checks (ruff, black, isort, mypy)</li> </ul>"},{"location":"development/changelog/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Python: 3.11, 3.12, 3.13</li> <li>Operating Systems: Linux, macOS, Windows (via WSL)</li> <li>Vector Databases: Milvus 2.3+</li> <li>LLM Providers: Ollama, OpenAI</li> </ul>"},{"location":"development/changelog/#dependencies","title":"Dependencies","text":"<ul> <li>Core: <code>pydantic&gt;=2.0.0</code>, <code>pydantic-settings&gt;=2.0.0</code>, <code>pymilvus&gt;=2.3.0</code></li> <li>Optional: <code>ollama</code>, <code>openai</code>, <code>pymupdf</code>, <code>python-docx</code>, <code>easyocr</code>, <code>langdetect</code></li> </ul>"},{"location":"development/changelog/#known-limitations","title":"Known Limitations","text":"<ul> <li>Test coverage at 19% (focused on core components)</li> <li>Milvus is currently the only built-in vector store (more coming soon)</li> <li>Document parsers (PDF, DOCX) included but minimally tested</li> <li>OCR support experimental</li> </ul>"},{"location":"development/changelog/#breaking-changes","title":"Breaking Changes","text":"<p>None - initial release.</p>"},{"location":"development/changelog/#migration-guide","title":"Migration Guide","text":"<p>Not applicable - initial release.</p>"},{"location":"development/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"development/changelog/#added_1","title":"Added","text":""},{"location":"development/changelog/#vector-store-integrations-2025-12-23-to-2025-12-26","title":"Vector Store Integrations (2025-12-23 to 2025-12-26)","text":"<ul> <li>Qdrant Integration: Full implementation with connection management, collection operations, and hybrid search</li> <li>ChromaDB Integration: Complete support for local and client-server deployments</li> <li>Unified Testing Framework: Comprehensive test suite for all vector store implementations</li> <li>Docker Compose Setup: Development environment with Milvus and Qdrant containers</li> <li>Benchmark Framework: Performance comparison tool across vector stores (Milvus, Qdrant, ChromaDB)</li> <li>Query latency measurement</li> <li>Indexing throughput analysis</li> <li>Memory usage profiling</li> <li>HTML report generation</li> </ul>"},{"location":"development/changelog/#migration-tools-2026-01-02","title":"Migration Tools (2026-01-02)","text":"<ul> <li>VectorStoreMigrator: Production-ready migration engine for vector data transfers across stores (Milvus, Qdrant, ChromaDB)</li> <li>Advanced Features: Filtered migration, dry-run mode, retry logic with exponential backoff</li> <li>Models &amp; Exceptions: Complete migration lifecycle support with validation and error handling</li> <li>Documentation: Comprehensive guide with production examples and roadmap</li> <li>Test Coverage: 60 tests (100% pass rate)</li> </ul>"},{"location":"development/changelog/#changed","title":"Changed","text":"<ul> <li>Documentation: Updated roadmap reflecting completed vector store integrations</li> <li>Infrastructure: Enhanced Docker setup for local development and testing</li> </ul>"},{"location":"development/changelog/#planned-features-next-releases","title":"Planned Features (Next Releases)","text":""},{"location":"development/changelog/#phase-2-priority-2","title":"Phase 2 Priority 2","text":"<ul> <li>Incremental Migration: Checkpoint-based resume capability for large datasets</li> <li>Schema Mapping: Field transformation and mapping between different vector stores</li> </ul>"},{"location":"development/changelog/#phase-3","title":"Phase 3","text":"<ul> <li>CLI Tool: Command-line interface with YAML configuration support</li> <li>Parallel Migration: Multi-threaded/multi-process batch processing</li> <li>Metrics &amp; Observability: Prometheus metrics and OpenTelemetry tracing</li> </ul>"},{"location":"development/changelog/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Enhanced document parsing with better table extraction</li> <li>Query expansion strategies</li> <li>Caching layer for embeddings and LLM responses</li> <li>Async support for all I/O operations</li> <li>Evaluation framework for RAG quality metrics</li> <li>Examples for production deployments (Kubernetes, cloud platforms)</li> <li>Zero-downtime migration with dual-write pattern</li> <li>Data quality validation (embedding drift detection)</li> </ul>"},{"location":"development/contributing/","title":"Contributing","text":""},{"location":"development/contributing/#contributing-to-rag-toolkit","title":"Contributing to rag-toolkit","text":"<p>Thank you for your interest in contributing to rag-toolkit! This document provides guidelines and instructions for contributors.</p>"},{"location":"development/contributing/#development-philosophy","title":"\ud83c\udfaf Development Philosophy","text":"<ul> <li>Clean code: Readable, maintainable, well-documented</li> <li>Type safety: Full type hints, mypy compliance</li> <li>Test coverage: Aim for &gt;80% coverage</li> <li>Protocol-based: Favor composition over inheritance</li> <li>Performance: Optimize for production use cases</li> </ul>"},{"location":"development/contributing/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"development/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"Bash<pre><code>git clone https://github.com/YOUR_USERNAME/rag-toolkit.git\ncd rag-toolkit\n</code></pre>"},{"location":"development/contributing/#2-setup-development-environment","title":"2. Setup Development Environment","text":"Bash<pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev,all]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"development/contributing/#3-create-a-branch","title":"3. Create a Branch","text":"Bash<pre><code>git checkout -b feature/my-new-feature\n# or\ngit checkout -b fix/issue-123\n</code></pre>"},{"location":"development/contributing/#code-standards","title":"\ud83d\udcdd Code Standards","text":""},{"location":"development/contributing/#style-guide","title":"Style Guide","text":"<p>We follow PEP 8 with these tools:</p> Bash<pre><code># Format code\nblack src/rag_toolkit tests\n\n# Sort imports\nisort src/rag_toolkit tests\n\n# Lint\nruff check src/rag_toolkit tests\n\n# Type check\nmypy src/rag_toolkit\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>All public APIs must have complete type hints:</p> Python<pre><code># \u2705 Good\ndef embed(self, text: str) -&gt; list[float]:\n    \"\"\"Convert text to embedding vector.\"\"\"\n    return self._model.encode(text).tolist()\n\n# \u274c Bad\ndef embed(self, text):\n    return self._model.encode(text).tolist()\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> Python<pre><code>def search(\n    self,\n    query: str,\n    top_k: int = 10,\n    filters: dict[str, Any] | None = None\n) -&gt; list[SearchResult]:\n    \"\"\"\n    Search for similar documents.\n\n    Args:\n        query: Search query text\n        top_k: Number of results to return\n        filters: Optional metadata filters\n\n    Returns:\n        List of search results ordered by relevance\n\n    Raises:\n        ValueError: If top_k &lt;= 0\n\n    Example:\n        &gt;&gt;&gt; results = searcher.search(\"RAG tutorial\", top_k=5)\n        &gt;&gt;&gt; for result in results:\n        ...     print(result.text)\n    \"\"\"\n</code></pre>"},{"location":"development/contributing/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"Bash<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=rag_toolkit --cov-report=html\n\n# Run specific test file\npytest tests/core/test_embedding.py\n\n# Run specific test\npytest tests/core/test_embedding.py::test_embed_batch\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"Python<pre><code>import pytest\nfrom rag_toolkit.core import EmbeddingClient\n\ndef test_embedding_protocol():\n    \"\"\"Test that custom embedding implements protocol.\"\"\"\n\n    class MockEmbedding:\n        def embed(self, text: str) -&gt; list[float]:\n            return [0.1] * 384\n\n        @property\n        def model_name(self) -&gt; str:\n            return \"mock\"\n\n    # Should work with Protocol\n    client: EmbeddingClient = MockEmbedding()\n    vector = client.embed(\"test\")\n\n    assert len(vector) == 384\n    assert all(isinstance(x, float) for x in vector)\n</code></pre>"},{"location":"development/contributing/#mocking-external-services","title":"Mocking External Services","text":"Python<pre><code>@pytest.fixture\ndef mock_ollama():\n    \"\"\"Mock Ollama API responses.\"\"\"\n    with patch(\"rag_toolkit.infra.embedding.ollama.requests.post\") as mock:\n        mock.return_value.json.return_value = {\n            \"embedding\": [0.1] * 768\n        }\n        yield mock\n\ndef test_ollama_embedding(mock_ollama):\n    \"\"\"Test Ollama embedding with mocked API.\"\"\"\n    embedding = OllamaEmbedding(model=\"nomic-embed-text\")\n    vector = embedding.embed(\"test\")\n\n    assert len(vector) == 768\n    mock_ollama.assert_called_once()\n</code></pre>"},{"location":"development/contributing/#adding-new-features","title":"\ud83d\udd27 Adding New Features","text":""},{"location":"development/contributing/#adding-a-new-vector-store","title":"Adding a New Vector Store","text":"<ol> <li>Create Protocol implementation in <code>src/rag_toolkit/infra/vectorstore/</code>:</li> </ol> Python<pre><code># src/rag_toolkit/infra/vectorstore/pinecone/client.py\nfrom rag_toolkit.core import VectorStoreClient, SearchResult\n\nclass PineconeVectorStore:\n    \"\"\"Pinecone vector store implementation.\"\"\"\n\n    def __init__(self, api_key: str, environment: str):\n        import pinecone\n        pinecone.init(api_key=api_key, environment=environment)\n\n    def create_collection(self, name: str, dimension: int, **kwargs):\n        # Implementation\n        pass\n\n    # Implement all Protocol methods...\n</code></pre> <ol> <li>Add tests in <code>tests/infra/vectorstore/</code>:</li> </ol> Python<pre><code>def test_pinecone_create_collection(pinecone_store):\n    pinecone_store.create_collection(\"test\", dimension=384)\n    assert pinecone_store.collection_exists(\"test\")\n</code></pre> <ol> <li> <p>Update documentation in <code>docs/</code> and <code>README.md</code></p> </li> <li> <p>Add to optional dependencies:</p> </li> </ol> TOML<pre><code>[project.optional-dependencies]\npinecone = [\"pinecone-client&gt;=2.0.0\"]\n</code></pre>"},{"location":"development/contributing/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<p>Similar process in <code>src/rag_toolkit/infra/llm/</code>:</p> Python<pre><code>from rag_toolkit.core import LLMClient\n\nclass AnthropicLLMClient:\n    \"\"\"Anthropic Claude LLM client.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"claude-3-opus\"):\n        import anthropic\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self._model = model\n\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        response = self.client.messages.create(\n            model=self._model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            **kwargs\n        )\n        return response.content[0].text\n\n    @property\n    def model_name(self) -&gt; str:\n        return self._model\n</code></pre>"},{"location":"development/contributing/#documentation","title":"\ud83d\udcda Documentation","text":""},{"location":"development/contributing/#building-docs-locally","title":"Building Docs Locally","text":"Bash<pre><code>cd docs\nmake html\nopen build/html/index.html  # macOS\n# or\nxdg-open build/html/index.html  # Linux\n</code></pre>"},{"location":"development/contributing/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>All public APIs must be documented</li> <li>Include usage examples</li> <li>Link to related concepts</li> <li>Keep examples up-to-date</li> </ul>"},{"location":"development/contributing/#reporting-bugs","title":"\ud83d\udc1b Reporting Bugs","text":"<p>Use GitHub Issues with this template:</p> Markdown<pre><code>**Describe the bug**\nA clear description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce:\n1. Install rag-toolkit with '...'\n2. Run code '...'\n3. See error\n\n**Expected behavior**\nWhat you expected to happen.\n\n**Environment**\n- OS: [e.g., Ubuntu 22.04]\n- Python version: [e.g., 3.11.5]\n- rag-toolkit version: [e.g., 0.1.0]\n- Relevant dependencies: [e.g., pymilvus 2.3.0]\n\n**Additional context**\nAny other context about the problem.\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"\ud83c\udf89 Pull Request Process","text":"<ol> <li>Update tests: Ensure all tests pass</li> <li>Update docs: Add/update relevant documentation</li> <li>Update CHANGELOG: Add entry under \"Unreleased\"</li> <li>Run checks: <code>pytest &amp;&amp; mypy src/rag_toolkit &amp;&amp; ruff check .</code></li> <li>Create PR: Use descriptive title and description</li> </ol>"},{"location":"development/contributing/#pr-title-format","title":"PR Title Format","text":"Text Only<pre><code>type(scope): brief description\n\nExamples:\nfeat(vectorstore): add Pinecone support\nfix(embedding): handle empty text input\ndocs(readme): update installation instructions\ntest(rag): add integration tests for pipeline\n</code></pre>"},{"location":"development/contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Tests pass locally (<code>pytest</code>)</li> <li> Type checks pass (<code>mypy src/rag_toolkit</code>)</li> <li> Linting passes (<code>ruff check .</code>)</li> <li> Code formatted (<code>black .</code> and <code>isort .</code>)</li> <li> Documentation updated</li> <li> CHANGELOG.md updated</li> <li> PR description explains changes</li> </ul>"},{"location":"development/contributing/#development-workflow","title":"\ud83d\udd04 Development Workflow","text":"<ol> <li>Pick an issue or feature</li> <li>Create branch</li> <li>Make changes with tests</li> <li>Run all checks</li> <li>Push and create PR</li> <li>Address review feedback</li> <li>Merge!</li> </ol>"},{"location":"development/contributing/#code-of-conduct","title":"\ud83e\udd1d Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Provide constructive feedback</li> <li>Focus on the code, not the person</li> <li>Help newcomers learn</li> </ul>"},{"location":"development/contributing/#questions","title":"\ud83d\udcde Questions?","text":"<ul> <li>Discussions: GitHub Discussions</li> <li>Issues: GitHub Issues</li> </ul> <p>Thank you for contributing to rag-toolkit! \ud83d\ude80</p>"},{"location":"development/roadmap/","title":"Roadmap","text":""},{"location":"development/roadmap/#roadmap","title":"Roadmap","text":"<p>The future development plan for rag-toolkit.</p>"},{"location":"development/roadmap/#current-status-v010","title":"Current Status: v0.1.0","text":"<p>\u2705 Core architecture with Protocol-based design \u2705 Ollama and OpenAI integrations \u2705 Milvus vector store \u2705 Basic RAG pipeline \u2705 Documentation and examples  </p>"},{"location":"development/roadmap/#version-020-q1-2025","title":"Version 0.2.0 (Q1 2025)","text":"<p>Focus: Multi-VectorStore Support</p>"},{"location":"development/roadmap/#planned-features","title":"Planned Features","text":"<p>Vector Stores - [ ] Pinecone integration - [X] Qdrant integration - [X] ChromaDB integration - [ ] Weaviate integration (community contribution)</p> <p>Improvements - [X] Unified vector store testing suite - [X] Performance benchmarks across stores - [ ] Migration tools between vector stores - [ ] Vector store selection guide</p> <p>Documentation - [ ] Vector store comparison guide - [ ] Migration tutorials - [ ] Performance optimization guide</p>"},{"location":"development/roadmap/#version-030-q2-2025","title":"Version 0.3.0 (Q2 2025)","text":"<p>Focus: Enhanced RAG Techniques</p>"},{"location":"development/roadmap/#planned-features_1","title":"Planned Features","text":"<p>Retrieval Enhancements - [ ] Cross-encoder reranking (Cohere, Jina) - [ ] Reciprocal Rank Fusion (RRF) - [ ] Query decomposition - [ ] Multi-query retrieval with fusion - [ ] Parent-child document retrieval</p> <p>Chunking Strategies - [ ] Semantic chunking - [ ] Sliding window with overlap - [ ] Hierarchical chunking - [ ] Custom chunking callbacks</p> <p>Context Management - [ ] Smart context pruning - [ ] Context caching - [ ] Conversation memory - [ ] Multi-turn dialogue support</p> <p>Documentation - [ ] Advanced RAG techniques guide - [ ] Reranking strategies comparison - [ ] Context management best practices</p>"},{"location":"development/roadmap/#version-040-q3-2025","title":"Version 0.4.0 (Q3 2025)","text":"<p>Focus: Production Features</p>"},{"location":"development/roadmap/#planned-features_2","title":"Planned Features","text":"<p>Observability - [ ] Built-in tracing with OpenTelemetry - [ ] Performance metrics - [ ] Cost tracking - [ ] Query analytics dashboard</p> <p>Optimization - [ ] Response caching - [ ] Batch processing optimization - [ ] Streaming responses - [ ] GPU acceleration support</p> <p>Evaluation - [ ] Built-in evaluation framework - [ ] Retrieval metrics (Precision@K, MRR, NDCG) - [ ] Generation metrics (BLEU, ROUGE, BERTScore) - [ ] A/B testing support</p> <p>Deployment - [ ] Docker compose templates - [ ] Kubernetes manifests - [ ] Serverless deployment guides - [ ] Load balancing strategies</p> <p>Documentation - [ ] Production deployment guide - [ ] Monitoring and observability - [ ] Performance tuning - [ ] Cost optimization</p>"},{"location":"development/roadmap/#version-050-q4-2025","title":"Version 0.5.0 (Q4 2025)","text":"<p>Focus: Advanced Features</p>"},{"location":"development/roadmap/#planned-features_3","title":"Planned Features","text":"<p>Multi-Modal RAG - [ ] Image embeddings (CLIP) - [ ] Document image processing - [ ] Multi-modal retrieval - [ ] Vision-language models integration</p> <p>Graph RAG - [ ] Knowledge graph integration - [ ] Entity extraction - [ ] Relationship mapping - [ ] Graph-based retrieval</p> <p>Advanced LLM Features - [ ] Function calling support - [ ] Tool use integration - [ ] Agent-based RAG - [ ] Self-querying retrieval</p> <p>Security - [ ] API key management - [ ] Rate limiting - [ ] User authentication - [ ] Data privacy controls</p>"},{"location":"development/roadmap/#long-term-vision","title":"Long-term Vision","text":""},{"location":"development/roadmap/#community-features","title":"Community Features","text":"<ul> <li>Plugin system for custom components</li> <li>Community vector store adapters</li> <li>Shared embedding models</li> <li>Pre-built RAG templates</li> </ul>"},{"location":"development/roadmap/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Multi-tenancy support</li> <li>RBAC (Role-Based Access Control)</li> <li>Audit logging</li> <li>Compliance tools (GDPR, SOC2)</li> </ul>"},{"location":"development/roadmap/#research-integration","title":"Research Integration","text":"<ul> <li>Latest RAG techniques from papers</li> <li>Experimental features branch</li> <li>Research partnership program</li> </ul>"},{"location":"development/roadmap/#how-to-contribute","title":"How to Contribute","text":"<p>We welcome contributions! Here's how you can help:</p>"},{"location":"development/roadmap/#priority-areas","title":"Priority Areas","text":"<ol> <li>Vector Store Implementations</li> <li>Implement new vector store adapters</li> <li> <p>See <code>src/rag_toolkit/core/vectorstore.py</code> for the protocol</p> </li> <li> <p>LLM Provider Integrations</p> </li> <li>Add support for new LLM providers</li> <li> <p>See <code>src/rag_toolkit/core/llm.py</code> for the protocol</p> </li> <li> <p>Embedding Models</p> </li> <li>Integrate new embedding models</li> <li> <p>See <code>src/rag_toolkit/core/embedding.py</code> for the protocol</p> </li> <li> <p>Examples and Tutorials</p> </li> <li>Write tutorials for specific use cases</li> <li>Share your RAG applications</li> <li> <p>Add to <code>examples/</code> directory</p> </li> <li> <p>Documentation</p> </li> <li>Improve existing docs</li> <li>Add missing sections</li> <li>Translate to other languages</li> </ol>"},{"location":"development/roadmap/#feature-requests","title":"Feature Requests","text":"<p>Have an idea? Open an issue with: - Clear description of the feature - Use case and benefits - Proposed implementation (optional) - Willingness to contribute</p>"},{"location":"development/roadmap/#voting","title":"Voting","text":"<p>Check our GitHub Discussions to: - Vote on proposed features - Suggest new features - Share your use cases</p>"},{"location":"development/roadmap/#release-schedule","title":"Release Schedule","text":"<ul> <li>Minor versions (0.x.0): Quarterly</li> <li>Patch versions (0.x.y): As needed for bug fixes</li> <li>Major version (1.0.0): When API is stable and battle-tested</li> </ul>"},{"location":"development/roadmap/#stability-guarantees","title":"Stability Guarantees","text":""},{"location":"development/roadmap/#current-0xx","title":"Current (0.x.x)","text":"<ul> <li>\u26a0\ufe0f API may change between minor versions</li> <li>\u2705 We'll provide migration guides</li> <li>\u2705 Deprecation warnings before breaking changes</li> </ul>"},{"location":"development/roadmap/#future-100","title":"Future (1.0.0+)","text":"<ul> <li>\u2705 Semantic versioning</li> <li>\u2705 Stable API with deprecation cycles</li> <li>\u2705 LTS (Long Term Support) releases</li> </ul>"},{"location":"development/roadmap/#stay-updated","title":"Stay Updated","text":"<ul> <li>\ud83d\udce7 Mailing list (coming soon)</li> <li>\ud83d\udc26 Twitter (coming soon)</li> <li>\ud83d\udcac Discord community (coming soon)</li> <li>\u2b50 Star on GitHub</li> </ul> <p>This roadmap is subject to change based on community feedback and priorities.</p> <p>Last updated: December 20, 2024</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>Real-World Applications</p> <p>Practical examples showing how to build production-ready RAG applications with rag-toolkit.</p>"},{"location":"examples/#featured-examples","title":"Featured Examples","text":"<ul> <li> <p> Basic RAG</p> <p>Build your first RAG application with document indexing and querying.</p> <p> Get Started</p> </li> <li> <p> Custom Vector Store</p> <p>Implement a custom vector store using ChromaDB or other databases.</p> <p> Learn More</p> </li> <li> <p> Hybrid Search</p> <p>Combine vector search with keyword search for better results.</p> <p> Explore</p> </li> <li> <p> Advanced Pipeline</p> <p>Build production-ready pipelines with reranking, query rewriting, and more.</p> <p> Deep Dive</p> </li> <li> <p> Production Setup</p> <p>Deploy rag-toolkit in production with monitoring, scaling, and best practices.</p> <p> Deploy</p> </li> </ul>"},{"location":"examples/#example-categories","title":"Example Categories","text":"\ud83c\udfaf Getting Started\ud83c\udfa8 Customization\u26a1 Advanced Features\ud83d\ude80 Production <ul> <li> <p> Basic RAG</p> <p>Simple document Q&amp;A to get started</p> </li> <li> <p> PDF Processing</p> <p>Parse and index PDF files</p> </li> <li> <p> Multiple Collections</p> <p>Organize documents efficiently</p> </li> </ul> <ul> <li> <p> Custom Vector Store</p> <p>Implement your own storage backend</p> </li> <li> <p> Custom Embeddings</p> <p>Use different embedding models</p> </li> <li> <p> Custom LLMs</p> <p>Integrate new LLM providers</p> </li> </ul> <ul> <li> <p> Hybrid Search</p> <p>Vector + keyword search combination</p> </li> <li> <p> Query Rewriting</p> <p>Improve retrieval accuracy</p> </li> <li> <p> Reranking</p> <p>Better result ordering</p> </li> </ul> <ul> <li> <p> Monitoring</p> <p>Track performance metrics</p> </li> <li> <p> Scaling</p> <p>Handle high traffic loads</p> </li> <li> <p> Caching</p> <p>Optimize response times</p> </li> </ul>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>Quick Start</p> <p>All examples are ready to run from the <code>examples/</code> directory.</p> Clone &amp; InstallRun ExampleJupyter Notebook setup.sh<pre><code># Clone the repository\ngit clone https://github.com/gmottola00/rag-toolkit.git\ncd rag-toolkit\n\n# Install with all dependencies\npip install -e \".[all]\"\n</code></pre> run_example.sh<pre><code># Run basic RAG example\npython examples/basic_rag.py\n\n# Run with custom configuration\npython examples/basic_rag.py --config my_config.yaml\n</code></pre> jupyter_setup.sh<pre><code># Install Jupyter\npip install jupyter\n\n# Start notebook\njupyter notebook examples/\n</code></pre>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li> <p> User Guide</p> <p>Comprehensive documentation and tutorials</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation</p> </li> <li> <p> Discussions</p> <p>Ask questions and share ideas</p> </li> <li> <p> Report Issues</p> <p>Found a bug? Let us know!</p> </li> </ul>"},{"location":"examples/advanced_pipeline/","title":"Advanced Pipeline","text":""},{"location":"examples/advanced_pipeline/#advanced-rag-pipeline","title":"Advanced RAG Pipeline","text":"<p>Production-Ready Features</p> <p>Learn how to build a production-ready RAG pipeline with advanced features like query rewriting, reranking, conversational memory, and multi-query strategies.</p>"},{"location":"examples/advanced_pipeline/#complete-advanced-pipeline","title":"Complete Advanced Pipeline","text":""},{"location":"examples/advanced_pipeline/#full-featured-implementation","title":"Full-Featured Implementation","text":"<p>All Features Enabled</p> <p>This implementation includes all advanced RAG capabilities.</p> advanced_pipeline.py<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\nfrom rag_toolkit.infra.llm import OpenAILLM\nfrom rag_toolkit.core.vectorstore import MilvusVectorStore\nfrom rag_toolkit.core.reranker import CrossEncoderReranker\nfrom rag_toolkit.core.types import SearchResult\nimport asyncio\n\nclass AdvancedRagPipeline:\n    \"\"\"Advanced RAG pipeline with all features.\"\"\"\n\n    def __init__(\n        self,\n        embedding_client: OpenAIEmbedding,\n        vector_store: MilvusVectorStore,\n        llm_client: OpenAILLM,\n        reranker: CrossEncoderReranker | None = None,\n        enable_query_rewriting: bool = True,\n        enable_multi_query: bool = False,\n        enable_conversation: bool = True,\n    ):\n        self.embedding = embedding_client\n        self.vector_store = vector_store\n        self.llm = llm_client\n        self.reranker = reranker\n\n        self.enable_query_rewriting = enable_query_rewriting\n        self.enable_multi_query = enable_multi_query\n        self.enable_conversation = enable_conversation\n\n        # Conversation history\n        self.conversation_history: list[dict] = []\n\n    async def query(\n        self,\n        query: str,\n        limit: int = 5,\n        filter: dict | None = None,\n    ):\n        \"\"\"Advanced query with all features.\"\"\"\n        # 1. Query rewriting\n        if self.enable_query_rewriting:\n            rewritten_query = await self._rewrite_query(query)\n        else:\n            rewritten_query = query\n\n        # 2. Multi-query generation\n        if self.enable_multi_query:\n            queries = await self._generate_multi_queries(rewritten_query)\n        else:\n            queries = [rewritten_query]\n\n        # 3. Retrieve documents\n        all_results = await self._retrieve_multi_query(\n            queries=queries,\n            limit=limit * 2,\n            filter=filter\n        )\n\n        # 4. Rerank\n        if self.reranker:\n            reranked = await self.reranker.rerank(\n                query=query,\n                documents=all_results,\n                top_k=limit\n            )\n        else:\n            reranked = all_results[:limit]\n\n        # 5. Generate answer\n        answer = await self._generate_answer(\n            query=query,\n            context=reranked\n        )\n\n        # 6. Update conversation history\n        if self.enable_conversation:\n            self._update_history(query, answer)\n\n        return {\n            \"answer\": answer,\n            \"sources\": reranked,\n            \"original_query\": query,\n            \"rewritten_query\": rewritten_query\n        }\n\n# Usage\npipeline = AdvancedRagPipeline(\n    embedding_client=OpenAIEmbedding(),\n    vector_store=MilvusVectorStore(\n        collection_name=\"docs\",\n        dimension=1536\n    ),\n    llm_client=OpenAILLM(model=\"gpt-4-turbo\"),\n    reranker=CrossEncoderReranker(),\n    enable_query_rewriting=True,\n    enable_conversation=True,\n)\n\nresult = await pipeline.query(\"What is machine learning?\")\nprint(result[\"answer\"])\n</code></pre>"},{"location":"examples/advanced_pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Enable Query Rewriting - Improves retrieval quality</li> <li>Use Reranking - Better result ranking</li> <li>Implement Caching - Faster repeated queries</li> <li>Monitor Performance - Track metrics</li> <li>Test Thoroughly - Evaluate on real queries</li> </ol>"},{"location":"examples/advanced_pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Production Setup - Deploy to production</li> <li>Reranking Guide - Improve ranking</li> <li>RAG Pipeline Guide - Deep dive</li> </ul>"},{"location":"examples/basic_rag/","title":"Basic RAG","text":""},{"location":"examples/basic_rag/#basic-rag-example","title":"Basic RAG Example","text":"<p>Your First RAG Application</p> <p>Learn how to build a simple RAG application for document question-answering in minutes.</p>"},{"location":"examples/basic_rag/#complete-example","title":"Complete Example","text":"<p>Full Working Code</p> <p>Copy and run this complete example to see RAG in action.</p> basic_rag.py<pre><code>\"\"\"\nBasic RAG Pipeline Example\n===========================\n\nThis example demonstrates:\n1. Setting up embedding and LLM clients\n2. Creating a vector store\n3. Indexing documents\n4. Querying with context\n\"\"\"\n\nfrom rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding.ollama import OllamaEmbedding\nfrom rag_toolkit.infra.llm.ollama import OllamaLLMClient\nfrom rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\n\ndef main():\n    # Step 1: Initialize components\n    print(\"\ud83d\ude80 Initializing components...\")\n\n    embedding = OllamaEmbedding(\n        base_url=\"http://localhost:11434\",\n        model=\"nomic-embed-text\",\n    )\n\n    llm = OllamaLLMClient(\n        base_url=\"http://localhost:11434\",\n        model=\"llama2\",\n    )\n\n    vector_store = MilvusVectorStore(\n        host=\"localhost\",\n        port=\"19530\",\n        collection_name=\"basic_rag_docs\",\n    )\n\n    # Step 2: Create RAG pipeline\n    pipeline = RagPipeline(\n        embedding_client=embedding,\n        llm_client=llm,\n        vector_store=vector_store,\n        chunk_size=512,\n        chunk_overlap=50,\n    )\n\n    # Step 3: Prepare documents\n    documents = [\n        \"\"\"\n        RAG (Retrieval-Augmented Generation) is a technique that enhances \n        large language models by retrieving relevant information from a \n        knowledge base before generating responses. This approach reduces \n        hallucinations and provides more accurate, contextual answers.\n        \"\"\",\n        \"\"\"\n        Vector stores are databases optimized for storing and searching \n        high-dimensional vectors. They enable semantic search by finding \n        vectors that are mathematically similar to a query vector, allowing \n        retrieval of semantically related content.\n        \"\"\",\n        \"\"\"\n        Embeddings are numerical representations of text that capture \n        semantic meaning. Similar texts have similar embedding vectors, \n        enabling machines to understand and compare text based on meaning \n        rather than just keywords.\n        \"\"\",\n        \"\"\"\n        Chunking is the process of breaking down large documents into \n        smaller, manageable pieces. This is important for RAG systems \n        because it allows more precise retrieval and helps manage context \n        window limitations of language models.\n        \"\"\",\n    ]\n\n    # Step 4: Index documents\n    print(\"\\n\ud83d\udcda Indexing documents...\")\n    pipeline.index_documents(\n        documents=documents,\n        metadata=[\n            {\"topic\": \"RAG\", \"source\": \"tutorial\"},\n            {\"topic\": \"Vector Stores\", \"source\": \"tutorial\"},\n            {\"topic\": \"Embeddings\", \"source\": \"tutorial\"},\n            {\"topic\": \"Chunking\", \"source\": \"tutorial\"},\n        ],\n    )\n    print(\"\u2705 Indexing complete!\")\n\n    # Step 5: Query the system\n    queries = [\n        \"What is RAG and why is it useful?\",\n        \"How do vector stores work?\",\n        \"Explain the relationship between embeddings and semantic search\",\n    ]\n\n    print(\"\\n\ud83d\udd0d Running queries...\\n\")\n    for query in queries:\n        print(f\"Q: {query}\")\n        response = pipeline.query(query, top_k=2)\n        print(f\"A: {response.answer}\\n\")\n        print(f\"Sources used: {len(response.sources)}\")\n        for i, source in enumerate(response.sources, 1):\n            print(f\"  {i}. {source.text[:100]}...\")\n        print(\"-\" * 80 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/basic_rag/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":""},{"location":"examples/basic_rag/#component-setup","title":"Component Setup","text":"<p>Initialize Core Components</p> <p>Set up the embedding model, LLM, and vector store.</p> EmbeddingLLMVector Store setup_embedding.py<pre><code># Embedding: Converts text to vectors\nembedding = OllamaEmbedding(\n    model=\"nomic-embed-text\",  # 768-dimensional embeddings\n    base_url=\"http://localhost:11434\"\n)\n</code></pre> setup_llm.py<pre><code># LLM: Generates responses\nllm = OllamaLLMClient(\n    model=\"llama2\",  # or \"mistral\", \"mixtral\", etc.\n    base_url=\"http://localhost:11434\"\n)\n</code></pre> setup_vectorstore.py<pre><code># Vector Store: Stores and searches embeddings\nvector_store = MilvusVectorStore(\n    collection_name=\"my_documents\",\n    host=\"localhost\",\n    port=\"19530\"\n)\n</code></pre>"},{"location":"examples/basic_rag/#pipeline-creation","title":"Pipeline Creation","text":"<p>Configure Your Pipeline</p> <p>Combine components into a unified RAG pipeline.</p> create_pipeline.py<pre><code>pipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n    chunk_size=512,        # Chunk documents into 512 chars\n    chunk_overlap=50,      # 50 char overlap between chunks\n)\n</code></pre>"},{"location":"examples/basic_rag/#document-indexing","title":"Document Indexing","text":"<p>Add Your Documents</p> <p>Index documents with optional metadata for better organization.</p> index_documents.py<pre><code>documents = [\n    \"Your document text here...\",\n    \"Another document...\",\n]\n\nmetadata = [\n    {\"source\": \"doc1.pdf\", \"page\": 1},\n    {\"source\": \"doc2.pdf\", \"page\": 1},\n]\n\npipeline.index_documents(\n    documents=documents,\n    metadata=metadata,  # Optional but recommended\n)\n</code></pre>"},{"location":"examples/basic_rag/#querying","title":"Querying","text":"<p>Ask Questions</p> <p>Query your indexed documents and get contextual answers.</p> query.py<pre><code>response = pipeline.query(\n    \"What is RAG?\",\n    top_k=5,  # Retrieve top 5 most relevant chunks\n)\n\nprint(response.answer)      # Generated answer\nprint(response.sources)     # Retrieved chunks used\nprint(response.metadata)    # Additional info\n</code></pre>"},{"location":"examples/basic_rag/#pdf-documents","title":"PDF Documents","text":"<p>Process PDF Files</p> <p>Extract text from PDFs and index them directly.</p> pdf_processing.py<pre><code>from rag_toolkit.infra.parsers.pdf import PDFParser\n\n# Parse PDF\nparser = PDFParser()\ntext = parser.parse(\"document.pdf\")\n\n# Index\npipeline.index_documents([text])\n\n# Query\nresponse = pipeline.query(\"What does the document say about...?\")\n</code></pre>"},{"location":"examples/basic_rag/#multiple-collections","title":"Multiple Collections","text":"<p>Organize Documents</p> <p>Separate documents into different collections for better organization.</p> multiple_collections.py<pre><code># Technical docs collection\ntech_store = MilvusVectorStore(collection_name=\"tech_docs\")\ntech_pipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=tech_store,\n)\n\n# Marketing docs collection\nmarketing_store = MilvusVectorStore(collection_name=\"marketing_docs\")\nmarketing_pipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=marketing_store,\n)\n\n# Index separately\ntech_pipeline.index_documents(technical_documents)\nmarketing_pipeline.index_documents(marketing_documents)\n\n# Query specific collections\ntech_response = tech_pipeline.query(\"How does the API work?\")\nmarketing_response = marketing_pipeline.query(\"What's our value proposition?\")\n</code></pre>"},{"location":"examples/basic_rag/#filtering-by-metadata","title":"Filtering by Metadata","text":"<p>Smart Filtering</p> <p>Use metadata to filter search results and improve relevance.</p> metadata_filtering.py<pre><code># Index with metadata\npipeline.index_documents(\n    documents=documents,\n    metadata=[\n        {\"category\": \"api\", \"version\": \"2.0\"},\n        {\"category\": \"tutorial\", \"version\": \"2.0\"},\n        {\"category\": \"api\", \"version\": \"1.0\"},\n    ],\n)\n\n# Query with filters\nresponse = pipeline.query(\n    \"How to use the API?\",\n    filters={\"category\": \"api\", \"version\": \"2.0\"},\n)\n</code></pre>"},{"location":"examples/basic_rag/#async-support","title":"Async Support","text":"<p>Concurrent Operations</p> <p>Use async methods for better performance with multiple queries.</p> Single QueryBatch Queries async_single.py<pre><code>import asyncio\n\nasync def async_query_example():\n    response = await pipeline.aquery(\"What is RAG?\")\n    return response\n</code></pre> async_batch.py<pre><code>async def batch_query():\n    queries = [\n        \"What is RAG?\",\n        \"How do embeddings work?\",\n        \"Explain vector stores\",\n    ]\n\n    tasks = [pipeline.aquery(q) for q in queries]\n    responses = await asyncio.gather(*tasks)\n\n    return responses\n\n# Run\nresponses = asyncio.run(batch_query())\n</code></pre>"},{"location":"examples/basic_rag/#error-handling","title":"Error Handling","text":"<p>Handle Errors Gracefully</p> <p>Implement proper error handling for production applications.</p> error_handling.py<pre><code>try:\n    response = pipeline.query(\"What is RAG?\")\nexcept ConnectionError as e:\n    print(f\"Failed to connect to services: {e}\")\nexcept ValueError as e:\n    print(f\"Invalid input: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"examples/basic_rag/#complete-working-example","title":"Complete Working Example","text":"<p>Production-Ready Application</p> <p>Save this as <code>my_rag_app.py</code> for a complete interactive RAG application.</p> my_rag_app.py<pre><code>#!/usr/bin/env python3\n\"\"\"Complete RAG application.\"\"\"\n\nfrom rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding.ollama import OllamaEmbedding\nfrom rag_toolkit.infra.llm.ollama import OllamaLLMClient\nfrom rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\n\ndef setup_pipeline():\n    \"\"\"Initialize and return configured pipeline.\"\"\"\n    embedding = OllamaEmbedding(model=\"nomic-embed-text\")\n    llm = OllamaLLMClient(model=\"llama2\")\n    store = MilvusVectorStore(collection_name=\"docs\")\n\n    return RagPipeline(\n        embedding_client=embedding,\n        llm_client=llm,\n        vector_store=store,\n    )\n\ndef index_documents(pipeline, documents):\n    \"\"\"Index documents into the pipeline.\"\"\"\n    print(\"\ud83d\udcda Indexing documents...\")\n    pipeline.index_documents(documents)\n    print(\"\u2705 Done!\")\n\ndef interactive_query(pipeline):\n    \"\"\"Interactive query loop.\"\"\"\n    print(\"\\n\ud83e\udd16 RAG Assistant Ready!\")\n    print(\"Type 'quit' to exit\\n\")\n\n    while True:\n        query = input(\"You: \").strip()\n\n        if query.lower() in ('quit', 'exit', 'q'):\n            print(\"Goodbye!\")\n            break\n\n        if not query:\n            continue\n\n        try:\n            response = pipeline.query(query)\n            print(f\"\\nAssistant: {response.answer}\\n\")\n\n            if response.sources:\n                print(f\"\ud83d\udcce {len(response.sources)} sources used\")\n\n        except Exception as e:\n            print(f\"Error: {e}\\n\")\n\ndef main():\n    # Setup\n    pipeline = setup_pipeline()\n\n    # Index your documents\n    documents = [\n        \"Your document content here...\",\n        # Add more documents\n    ]\n    index_documents(pipeline, documents)\n\n    # Interactive queries\n    interactive_query(pipeline)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run the Application</p> Bash<pre><code>python my_rag_app.py\n</code></pre>"},{"location":"examples/basic_rag/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Custom Vector Store</p> <p>Use different databases for storage</p> </li> <li> <p> Hybrid Search</p> <p>Improve retrieval with hybrid search</p> </li> <li> <p> Advanced Pipeline</p> <p>Add production features</p> </li> <li> <p> Production Setup</p> <p>Deploy to production</p> </li> </ul>"},{"location":"examples/custom_vectorstore/","title":"Custom Vector Store","text":""},{"location":"examples/custom_vectorstore/#custom-vector-store-implementation","title":"Custom Vector Store Implementation","text":"<p>Build Your Own Storage Backend</p> <p>Learn how to implement your own vector store by following the <code>VectorStoreClient</code> protocol. This example shows a complete implementation using ChromaDB.</p>"},{"location":"examples/custom_vectorstore/#why-custom-vector-stores","title":"Why Custom Vector Stores?","text":"<p>When to Build Custom Stores</p> <p>While rag-toolkit includes Milvus by default, you might want to:</p> <ul> <li> <p> Different Database</p> <p>Use Pinecone, Qdrant, ChromaDB, Weaviate</p> </li> <li> <p> Existing Infrastructure</p> <p>Integrate with your current systems</p> </li> <li> <p> Custom Logic</p> <p>Implement specialized search features</p> </li> <li> <p> Specialized Features</p> <p>Add domain-specific capabilities</p> </li> </ul>"},{"location":"examples/custom_vectorstore/#the-vectorstoreclient-protocol","title":"The VectorStoreClient Protocol","text":"<p>Protocol Definition</p> <p>All vector stores must implement this protocol for compatibility.</p> protocol.py<pre><code>from typing import Protocol, runtime_checkable\nfrom rag_toolkit.core.types import SearchResult\n\n@runtime_checkable\nclass VectorStoreClient(Protocol):\n    \"\"\"Protocol for vector store implementations.\"\"\"\n\n    async def upsert(\n        self,\n        ids: list[str] | None = None,\n        texts: list[str] | None = None,\n        embeddings: list[list[float]] | None = None,\n        metadatas: list[dict] | None = None,\n    ) -&gt; list[str]:\n        \"\"\"Insert or update documents.\"\"\"\n        ...\n\n    async def search(\n        self,\n        query: str | list[float],\n        limit: int = 5,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Search for similar documents.\"\"\"\n        ...\n\n    async def delete(\n        self,\n        ids: list[str] | None = None,\n        filter: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Delete documents.\"\"\"\n        ...\n\n    async def get(\n        self,\n        ids: list[str] | None = None,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Retrieve documents.\"\"\"\n        ...\n</code></pre>"},{"location":"examples/custom_vectorstore/#complete-chromadb-implementation","title":"Complete ChromaDB Implementation","text":"<p>Full-Featured Example</p> <p>Here's a complete implementation using ChromaDB.</p> chromadb_store.py<pre><code>import uuid\nfrom typing import Optional\nimport chromadb\nfrom chromadb.config import Settings\nfrom rag_toolkit.core.vectorstore import VectorStoreClient\nfrom rag_toolkit.core.embedding import EmbeddingClient\nfrom rag_toolkit.core.types import SearchResult\n\nclass ChromaVectorStore:\n    \"\"\"ChromaDB vector store implementation.\"\"\"\n\n    def __init__(\n        self,\n        collection_name: str,\n        embedding_client: EmbeddingClient,\n        persist_directory: str = \"./chroma_db\",\n        distance_metric: str = \"cosine\",  # or \"l2\", \"ip\"\n    ):\n        \"\"\"Initialize ChromaDB vector store.\n\n        Args:\n            collection_name: Name of the collection\n            embedding_client: Embedding client for text\u2192vector\n            persist_directory: Where to store the database\n            distance_metric: Distance metric (cosine, l2, ip)\n        \"\"\"\n        self.collection_name = collection_name\n        self.embedding_client = embedding_client\n        self.distance_metric = distance_metric\n\n        # Initialize Chroma client\n        self.client = chromadb.Client(\n            Settings(\n                chroma_db_impl=\"duckdb+parquet\",\n                persist_directory=persist_directory\n            )\n        )\n\n        # Get or create collection\n        self.collection = self.client.get_or_create_collection(\n            name=collection_name,\n            metadata={\"hnsw:space\": distance_metric}\n        )\n\n    async def upsert(\n        self,\n        ids: list[str] | None = None,\n        texts: list[str] | None = None,\n        embeddings: list[list[float]] | None = None,\n        metadatas: list[dict] | None = None,\n    ) -&gt; list[str]:\n        \"\"\"Insert or update documents.\"\"\"\n        # Generate IDs if not provided\n        if ids is None:\n            ids = [str(uuid.uuid4()) for _ in range(len(texts or embeddings))]\n\n        # Compute embeddings if not provided\n        if embeddings is None and texts is not None:\n            embeddings = await self.embedding_client.embed_batch(texts)\n\n        # Prepare metadata\n        if metadatas is None:\n            metadatas = [{} for _ in ids]\n\n        # ChromaDB requires documents (texts)\n        if texts is None:\n            texts = [\"\" for _ in ids]\n\n        # Upsert to ChromaDB\n        self.collection.upsert(\n            ids=ids,\n            embeddings=embeddings,\n            documents=texts,\n            metadatas=metadatas\n        )\n\n        return ids\n\n    async def search(\n        self,\n        query: str | list[float],\n        limit: int = 5,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Search for similar documents.\"\"\"\n        # Embed query if string\n        if isinstance(query, str):\n            query_vector = await self.embedding_client.embed(query)\n        else:\n            query_vector = query\n\n        # Convert filter to ChromaDB format\n        where = self._convert_filter(filter) if filter else None\n\n        # Search\n        results = self.collection.query(\n            query_embeddings=[query_vector],\n            n_results=limit,\n            where=where,\n            include=[\"documents\", \"metadatas\", \"distances\", \"embeddings\"]\n        )\n\n        # Convert to SearchResult\n        search_results = []\n        for i in range(len(results['ids'][0])):\n            # Convert distance to similarity score\n            distance = results['distances'][0][i]\n            score = self._distance_to_score(distance)\n\n            search_results.append(\n                SearchResult(\n                    id=results['ids'][0][i],\n                    text=results['documents'][0][i],\n                    metadata=results['metadatas'][0][i],\n                    score=score,\n                    vector=results['embeddings'][0][i] if results['embeddings'] else None\n                )\n            )\n\n        return search_results\n\n    async def delete(\n        self,\n        ids: list[str] | None = None,\n        filter: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Delete documents.\"\"\"\n        if ids:\n            # Delete by IDs\n            self.collection.delete(ids=ids)\n        elif filter:\n            # Delete by filter\n            where = self._convert_filter(filter)\n            self.collection.delete(where=where)\n        else:\n            # Delete all (careful!)\n            self.client.delete_collection(self.collection_name)\n            self.collection = self.client.create_collection(\n                name=self.collection_name,\n                metadata={\"hnsw:space\": self.distance_metric}\n            )\n\n    async def get(\n        self,\n        ids: list[str] | None = None,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Retrieve documents.\"\"\"\n        where = self._convert_filter(filter) if filter else None\n\n        results = self.collection.get(\n            ids=ids,\n            where=where,\n            include=[\"documents\", \"metadatas\", \"embeddings\"]\n        )\n\n        # Convert to SearchResult\n        search_results = []\n        for i in range(len(results['ids'])):\n            search_results.append(\n                SearchResult(\n                    id=results['ids'][i],\n                    text=results['documents'][i],\n                    metadata=results['metadatas'][i],\n                    score=1.0,  # No score for get operation\n                    vector=results['embeddings'][i] if results['embeddings'] else None\n                )\n            )\n\n        return search_results\n\n    def _convert_filter(self, filter: dict) -&gt; dict:\n        \"\"\"Convert rag-toolkit filter to ChromaDB where clause.\"\"\"\n        where = {}\n\n        for key, value in filter.items():\n            if isinstance(value, dict):\n                # Handle operators like $gte, $in\n                for op, val in value.items():\n                    if op == \"$gte\":\n                        where[key] = {\"$gte\": val}\n                    elif op == \"$lte\":\n                        where[key] = {\"$lte\": val}\n                    elif op == \"$in\":\n                        where[key] = {\"$in\": val}\n                    elif op == \"$ne\":\n                        where[key] = {\"$ne\": val}\n            else:\n                # Simple equality\n                where[key] = {\"$eq\": value}\n\n        return where\n\n    def _distance_to_score(self, distance: float) -&gt; float:\n        \"\"\"Convert distance to similarity score [0, 1].\"\"\"\n        if self.distance_metric == \"cosine\":\n            # Cosine distance is 1 - cosine_similarity\n            # So score = 1 - distance\n            return max(0.0, 1.0 - distance)\n        elif self.distance_metric == \"l2\":\n            # L2 distance: smaller is better\n            # Convert to similarity score\n            return 1.0 / (1.0 + distance)\n        elif self.distance_metric == \"ip\":\n            # Inner product: larger is better\n            # Already a similarity score\n            return distance\n        return distance\n</code></pre>"},{"location":"examples/custom_vectorstore/#using-the-custom-vector-store","title":"Using the Custom Vector Store","text":""},{"location":"examples/custom_vectorstore/#basic-usage","title":"Basic Usage","text":"Python<pre><code>from rag_toolkit.infra.embedding import OpenAIEmbedding\n\n# Create embedding client\nembedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Create ChromaDB vector store\nvector_store = ChromaVectorStore(\n    collection_name=\"my_documents\",\n    embedding_client=embedding,\n    persist_directory=\"./my_chroma_db\",\n)\n\n# Insert documents\nids = await vector_store.upsert(\n    texts=[\"Document 1\", \"Document 2\"],\n    metadatas=[{\"source\": \"doc1\"}, {\"source\": \"doc2\"}]\n)\n\n# Search\nresults = await vector_store.search(\n    query=\"relevant query\",\n    limit=5\n)\n\nfor result in results:\n    print(f\"Score: {result.score:.4f}\")\n    print(f\"Text: {result.text}\")\n</code></pre>"},{"location":"examples/custom_vectorstore/#integration-with-rag-pipeline","title":"Integration with RAG Pipeline","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.llm import OpenAILLM\n\n# Create RAG pipeline with custom vector store\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,  # Your custom implementation!\n    llm_client=OpenAILLM(model=\"gpt-4-turbo\"),\n)\n\n# Use as normal\nawait pipeline.index(texts=[\"Doc 1\", \"Doc 2\"])\nresult = await pipeline.query(\"What is in the documents?\")\nprint(result.answer)\n</code></pre>"},{"location":"examples/custom_vectorstore/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/custom_vectorstore/#custom-embeddings","title":"Custom Embeddings","text":"<p>You can also implement custom embedding clients:</p> Python<pre><code>from sentence_transformers import SentenceTransformer\n\nclass HuggingFaceEmbedding:\n    \"\"\"HuggingFace sentence-transformers embedding.\"\"\"\n\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        self.model = SentenceTransformer(model_name)\n        self._dimension = self.model.get_sentence_embedding_dimension()\n\n    @property\n    def dimension(self) -&gt; int:\n        return self._dimension\n\n    async def embed(self, text: str) -&gt; list[float]:\n        embedding = self.model.encode(text, convert_to_tensor=False)\n        return embedding.tolist()\n\n    async def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        embeddings = self.model.encode(\n            texts,\n            batch_size=32,\n            show_progress_bar=False,\n            convert_to_tensor=False\n        )\n        return embeddings.tolist()\n\n# Use together\nembedding = HuggingFaceEmbedding(\"all-MiniLM-L6-v2\")\nvector_store = ChromaVectorStore(\n    collection_name=\"docs\",\n    embedding_client=embedding\n)\n</code></pre>"},{"location":"examples/custom_vectorstore/#hybrid-search","title":"Hybrid Search","text":"<p>Add keyword search to your vector store:</p> Python<pre><code>class ChromaVectorStoreWithHybrid(ChromaVectorStore):\n    \"\"\"ChromaDB with hybrid search support.\"\"\"\n\n    async def hybrid_search(\n        self,\n        query: str,\n        limit: int = 10,\n        alpha: float = 0.7,  # 0.7 vector + 0.3 keyword\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Hybrid vector + keyword search.\"\"\"\n        # Vector search\n        vector_results = await self.search(\n            query=query,\n            limit=limit * 2,  # Get more candidates\n            filter=filter\n        )\n\n        # Keyword search (simple text matching)\n        keyword_results = await self._keyword_search(\n            query=query,\n            limit=limit * 2,\n            filter=filter\n        )\n\n        # Combine scores\n        combined = self._combine_results(\n            vector_results,\n            keyword_results,\n            alpha=alpha\n        )\n\n        # Return top K\n        combined.sort(key=lambda x: x.score, reverse=True)\n        return combined[:limit]\n\n    async def _keyword_search(\n        self,\n        query: str,\n        limit: int,\n        filter: dict | None = None\n    ) -&gt; list[SearchResult]:\n        \"\"\"Simple keyword-based search.\"\"\"\n        where = self._convert_filter(filter) if filter else None\n\n        # Get all documents (or filtered subset)\n        results = self.collection.get(\n            where=where,\n            include=[\"documents\", \"metadatas\"]\n        )\n\n        # Score by keyword presence\n        query_lower = query.lower()\n        scored_results = []\n\n        for i in range(len(results['ids'])):\n            doc_lower = results['documents'][i].lower()\n\n            # Simple TF score\n            score = sum(\n                doc_lower.count(word.lower())\n                for word in query_lower.split()\n            )\n\n            if score &gt; 0:\n                scored_results.append(\n                    SearchResult(\n                        id=results['ids'][i],\n                        text=results['documents'][i],\n                        metadata=results['metadatas'][i],\n                        score=score,\n                        vector=None\n                    )\n                )\n\n        # Normalize scores\n        if scored_results:\n            max_score = max(r.score for r in scored_results)\n            for result in scored_results:\n                result.score = result.score / max_score\n\n        # Sort and return top K\n        scored_results.sort(key=lambda x: x.score, reverse=True)\n        return scored_results[:limit]\n\n    def _combine_results(\n        self,\n        vector_results: list[SearchResult],\n        keyword_results: list[SearchResult],\n        alpha: float\n    ) -&gt; list[SearchResult]:\n        \"\"\"Combine vector and keyword results.\"\"\"\n        # Create score map\n        scores = {}\n\n        # Add vector scores\n        for result in vector_results:\n            scores[result.id] = {\n                'vector': result.score * alpha,\n                'keyword': 0.0,\n                'result': result\n            }\n\n        # Add keyword scores\n        for result in keyword_results:\n            if result.id in scores:\n                scores[result.id]['keyword'] = result.score * (1 - alpha)\n            else:\n                scores[result.id] = {\n                    'vector': 0.0,\n                    'keyword': result.score * (1 - alpha),\n                    'result': result\n                }\n\n        # Combine scores\n        combined = []\n        for doc_id, data in scores.items():\n            result = data['result']\n            result.score = data['vector'] + data['keyword']\n            combined.append(result)\n\n        return combined\n\n# Usage\nvector_store = ChromaVectorStoreWithHybrid(\n    collection_name=\"docs\",\n    embedding_client=embedding\n)\n\nresults = await vector_store.hybrid_search(\n    query=\"machine learning algorithms\",\n    limit=10,\n    alpha=0.7  # Prefer vector search\n)\n</code></pre>"},{"location":"examples/custom_vectorstore/#testing-your-implementation","title":"Testing Your Implementation","text":""},{"location":"examples/custom_vectorstore/#unit-tests","title":"Unit Tests","text":"Python<pre><code>import pytest\n\n@pytest.mark.asyncio\nasync def test_upsert():\n    \"\"\"Test document insertion.\"\"\"\n    vector_store = ChromaVectorStore(\n        collection_name=\"test\",\n        embedding_client=embedding\n    )\n\n    ids = await vector_store.upsert(\n        texts=[\"Test document\"],\n        metadatas=[{\"source\": \"test\"}]\n    )\n\n    assert len(ids) == 1\n    assert ids[0] is not None\n\n@pytest.mark.asyncio\nasync def test_search():\n    \"\"\"Test search functionality.\"\"\"\n    vector_store = ChromaVectorStore(\n        collection_name=\"test\",\n        embedding_client=embedding\n    )\n\n    # Insert documents\n    await vector_store.upsert(\n        texts=[\"Machine learning is great\", \"Python is amazing\"],\n        metadatas=[{\"topic\": \"ML\"}, {\"topic\": \"Programming\"}]\n    )\n\n    # Search\n    results = await vector_store.search(\n        query=\"artificial intelligence\",\n        limit=2\n    )\n\n    assert len(results) &lt;= 2\n    assert all(isinstance(r, SearchResult) for r in results)\n    assert all(r.score &gt;= 0 for r in results)\n\n@pytest.mark.asyncio\nasync def test_metadata_filter():\n    \"\"\"Test metadata filtering.\"\"\"\n    vector_store = ChromaVectorStore(\n        collection_name=\"test\",\n        embedding_client=embedding\n    )\n\n    # Insert with metadata\n    await vector_store.upsert(\n        texts=[\"Doc 1\", \"Doc 2\"],\n        metadatas=[{\"category\": \"A\"}, {\"category\": \"B\"}]\n    )\n\n    # Search with filter\n    results = await vector_store.search(\n        query=\"document\",\n        filter={\"category\": \"A\"}\n    )\n\n    assert all(r.metadata[\"category\"] == \"A\" for r in results)\n</code></pre>"},{"location":"examples/custom_vectorstore/#integration-tests","title":"Integration Tests","text":"Python<pre><code>@pytest.mark.asyncio\nasync def test_rag_pipeline_integration():\n    \"\"\"Test integration with RAG pipeline.\"\"\"\n    from rag_toolkit import RagPipeline\n    from rag_toolkit.infra.llm import OpenAILLM\n\n    pipeline = RagPipeline(\n        embedding_client=embedding,\n        vector_store=vector_store,\n        llm_client=OpenAILLM()\n    )\n\n    # Index documents\n    await pipeline.index(\n        texts=[\"Machine learning is a subset of AI.\"],\n        metadatas=[{\"source\": \"textbook\"}]\n    )\n\n    # Query\n    result = await pipeline.query(\"What is ML?\")\n\n    assert result.answer is not None\n    assert len(result.sources) &gt; 0\n</code></pre>"},{"location":"examples/custom_vectorstore/#other-vector-store-examples","title":"Other Vector Store Examples","text":""},{"location":"examples/custom_vectorstore/#pinecone-implementation","title":"Pinecone Implementation","text":"Python<pre><code>import pinecone\n\nclass PineconeVectorStore:\n    \"\"\"Pinecone vector store implementation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: str,\n        embedding_client: EmbeddingClient,\n        api_key: str,\n        environment: str,\n    ):\n        pinecone.init(api_key=api_key, environment=environment)\n        self.index = pinecone.Index(index_name)\n        self.embedding_client = embedding_client\n\n    async def upsert(\n        self,\n        ids: list[str] | None = None,\n        texts: list[str] | None = None,\n        embeddings: list[list[float]] | None = None,\n        metadatas: list[dict] | None = None,\n    ) -&gt; list[str]:\n        if embeddings is None:\n            embeddings = await self.embedding_client.embed_batch(texts)\n\n        # Prepare vectors\n        vectors = [\n            (ids[i], embeddings[i], metadatas[i] if metadatas else {})\n            for i in range(len(ids))\n        ]\n\n        # Upsert to Pinecone\n        self.index.upsert(vectors=vectors)\n        return ids\n\n    # ... implement other methods\n</code></pre>"},{"location":"examples/custom_vectorstore/#qdrant-implementation","title":"Qdrant Implementation","text":"Python<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\n\nclass QdrantVectorStore:\n    \"\"\"Qdrant vector store implementation.\"\"\"\n\n    def __init__(\n        self,\n        collection_name: str,\n        embedding_client: EmbeddingClient,\n        url: str = \"localhost\",\n        port: int = 6333,\n    ):\n        self.client = QdrantClient(url=url, port=port)\n        self.collection_name = collection_name\n        self.embedding_client = embedding_client\n\n        # Create collection if not exists\n        try:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=VectorParams(\n                    size=embedding_client.dimension,\n                    distance=Distance.COSINE\n                )\n            )\n        except Exception:\n            pass  # Collection already exists\n\n    async def upsert(\n        self,\n        ids: list[str] | None = None,\n        texts: list[str] | None = None,\n        embeddings: list[list[float]] | None = None,\n        metadatas: list[dict] | None = None,\n    ) -&gt; list[str]:\n        if embeddings is None:\n            embeddings = await self.embedding_client.embed_batch(texts)\n\n        # Prepare points\n        points = [\n            PointStruct(\n                id=ids[i],\n                vector=embeddings[i],\n                payload={\n                    \"text\": texts[i] if texts else \"\",\n                    **metadatas[i] if metadatas else {}\n                }\n            )\n            for i in range(len(ids))\n        ]\n\n        # Upsert to Qdrant\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points\n        )\n\n        return ids\n\n    # ... implement other methods\n</code></pre>"},{"location":"examples/custom_vectorstore/#best-practices","title":"Best Practices","text":"<ol> <li>Follow the Protocol Strictly</li> <li>Implement all required methods</li> <li>Match signatures exactly</li> <li> <p>Return correct types</p> </li> <li> <p>Handle Errors Gracefully</p> </li> <li>Wrap external library exceptions</li> <li>Provide helpful error messages</li> <li> <p>Implement retries for transient failures</p> </li> <li> <p>Optimize Performance</p> </li> <li>Batch operations when possible</li> <li>Use connection pooling</li> <li> <p>Cache embeddings</p> </li> <li> <p>Test Thoroughly</p> </li> <li>Unit tests for each method</li> <li>Integration tests with RAG pipeline</li> <li> <p>Load tests for performance</p> </li> <li> <p>Document Your Implementation</p> </li> <li>Clear docstrings</li> <li>Usage examples</li> <li>Configuration options</li> </ol>"},{"location":"examples/custom_vectorstore/#next-steps","title":"Next Steps","text":"<ul> <li>Hybrid Search Example - Advanced search techniques</li> <li>Vector Stores Guide - Deep dive</li> <li>Production Setup - Deploy to production</li> </ul>"},{"location":"examples/custom_vectorstore/#see-also","title":"See Also","text":"<ul> <li>VectorStoreClient Protocol</li> <li>ChromaDB Documentation</li> <li>Pinecone Documentation</li> <li>Qdrant Documentation</li> </ul>"},{"location":"examples/hybrid_search/","title":"Hybrid Search","text":""},{"location":"examples/hybrid_search/#hybrid-search-implementation","title":"Hybrid Search Implementation","text":"<p>Best of Both Worlds</p> <p>Learn how to combine vector search with keyword search for more accurate and robust retrieval. Hybrid search leverages the strengths of both approaches to improve search quality.</p>"},{"location":"examples/hybrid_search/#why-hybrid-search","title":"Why Hybrid Search?","text":"<p>Complementary Strengths</p> <p>Vector search and keyword search each have unique advantages.</p> \ud83d\udd0d Vector Search\ud83d\udcdd Keyword Search\u26a1 Hybrid Search <p>Advantages:</p> <ul> <li>\u2705 Semantic understanding</li> <li>\u2705 Finds conceptually similar content</li> <li>\u2705 Handles synonyms and paraphrases</li> </ul> <p>Limitations:</p> <ul> <li>\u274c May miss exact keyword matches</li> <li>\u274c Less effective for proper nouns/codes</li> </ul> <p>Advantages:</p> <ul> <li>\u2705 Exact match precision</li> <li>\u2705 Great for proper nouns, codes, IDs</li> <li>\u2705 Fast and deterministic</li> </ul> <p>Limitations:</p> <ul> <li>\u274c No semantic understanding</li> <li>\u274c Misses synonyms</li> </ul> <p>Benefits:</p> <ul> <li>\u2705 Best of both worlds</li> <li>\u2705 Semantic + exact matching</li> <li>\u2705 More robust retrieval</li> <li>\u2705 Better overall accuracy</li> </ul>"},{"location":"examples/hybrid_search/#basic-hybrid-search","title":"Basic Hybrid Search","text":""},{"location":"examples/hybrid_search/#simple-implementation","title":"Simple Implementation","text":"Python<pre><code>from rag_toolkit.core.vectorstore import MilvusVectorStore\nfrom rag_toolkit.core.embedding import OpenAIEmbedding\nfrom rag_toolkit.core.types import SearchResult\n\nclass HybridSearcher:\n    \"\"\"Simple hybrid search combining vector + keyword.\"\"\"\n\n    def __init__(\n        self,\n        vector_store: MilvusVectorStore,\n        embedding_client: OpenAIEmbedding,\n        alpha: float = 0.7,  # Weight for vector search\n    ):\n        \"\"\"Initialize hybrid searcher.\n\n        Args:\n            vector_store: Vector store for semantic search\n            embedding_client: Embedding client\n            alpha: Weight for vector search (1-alpha for keyword)\n        \"\"\"\n        self.vector_store = vector_store\n        self.embedding_client = embedding_client\n        self.alpha = alpha\n\n    async def search(\n        self,\n        query: str,\n        limit: int = 10,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Perform hybrid search.\n\n        Args:\n            query: Search query\n            limit: Number of results\n            filter: Metadata filter\n\n        Returns:\n            Ranked search results\n        \"\"\"\n        # 1. Vector search\n        vector_results = await self.vector_store.search(\n            query=query,\n            limit=limit * 2,  # Get more candidates\n            filter=filter\n        )\n\n        # 2. Keyword search (BM25 or simple text matching)\n        keyword_results = await self._keyword_search(\n            query=query,\n            limit=limit * 2,\n            filter=filter\n        )\n\n        # 3. Combine scores\n        combined = self._reciprocal_rank_fusion(\n            vector_results,\n            keyword_results,\n            k=60  # RRF constant\n        )\n\n        # 4. Return top K\n        return combined[:limit]\n\n    async def _keyword_search(\n        self,\n        query: str,\n        limit: int,\n        filter: dict | None = None\n    ) -&gt; list[SearchResult]:\n        \"\"\"Simple BM25-style keyword search.\"\"\"\n        # Get all documents (in production, use Elasticsearch or similar)\n        all_docs = await self.vector_store.get(filter=filter)\n\n        # Score documents\n        query_terms = query.lower().split()\n        scored_docs = []\n\n        for doc in all_docs:\n            score = self._bm25_score(\n                doc.text.lower(),\n                query_terms\n            )\n            if score &gt; 0:\n                scored_docs.append((doc, score))\n\n        # Sort by score\n        scored_docs.sort(key=lambda x: x[1], reverse=True)\n\n        # Convert to SearchResult\n        results = []\n        for doc, score in scored_docs[:limit]:\n            results.append(\n                SearchResult(\n                    id=doc.id,\n                    text=doc.text,\n                    metadata=doc.metadata,\n                    score=score,\n                    vector=doc.vector\n                )\n            )\n\n        return results\n\n    def _bm25_score(\n        self,\n        text: str,\n        query_terms: list[str],\n        k1: float = 1.5,\n        b: float = 0.75\n    ) -&gt; float:\n        \"\"\"Simplified BM25 scoring.\"\"\"\n        score = 0.0\n\n        for term in query_terms:\n            # Term frequency\n            tf = text.count(term)\n\n            # Document length normalization\n            doc_len = len(text.split())\n            avg_doc_len = 100  # Approximate\n\n            # BM25 formula (simplified)\n            numerator = tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * doc_len / avg_doc_len)\n\n            score += numerator / denominator\n\n        return score\n\n    def _reciprocal_rank_fusion(\n        self,\n        vector_results: list[SearchResult],\n        keyword_results: list[SearchResult],\n        k: int = 60\n    ) -&gt; list[SearchResult]:\n        \"\"\"Combine results using Reciprocal Rank Fusion.\n\n        RRF formula: score(doc) = sum(1 / (k + rank))\n        \"\"\"\n        # Create score map\n        scores = {}\n\n        # Add vector scores (rank-based)\n        for rank, result in enumerate(vector_results):\n            if result.id not in scores:\n                scores[result.id] = {'doc': result, 'score': 0.0}\n            scores[result.id]['score'] += 1.0 / (k + rank + 1)\n\n        # Add keyword scores (rank-based)\n        for rank, result in enumerate(keyword_results):\n            if result.id not in scores:\n                scores[result.id] = {'doc': result, 'score': 0.0}\n            scores[result.id]['score'] += 1.0 / (k + rank + 1)\n\n        # Sort by combined score\n        ranked = sorted(\n            scores.values(),\n            key=lambda x: x['score'],\n            reverse=True\n        )\n\n        # Convert back to SearchResult\n        results = []\n        for item in ranked:\n            doc = item['doc']\n            doc.score = item['score']\n            results.append(doc)\n\n        return results\n\n# Usage\nsearcher = HybridSearcher(\n    vector_store=vector_store,\n    embedding_client=embedding,\n    alpha=0.7\n)\n\nresults = await searcher.search(\n    query=\"machine learning algorithms\",\n    limit=10\n)\n</code></pre>"},{"location":"examples/hybrid_search/#advanced-hybrid-search","title":"Advanced Hybrid Search","text":""},{"location":"examples/hybrid_search/#milvus-native-hybrid-search","title":"Milvus Native Hybrid Search","text":"<p>Milvus 2.4+ supports native hybrid search with multiple vector fields:</p> Python<pre><code>from pymilvus import Collection, connections\n\nclass MilvusHybridSearch:\n    \"\"\"Milvus native hybrid search with sparse + dense vectors.\"\"\"\n\n    def __init__(\n        self,\n        collection_name: str,\n        dense_embedding: OpenAIEmbedding,\n        sparse_embedding: SparseEmbedding,  # BM25 or SPLADE\n    ):\n        connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n        self.collection = Collection(collection_name)\n        self.dense_embedding = dense_embedding\n        self.sparse_embedding = sparse_embedding\n\n    async def hybrid_search(\n        self,\n        query: str,\n        limit: int = 10,\n        dense_weight: float = 0.7,\n        filter_expr: str | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Milvus native hybrid search.\"\"\"\n        # Generate both embeddings\n        dense_vector = await self.dense_embedding.embed(query)\n        sparse_vector = await self.sparse_embedding.embed(query)\n\n        # Hybrid search\n        results = self.collection.hybrid_search(\n            data=[\n                [dense_vector],  # Dense search\n                [sparse_vector]  # Sparse search\n            ],\n            anns_field=[\"dense_vector\", \"sparse_vector\"],\n            param=[\n                {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 64}},\n                {\"metric_type\": \"IP\", \"params\": {}}\n            ],\n            limit=limit,\n            expr=filter_expr,\n            output_fields=[\"text\", \"metadata\"],\n            rerank=RRFReranker(k=60, weights=[dense_weight, 1 - dense_weight])\n        )\n\n        # Convert to SearchResult\n        search_results = []\n        for hits in results:\n            for hit in hits:\n                search_results.append(\n                    SearchResult(\n                        id=hit.id,\n                        text=hit.entity.get(\"text\"),\n                        metadata=hit.entity.get(\"metadata\", {}),\n                        score=hit.score,\n                        vector=None\n                    )\n                )\n\n        return search_results\n</code></pre>"},{"location":"examples/hybrid_search/#elasticsearch-milvus-hybrid","title":"Elasticsearch + Milvus Hybrid","text":"<p>Use Elasticsearch for keyword search and Milvus for vector search:</p> Python<pre><code>from elasticsearch import Elasticsearch\n\nclass ElasticsearchMilvusHybrid:\n    \"\"\"Hybrid search with Elasticsearch + Milvus.\"\"\"\n\n    def __init__(\n        self,\n        es_client: Elasticsearch,\n        es_index: str,\n        milvus_store: MilvusVectorStore,\n        embedding: OpenAIEmbedding,\n    ):\n        self.es = es_client\n        self.es_index = es_index\n        self.milvus = milvus_store\n        self.embedding = embedding\n\n    async def search(\n        self,\n        query: str,\n        limit: int = 10,\n        alpha: float = 0.7,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Hybrid search with Elasticsearch + Milvus.\"\"\"\n        # 1. Elasticsearch BM25 search\n        es_results = self.es.search(\n            index=self.es_index,\n            body={\n                \"query\": {\n                    \"multi_match\": {\n                        \"query\": query,\n                        \"fields\": [\"text\", \"title\"],\n                        \"type\": \"best_fields\"\n                    }\n                },\n                \"size\": limit * 2\n            }\n        )\n\n        # 2. Milvus vector search\n        milvus_results = await self.milvus.search(\n            query=query,\n            limit=limit * 2\n        )\n\n        # 3. Combine scores (weighted average)\n        combined = self._weighted_fusion(\n            es_results['hits']['hits'],\n            milvus_results,\n            alpha=alpha\n        )\n\n        return combined[:limit]\n\n    def _weighted_fusion(\n        self,\n        es_hits: list,\n        milvus_hits: list[SearchResult],\n        alpha: float\n    ) -&gt; list[SearchResult]:\n        \"\"\"Weighted score fusion.\"\"\"\n        # Normalize scores\n        es_scores = {}\n        if es_hits:\n            max_es = max(hit['_score'] for hit in es_hits)\n            for hit in es_hits:\n                es_scores[hit['_id']] = hit['_score'] / max_es\n\n        milvus_scores = {}\n        if milvus_hits:\n            max_milvus = max(hit.score for hit in milvus_hits)\n            for hit in milvus_hits:\n                milvus_scores[hit.id] = hit.score / max_milvus\n\n        # Combine scores\n        combined = {}\n        all_ids = set(es_scores.keys()) | set(milvus_scores.keys())\n\n        for doc_id in all_ids:\n            es_score = es_scores.get(doc_id, 0.0)\n            milvus_score = milvus_scores.get(doc_id, 0.0)\n\n            combined_score = alpha * milvus_score + (1 - alpha) * es_score\n            combined[doc_id] = combined_score\n\n        # Sort and return\n        ranked = sorted(\n            combined.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )\n\n        # Convert to SearchResult (simplified)\n        results = []\n        for doc_id, score in ranked:\n            # Find original document\n            doc = next(\n                (h for h in milvus_hits if h.id == doc_id),\n                None\n            )\n            if doc:\n                doc.score = score\n                results.append(doc)\n\n        return results\n</code></pre>"},{"location":"examples/hybrid_search/#adaptive-hybrid-search","title":"Adaptive Hybrid Search","text":"<p>Adjust weights based on query type:</p> Python<pre><code>import re\n\nclass AdaptiveHybridSearch:\n    \"\"\"Adaptive hybrid search with query-specific weighting.\"\"\"\n\n    def __init__(\n        self,\n        vector_store: MilvusVectorStore,\n        embedding: OpenAIEmbedding,\n    ):\n        self.vector_store = vector_store\n        self.embedding = embedding\n\n    async def search(\n        self,\n        query: str,\n        limit: int = 10,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Adaptive hybrid search.\"\"\"\n        # Determine query type\n        alpha = self._determine_alpha(query)\n\n        # Perform hybrid search with adaptive weight\n        searcher = HybridSearcher(\n            vector_store=self.vector_store,\n            embedding_client=self.embedding,\n            alpha=alpha\n        )\n\n        return await searcher.search(query, limit)\n\n    def _determine_alpha(self, query: str) -&gt; float:\n        \"\"\"Determine vector search weight based on query type.\"\"\"\n        # Proper nouns or codes \u2192 prefer keyword search\n        if self._has_proper_nouns(query) or self._has_codes(query):\n            return 0.3  # 30% vector, 70% keyword\n\n        # Short queries \u2192 prefer keyword search\n        if len(query.split()) &lt;= 3:\n            return 0.4  # 40% vector, 60% keyword\n\n        # Questions or conceptual queries \u2192 prefer vector search\n        if query.startswith((\"what\", \"how\", \"why\", \"explain\")):\n            return 0.9  # 90% vector, 10% keyword\n\n        # Default: balanced\n        return 0.7  # 70% vector, 30% keyword\n\n    def _has_proper_nouns(self, query: str) -&gt; bool:\n        \"\"\"Check if query contains proper nouns.\"\"\"\n        # Simple heuristic: capitalized words mid-sentence\n        words = query.split()\n        if len(words) &gt; 1:\n            return any(w[0].isupper() for w in words[1:])\n        return False\n\n    def _has_codes(self, query: str) -&gt; bool:\n        \"\"\"Check if query contains codes or IDs.\"\"\"\n        # Pattern: numbers, hyphens, underscores\n        return bool(re.search(r'[A-Z0-9]{2,}[-_][A-Z0-9]+', query))\n\n# Usage\nadaptive = AdaptiveHybridSearch(\n    vector_store=vector_store,\n    embedding=embedding\n)\n\n# Conceptual query \u2192 high vector weight\nresults1 = await adaptive.search(\"What is machine learning?\")\n\n# Proper noun query \u2192 high keyword weight\nresults2 = await adaptive.search(\"Find documents about PyTorch\")\n\n# Code query \u2192 high keyword weight\nresults3 = await adaptive.search(\"error code ERR_123\")\n</code></pre>"},{"location":"examples/hybrid_search/#integration-with-rag-pipeline","title":"Integration with RAG Pipeline","text":""},{"location":"examples/hybrid_search/#custom-hybrid-rag","title":"Custom Hybrid RAG","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.llm import OpenAILLM\n\nclass HybridRagPipeline(RagPipeline):\n    \"\"\"RAG pipeline with hybrid search.\"\"\"\n\n    def __init__(\n        self,\n        embedding_client: OpenAIEmbedding,\n        vector_store: MilvusVectorStore,\n        llm_client: OpenAILLM,\n        alpha: float = 0.7,\n    ):\n        super().__init__(\n            embedding_client=embedding_client,\n            vector_store=vector_store,\n            llm_client=llm_client\n        )\n        self.hybrid_searcher = HybridSearcher(\n            vector_store=vector_store,\n            embedding_client=embedding_client,\n            alpha=alpha\n        )\n\n    async def query(\n        self,\n        query: str,\n        limit: int = 5,\n        use_hybrid: bool = True,\n        **kwargs\n    ):\n        \"\"\"Query with optional hybrid search.\"\"\"\n        if use_hybrid:\n            # Use hybrid search\n            retrieved = await self.hybrid_searcher.search(\n                query=query,\n                limit=limit\n            )\n        else:\n            # Use standard vector search\n            retrieved = await self.vector_store.search(\n                query=query,\n                limit=limit\n            )\n\n        # Assemble context\n        context = \"\\n\\n\".join(\n            f\"[{i+1}] {doc.text}\"\n            for i, doc in enumerate(retrieved)\n        )\n\n        # Generate answer\n        prompt = f\"\"\"Answer the question based on the context below.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n\n        answer = await self.llm_client.generate(prompt)\n\n        return {\n            \"answer\": answer,\n            \"sources\": retrieved,\n            \"context\": context\n        }\n\n# Usage\npipeline = HybridRagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=OpenAILLM(),\n    alpha=0.7\n)\n\n# Query with hybrid search\nresult = await pipeline.query(\n    \"What are the key features of Python?\",\n    use_hybrid=True\n)\n\nprint(result[\"answer\"])\n</code></pre>"},{"location":"examples/hybrid_search/#query-specific-hybrid-strategies","title":"Query-Specific Hybrid Strategies","text":""},{"location":"examples/hybrid_search/#multi-strategy-hybrid","title":"Multi-Strategy Hybrid","text":"Python<pre><code>class MultiStrategyHybrid:\n    \"\"\"Hybrid search with multiple strategies.\"\"\"\n\n    def __init__(\n        self,\n        vector_store: MilvusVectorStore,\n        embedding: OpenAIEmbedding,\n    ):\n        self.vector_store = vector_store\n        self.embedding = embedding\n\n    async def search(\n        self,\n        query: str,\n        limit: int = 10,\n        strategy: str = \"auto\",\n    ) -&gt; list[SearchResult]:\n        \"\"\"Multi-strategy hybrid search.\n\n        Args:\n            query: Search query\n            limit: Number of results\n            strategy: \"vector\", \"keyword\", \"balanced\", \"auto\"\n        \"\"\"\n        if strategy == \"auto\":\n            strategy = self._detect_strategy(query)\n\n        if strategy == \"vector\":\n            # Pure vector search\n            return await self.vector_store.search(query, limit)\n\n        elif strategy == \"keyword\":\n            # Pure keyword search\n            return await self._keyword_search(query, limit)\n\n        elif strategy == \"balanced\":\n            # Balanced hybrid\n            searcher = HybridSearcher(\n                vector_store=self.vector_store,\n                embedding_client=self.embedding,\n                alpha=0.5\n            )\n            return await searcher.search(query, limit)\n\n        else:\n            raise ValueError(f\"Unknown strategy: {strategy}\")\n\n    def _detect_strategy(self, query: str) -&gt; str:\n        \"\"\"Auto-detect best strategy for query.\"\"\"\n        # Question words \u2192 vector\n        if any(query.lower().startswith(w) for w in [\"what\", \"how\", \"why\"]):\n            return \"vector\"\n\n        # Exact phrases \u2192 keyword\n        if '\"' in query:\n            return \"keyword\"\n\n        # IDs or codes \u2192 keyword\n        if re.search(r'[A-Z0-9]{3,}[-_]', query):\n            return \"keyword\"\n\n        # Default: balanced\n        return \"balanced\"\n\n    async def _keyword_search(\n        self,\n        query: str,\n        limit: int\n    ) -&gt; list[SearchResult]:\n        \"\"\"Pure keyword search implementation.\"\"\"\n        # Implementation details...\n        pass\n</code></pre>"},{"location":"examples/hybrid_search/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/hybrid_search/#parallel-search","title":"Parallel Search","text":"<p>Execute vector and keyword searches in parallel:</p> Python<pre><code>import asyncio\n\nclass ParallelHybridSearch:\n    \"\"\"Hybrid search with parallel execution.\"\"\"\n\n    async def search(\n        self,\n        query: str,\n        limit: int = 10\n    ) -&gt; list[SearchResult]:\n        \"\"\"Parallel hybrid search.\"\"\"\n        # Execute searches in parallel\n        vector_task = self.vector_store.search(query, limit * 2)\n        keyword_task = self._keyword_search(query, limit * 2)\n\n        vector_results, keyword_results = await asyncio.gather(\n            vector_task,\n            keyword_task\n        )\n\n        # Combine results\n        return self._reciprocal_rank_fusion(\n            vector_results,\n            keyword_results\n        )[:limit]\n</code></pre>"},{"location":"examples/hybrid_search/#caching","title":"Caching","text":"<p>Cache hybrid search results:</p> Python<pre><code>from functools import lru_cache\nimport hashlib\n\nclass CachedHybridSearch:\n    \"\"\"Hybrid search with result caching.\"\"\"\n\n    def __init__(self, vector_store, embedding):\n        self.vector_store = vector_store\n        self.embedding = embedding\n        self._cache = {}\n\n    async def search(\n        self,\n        query: str,\n        limit: int = 10,\n        use_cache: bool = True\n    ) -&gt; list[SearchResult]:\n        \"\"\"Cached hybrid search.\"\"\"\n        # Generate cache key\n        cache_key = self._make_cache_key(query, limit)\n\n        # Check cache\n        if use_cache and cache_key in self._cache:\n            return self._cache[cache_key]\n\n        # Perform search\n        results = await self._hybrid_search(query, limit)\n\n        # Cache results\n        if use_cache:\n            self._cache[cache_key] = results\n\n        return results\n\n    def _make_cache_key(self, query: str, limit: int) -&gt; str:\n        \"\"\"Generate cache key.\"\"\"\n        content = f\"{query}:{limit}\"\n        return hashlib.md5(content.encode()).hexdigest()\n</code></pre>"},{"location":"examples/hybrid_search/#evaluation","title":"Evaluation","text":""},{"location":"examples/hybrid_search/#compare-strategies","title":"Compare Strategies","text":"Python<pre><code>async def evaluate_hybrid_search(\n    queries: list[str],\n    ground_truth: dict[str, list[str]],\n    searcher: HybridSearcher,\n):\n    \"\"\"Evaluate hybrid search performance.\"\"\"\n    metrics = {\n        \"precision@5\": [],\n        \"recall@5\": [],\n        \"mrr\": []\n    }\n\n    for query in queries:\n        # Search\n        results = await searcher.search(query, limit=5)\n        retrieved_ids = [r.id for r in results]\n\n        # Ground truth\n        relevant_ids = ground_truth.get(query, [])\n\n        if not relevant_ids:\n            continue\n\n        # Precision@5\n        relevant_retrieved = set(retrieved_ids) &amp; set(relevant_ids)\n        precision = len(relevant_retrieved) / len(retrieved_ids)\n        metrics[\"precision@5\"].append(precision)\n\n        # Recall@5\n        recall = len(relevant_retrieved) / len(relevant_ids)\n        metrics[\"recall@5\"].append(recall)\n\n        # MRR\n        for i, doc_id in enumerate(retrieved_ids):\n            if doc_id in relevant_ids:\n                metrics[\"mrr\"].append(1.0 / (i + 1))\n                break\n        else:\n            metrics[\"mrr\"].append(0.0)\n\n    # Average metrics\n    return {\n        key: sum(values) / len(values)\n        for key, values in metrics.items()\n    }\n\n# Compare strategies\nresults_vector = await evaluate_hybrid_search(\n    queries, ground_truth, \n    HybridSearcher(alpha=1.0)  # Pure vector\n)\n\nresults_keyword = await evaluate_hybrid_search(\n    queries, ground_truth,\n    HybridSearcher(alpha=0.0)  # Pure keyword\n)\n\nresults_hybrid = await evaluate_hybrid_search(\n    queries, ground_truth,\n    HybridSearcher(alpha=0.7)  # Hybrid\n)\n\nprint(\"Vector:\", results_vector)\nprint(\"Keyword:\", results_keyword)\nprint(\"Hybrid:\", results_hybrid)\n</code></pre>"},{"location":"examples/hybrid_search/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Alpha</li> <li>Start with 0.7 (70% vector, 30% keyword)</li> <li>Adjust based on your data and queries</li> <li> <p>Use adaptive weighting for different query types</p> </li> <li> <p>Optimize Both Search Paths</p> </li> <li>Tune vector index (HNSW, IVF)</li> <li>Optimize keyword index (BM25 parameters)</li> <li> <p>Consider caching frequently used results</p> </li> <li> <p>Use Reciprocal Rank Fusion</p> </li> <li>More robust than weighted averaging</li> <li>Less sensitive to score distribution</li> <li> <p>Works well with different score ranges</p> </li> <li> <p>Test on Real Queries</p> </li> <li>Evaluate on your actual use case</li> <li>Compare pure vs. hybrid approaches</li> <li> <p>A/B test different alpha values</p> </li> <li> <p>Monitor Performance</p> </li> <li>Track search latency</li> <li>Measure retrieval quality</li> <li>Log query patterns for tuning</li> </ol>"},{"location":"examples/hybrid_search/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Pipeline - Complete RAG pipeline</li> <li>Reranking Guide - Improve ranking quality</li> <li>Vector Stores - Vector search deep dive</li> </ul>"},{"location":"examples/hybrid_search/#see-also","title":"See Also","text":"<ul> <li>Reciprocal Rank Fusion Paper</li> <li>BM25 Algorithm</li> <li>Milvus Hybrid Search</li> </ul>"},{"location":"examples/production_setup/","title":"Production Setup","text":""},{"location":"examples/production_setup/#production-setup-guide","title":"Production Setup Guide","text":"<p>Deploy to Production</p> <p>Deploy your RAG application to production with Docker, monitoring, scaling, and best practices.</p>"},{"location":"examples/production_setup/#docker-deployment","title":"Docker Deployment","text":""},{"location":"examples/production_setup/#complete-docker-setup","title":"Complete Docker Setup","text":"<p>Docker Compose Configuration</p> <p>Use this complete docker-compose file for production deployment.</p> docker-compose.yml<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  # Milvus vector database\n  etcd:\n    container_name: milvus-etcd\n    image: quay.io/coreos/etcd:v3.5.5\n    environment:\n      - ETCD_AUTO_COMPACTION_MODE=revision\n      - ETCD_AUTO_COMPACTION_RETENTION=1000\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n    volumes:\n      - etcd_data:/etcd\n    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n\n  minio:\n    container_name: milvus-minio\n    image: minio/minio:RELEASE.2023-03-20T20-16-18Z\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    volumes:\n      - minio_data:/minio_data\n    command: minio server /minio_data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n\n  milvus:\n    container_name: milvus-standalone\n    image: milvusdb/milvus:v2.3.3\n    command: [\"milvus\", \"run\", \"standalone\"]\n    environment:\n      ETCD_ENDPOINTS: etcd:2379\n      MINIO_ADDRESS: minio:9000\n    volumes:\n      - milvus_data:/var/lib/milvus\n    ports:\n      - \"19530:19530\"\n      - \"9091:9091\"\n    depends_on:\n      - \"etcd\"\n      - \"minio\"\n\n  # RAG API service\n  rag-api:\n    build: .\n    container_name: rag-api\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MILVUS_HOST=milvus\n      - MILVUS_PORT=19530\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n    depends_on:\n      - milvus\n    volumes:\n      - ./data:/app/data\n      - ./logs:/app/logs\n\n  # Redis for caching\n  redis:\n    image: redis:7-alpine\n    container_name: rag-redis\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  # Monitoring with Prometheus\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: rag-prometheus\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_data:/prometheus\n\n  # Grafana for dashboards\n  grafana:\n    image: grafana/grafana:latest\n    container_name: rag-grafana\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    volumes:\n      - grafana_data:/var/lib/grafana\n\nvolumes:\n  etcd_data:\n  minio_data:\n  milvus_data:\n  redis_data:\n  prometheus_data:\n  grafana_data:\n</code></pre>"},{"location":"examples/production_setup/#dockerfile","title":"Dockerfile","text":"Docker<pre><code># Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Create directories\nRUN mkdir -p /app/data /app/logs\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=40s \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# Run application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"examples/production_setup/#requirements","title":"Requirements","text":"Text Only<pre><code># requirements.txt\nrag-toolkit&gt;=0.1.0\nfastapi&gt;=0.104.0\nuvicorn[standard]&gt;=0.24.0\nredis&gt;=5.0.0\nprometheus-client&gt;=0.19.0\npython-multipart&gt;=0.0.6\naiofiles&gt;=23.2.1\n</code></pre>"},{"location":"examples/production_setup/#production-application","title":"Production Application","text":""},{"location":"examples/production_setup/#fastapi-service","title":"FastAPI Service","text":"Python<pre><code># main.py\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nimport redis.asyncio as redis\nimport hashlib\nimport json\nfrom rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\nfrom rag_toolkit.infra.llm import OpenAILLM\nfrom rag_toolkit.core.vectorstore import MilvusVectorStore\nfrom prometheus_client import Counter, Histogram, generate_latest\nimport time\nimport logging\n\n# Logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Metrics\nQUERY_COUNTER = Counter('rag_queries_total', 'Total RAG queries')\nQUERY_LATENCY = Histogram('rag_query_latency_seconds', 'Query latency')\nCACHE_HITS = Counter('rag_cache_hits_total', 'Cache hits')\nCACHE_MISSES = Counter('rag_cache_misses_total', 'Cache misses')\n\napp = FastAPI(title=\"RAG API\", version=\"1.0.0\")\n\n# CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Redis cache\ncache: redis.Redis = None\n\n# RAG pipeline\npipeline: RagPipeline = None\n\nclass QueryRequest(BaseModel):\n    query: str\n    limit: int = 5\n    use_cache: bool = True\n\nclass QueryResponse(BaseModel):\n    answer: str\n    sources: list[dict]\n    cached: bool = False\n    latency_ms: float\n\n@app.on_event(\"startup\")\nasync def startup():\n    \"\"\"Initialize services.\"\"\"\n    global cache, pipeline\n\n    # Redis cache\n    cache = await redis.from_url(\n        \"redis://redis:6379\",\n        encoding=\"utf-8\",\n        decode_responses=True\n    )\n\n    # RAG pipeline\n    pipeline = RagPipeline(\n        embedding_client=OpenAIEmbedding(\n            model=\"text-embedding-3-small\"\n        ),\n        vector_store=MilvusVectorStore(\n            host=\"milvus\",\n            port=19530,\n            collection_name=\"documents\",\n            dimension=1536\n        ),\n        llm_client=OpenAILLM(\n            model=\"gpt-4-turbo\"\n        )\n    )\n\n    logger.info(\"Services initialized\")\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    \"\"\"Cleanup.\"\"\"\n    await cache.close()\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check.\"\"\"\n    return {\"status\": \"healthy\"}\n\n@app.post(\"/query\", response_model=QueryResponse)\nasync def query(request: QueryRequest):\n    \"\"\"Query endpoint with caching.\"\"\"\n    QUERY_COUNTER.inc()\n    start_time = time.time()\n\n    try:\n        # Check cache\n        cached_result = None\n        if request.use_cache:\n            cache_key = _make_cache_key(request.query, request.limit)\n            cached_result = await cache.get(cache_key)\n\n            if cached_result:\n                CACHE_HITS.inc()\n                result = json.loads(cached_result)\n                latency = (time.time() - start_time) * 1000\n\n                return QueryResponse(\n                    **result,\n                    cached=True,\n                    latency_ms=latency\n                )\n\n            CACHE_MISSES.inc()\n\n        # Query RAG pipeline\n        result = await pipeline.query(\n            query=request.query,\n            limit=request.limit\n        )\n\n        # Format response\n        response_data = {\n            \"answer\": result.answer,\n            \"sources\": [\n                {\n                    \"text\": src.text,\n                    \"metadata\": src.metadata,\n                    \"score\": src.score\n                }\n                for src in result.sources\n            ]\n        }\n\n        # Cache result\n        if request.use_cache:\n            await cache.setex(\n                cache_key,\n                3600,  # 1 hour TTL\n                json.dumps(response_data)\n            )\n\n        latency = (time.time() - start_time) * 1000\n        QUERY_LATENCY.observe(latency / 1000)\n\n        return QueryResponse(\n            **response_data,\n            cached=False,\n            latency_ms=latency\n        )\n\n    except Exception as e:\n        logger.error(f\"Query error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/metrics\")\nasync def metrics():\n    \"\"\"Prometheus metrics.\"\"\"\n    return generate_latest()\n\ndef _make_cache_key(query: str, limit: int) -&gt; str:\n    \"\"\"Generate cache key.\"\"\"\n    content = f\"{query}:{limit}\"\n    return f\"rag:{hashlib.md5(content.encode()).hexdigest()}\"\n</code></pre>"},{"location":"examples/production_setup/#monitoring","title":"Monitoring","text":""},{"location":"examples/production_setup/#prometheus-configuration","title":"Prometheus Configuration","text":"YAML<pre><code># prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'rag-api'\n    static_configs:\n      - targets: ['rag-api:8000']\n    metrics_path: '/metrics'\n</code></pre>"},{"location":"examples/production_setup/#grafana-dashboard","title":"Grafana Dashboard","text":"JSON<pre><code>{\n  \"dashboard\": {\n    \"title\": \"RAG Pipeline Metrics\",\n    \"panels\": [\n      {\n        \"title\": \"Query Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(rag_queries_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Query Latency\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rag_query_latency_seconds)\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Cache Hit Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(rag_cache_hits_total[5m]) / (rate(rag_cache_hits_total[5m]) + rate(rag_cache_misses_total[5m]))\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"examples/production_setup/#scaling","title":"Scaling","text":""},{"location":"examples/production_setup/#horizontal-scaling","title":"Horizontal Scaling","text":"YAML<pre><code># docker-compose.scale.yml\nversion: '3.8'\n\nservices:\n  rag-api:\n    deploy:\n      replicas: 4\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n        reservations:\n          cpus: '1'\n          memory: 2G\n\n  # Load balancer\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - rag-api\n</code></pre>"},{"location":"examples/production_setup/#nginx-configuration","title":"Nginx Configuration","text":"Nginx Configuration File<pre><code># nginx.conf\nupstream rag_backend {\n    least_conn;\n    server rag-api-1:8000;\n    server rag-api-2:8000;\n    server rag-api-3:8000;\n    server rag-api-4:8000;\n}\n\nserver {\n    listen 80;\n\n    location / {\n        proxy_pass http://rag_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        # Timeouts\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n}\n</code></pre>"},{"location":"examples/production_setup/#best-practices","title":"Best Practices","text":""},{"location":"examples/production_setup/#1-environment-configuration","title":"1. Environment Configuration","text":"Python<pre><code># config.py\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    # Milvus\n    milvus_host: str = \"localhost\"\n    milvus_port: int = 19530\n\n    # OpenAI\n    openai_api_key: str\n    openai_model: str = \"gpt-4-turbo\"\n    embedding_model: str = \"text-embedding-3-small\"\n\n    # Redis\n    redis_url: str = \"redis://localhost:6379\"\n    cache_ttl: int = 3600\n\n    # Performance\n    max_workers: int = 4\n    request_timeout: int = 60\n\n    class Config:\n        env_file = \".env\"\n\nsettings = Settings()\n</code></pre>"},{"location":"examples/production_setup/#2-error-handling","title":"2. Error Handling","text":"Python<pre><code>from fastapi import Request\nfrom fastapi.responses import JSONResponse\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request: Request, exc: Exception):\n    \"\"\"Global error handler.\"\"\"\n    logger.error(f\"Global error: {exc}\", exc_info=True)\n\n    return JSONResponse(\n        status_code=500,\n        content={\n            \"error\": \"Internal server error\",\n            \"message\": str(exc)\n        }\n    )\n</code></pre>"},{"location":"examples/production_setup/#3-rate-limiting","title":"3. Rate Limiting","text":"Python<pre><code>from fastapi_limiter import FastAPILimiter\nfrom fastapi_limiter.depends import RateLimiter\n\n@app.on_event(\"startup\")\nasync def startup():\n    await FastAPILimiter.init(cache)\n\n@app.post(\"/query\", dependencies=[Depends(RateLimiter(times=10, seconds=60))])\nasync def query(request: QueryRequest):\n    \"\"\"Rate-limited query endpoint.\"\"\"\n    # ...\n</code></pre>"},{"location":"examples/production_setup/#4-logging","title":"4. Logging","text":"Python<pre><code>import logging\nfrom logging.handlers import RotatingFileHandler\n\n# Configure logging\nhandler = RotatingFileHandler(\n    \"logs/rag.log\",\n    maxBytes=10_000_000,  # 10MB\n    backupCount=5\n)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[handler, logging.StreamHandler()]\n)\n</code></pre>"},{"location":"examples/production_setup/#deployment","title":"Deployment","text":""},{"location":"examples/production_setup/#development","title":"Development","text":"Bash<pre><code># Start services\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f rag-api\n\n# Stop services\ndocker-compose down\n</code></pre>"},{"location":"examples/production_setup/#production","title":"Production","text":"Bash<pre><code># Build and push image\ndocker build -t myregistry/rag-api:latest .\ndocker push myregistry/rag-api:latest\n\n# Deploy with scaling\ndocker-compose -f docker-compose.yml -f docker-compose.scale.yml up -d\n\n# Update service\ndocker-compose pull rag-api\ndocker-compose up -d rag-api\n</code></pre>"},{"location":"examples/production_setup/#kubernetes","title":"Kubernetes","text":"YAML<pre><code># kubernetes/deployment.yml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rag-api\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: rag-api\n  template:\n    metadata:\n      labels:\n        app: rag-api\n    spec:\n      containers:\n      - name: rag-api\n        image: myregistry/rag-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MILVUS_HOST\n          value: \"milvus-service\"\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: openai-secret\n              key: api-key\n        resources:\n          limits:\n            cpu: \"2\"\n            memory: \"4Gi\"\n          requests:\n            cpu: \"1\"\n            memory: \"2Gi\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: rag-api-service\nspec:\n  selector:\n    app: rag-api\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer\n</code></pre>"},{"location":"examples/production_setup/#performance-tuning","title":"Performance Tuning","text":""},{"location":"examples/production_setup/#1-batch-processing","title":"1. Batch Processing","text":"Python<pre><code>async def batch_query(queries: list[str]) -&gt; list[dict]:\n    \"\"\"Process multiple queries in parallel.\"\"\"\n    tasks = [pipeline.query(q) for q in queries]\n    results = await asyncio.gather(*tasks)\n    return results\n</code></pre>"},{"location":"examples/production_setup/#2-connection-pooling","title":"2. Connection Pooling","text":"Python<pre><code>from pymilvus import connections\n\n# Configure connection pool\nconnections.connect(\n    alias=\"default\",\n    host=\"milvus\",\n    port=19530,\n    pool_size=10\n)\n</code></pre>"},{"location":"examples/production_setup/#3-caching-strategy","title":"3. Caching Strategy","text":"Python<pre><code># Multi-level caching\nclass CacheManager:\n    def __init__(self):\n        self.memory_cache = {}  # In-memory\n        self.redis_cache = redis_client  # Distributed\n\n    async def get(self, key: str):\n        # Check memory first\n        if key in self.memory_cache:\n            return self.memory_cache[key]\n\n        # Check Redis\n        value = await self.redis_cache.get(key)\n        if value:\n            self.memory_cache[key] = value\n\n        return value\n</code></pre>"},{"location":"examples/production_setup/#security","title":"Security","text":""},{"location":"examples/production_setup/#1-api-authentication","title":"1. API Authentication","text":"Python<pre><code>from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\nsecurity = HTTPBearer()\n\n@app.post(\"/query\")\nasync def query(\n    request: QueryRequest,\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Protected endpoint.\"\"\"\n    # Verify token\n    if not verify_token(credentials.credentials):\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n\n    # Process query\n    # ...\n</code></pre>"},{"location":"examples/production_setup/#2-input-validation","title":"2. Input Validation","text":"Python<pre><code>from pydantic import BaseModel, validator\n\nclass QueryRequest(BaseModel):\n    query: str\n    limit: int = 5\n\n    @validator('query')\n    def validate_query(cls, v):\n        if len(v) &gt; 1000:\n            raise ValueError('Query too long')\n        if not v.strip():\n            raise ValueError('Query cannot be empty')\n        return v\n\n    @validator('limit')\n    def validate_limit(cls, v):\n        if v &lt; 1 or v &gt; 100:\n            raise ValueError('Limit must be between 1 and 100')\n        return v\n</code></pre>"},{"location":"examples/production_setup/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Vector Store - Implement custom stores</li> <li>Hybrid Search - Advanced search</li> <li>RAG Pipeline Guide - Pipeline details</li> </ul>"},{"location":"examples/production_setup/#resources","title":"Resources","text":"<ul> <li>Docker Documentation</li> <li>FastAPI Documentation</li> <li>Kubernetes Documentation</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#getting-started","title":"Getting Started","text":"<p>Welcome to RAG Toolkit! This section will help you get up and running quickly.</p>"},{"location":"getting_started/#what-is-rag-toolkit","title":"What is RAG Toolkit?","text":"<p>RAG Toolkit is a production-ready library for building Retrieval-Augmented Generation (RAG) systems with:</p> <ul> <li>Protocol-based architecture for maximum flexibility</li> <li>Multi-vectorstore support (Milvus, Qdrant, ChromaDB)</li> <li>Multiple LLM providers (Ollama, OpenAI)</li> <li>Production-grade features (migration tools, benchmarks, validation)</li> </ul>"},{"location":"getting_started/#quick-links","title":"Quick Links","text":"<ul> <li> <p> Installation</p> <p>Install RAG Toolkit with pip or uv in seconds</p> </li> <li> <p> Quick Start</p> <p>Build your first RAG application in 5 minutes</p> </li> <li> <p>:material-architecture: Architecture</p> <p>Understand the protocol-based design</p> </li> </ul>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ol> <li>Install RAG Toolkit</li> <li>Quick Start - Build a basic RAG pipeline</li> <li>User Guide - Learn core concepts</li> <li>Examples - See real-world applications</li> </ol>"},{"location":"getting_started/architecture/","title":"Architecture","text":""},{"location":"getting_started/architecture/#architecture-overview","title":"Architecture Overview","text":"<p>RAG Toolkit is built with a clean, layered architecture emphasizing testability, extensibility, and maintainability.</p>"},{"location":"getting_started/architecture/#design-principles","title":"Design Principles","text":""},{"location":"getting_started/architecture/#1-protocol-based-design","title":"1.  Protocol-Based Design","text":"<p>Type-Safe Duck Typing</p> <p>We use Python Protocols instead of abstract base classes for maximum flexibility.</p> Example Protocol<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass EmbeddingClient(Protocol):\n    \"\"\"Any class with this signature is an EmbeddingClient.\"\"\"\n    def embed(self, text: str) -&gt; list[float]: ...\n</code></pre> <p>Benefits:</p> <ul> <li> <p> No Inheritance Required</p> <p>Implement the protocol without extending base classes</p> </li> <li> <p> Duck Typing with Type Safety</p> <p>Get runtime checks and IDE support</p> </li> <li> <p> Easy Mocking for Tests</p> <p>Create simple mock objects without complex setup</p> </li> <li> <p> Flexible Implementations</p> <p>Swap implementations without changing code</p> </li> </ul>"},{"location":"getting_started/architecture/#2-dependency-injection","title":"2.  Dependency Injection","text":"<p>Explicit Dependencies</p> <p>Components receive their dependencies explicitly through constructor injection.</p> Dependency Injection Example<pre><code>pipeline = RagPipeline(\n    embedding_client=my_embedding,  # Injected\n    llm_client=my_llm,              # Injected\n    vector_store=my_store,          # Injected\n)\n</code></pre> <p>Benefits:</p> Benefit Description Testability Easy to inject mocks for unit tests Clarity Dependencies are explicit and visible Flexibility Change implementations at runtime :material-ban: No Global State Avoid singleton and global variables"},{"location":"getting_started/architecture/#3-layered-architecture","title":"3.  Layered Architecture","text":"<p>Clean Separation of Concerns</p> <pre><code>graph TB\n    subgraph Application[\"\ud83d\ude80 Application Layer\"]\n        APP[Your RAG Applications]\n    end\n\n    subgraph Pipeline[\"\u2699\ufe0f RAG Pipeline Layer\"]\n        RP[RagPipeline]\n        QR[QueryRewriter]\n        CA[ContextAssembler]\n    end\n\n    subgraph Infrastructure[\"\ud83d\udd27 Infrastructure Layer\"]\n        OE[OllamaEmbedding]\n        OL[OpenAILLMClient]\n        MV[MilvusVectorStore]\n    end\n\n    subgraph Core[\"\ud83d\udce6 Core Layer\"]\n        EC[EmbeddingClient Protocol]\n        LC[LLMClient Protocol]\n        VC[VectorStoreClient Protocol]\n    end\n\n    APP --&gt; RP\n    RP --&gt; QR\n    RP --&gt; CA\n    RP --&gt; OE\n    RP --&gt; OL\n    RP --&gt; MV\n    OE -.implements.-&gt; EC\n    OL -.implements.-&gt; LC\n    MV -.implements.-&gt; VC\n\n    style Application fill:#e3f2fd\n    style Pipeline fill:#fff3e0\n    style Infrastructure fill:#f3e5f5\n    style Core fill:#e8f5e9</code></pre> <p>Each layer has a specific responsibility and depends only on layers below it.</p>"},{"location":"getting_started/architecture/#core-layer","title":"Core Layer","text":"<p>Foundation Layer</p> <p>Contains only protocol definitions with zero external dependencies.</p>"},{"location":"getting_started/architecture/#embeddingclient-protocol","title":"EmbeddingClient Protocol","text":"core/embedding/base.py<pre><code>@runtime_checkable\nclass EmbeddingClient(Protocol):\n    \"\"\"Protocol for embedding text into vectors.\"\"\"\n\n    def embed(self, text: str) -&gt; list[float]:\n        \"\"\"Embed a single text into a vector.\n\n        Args:\n            text: Input text to embed\n\n        Returns:\n            Vector representation as list of floats\n        \"\"\"\n        ...\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Embed multiple texts efficiently.\n\n        Args:\n            texts: List of texts to embed\n\n        Returns:\n            List of vectors\n        \"\"\"\n        ...\n</code></pre> <p>Protocol Benefits</p> <p>Any class implementing these methods can be used as an <code>EmbeddingClient</code>\u2014no inheritance needed!</p>"},{"location":"getting_started/architecture/#llmclient-protocol","title":"LLMClient Protocol","text":"core/llm/base.py<pre><code>@runtime_checkable\nclass LLMClient(Protocol):\n    \"\"\"Protocol for language model clients.\"\"\"\n\n    def generate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Generate text from a prompt.\n\n        Args:\n            prompt: Input prompt\n            max_tokens: Maximum tokens to generate\n            temperature: Sampling temperature\n            **kwargs: Provider-specific options\n\n        Returns:\n            Generated text\n        \"\"\"\n        ...\n\n    async def agenerate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Async version of generate.\"\"\"\n        ...\n</code></pre>"},{"location":"getting_started/architecture/#vectorstoreclient-protocol","title":"VectorStoreClient Protocol","text":"core/vectorstore/base.py<pre><code>@runtime_checkable\nclass VectorStoreClient(Protocol):\n    \"\"\"Protocol for vector store operations.\"\"\"\n\n    def create_collection(\n        self,\n        name: str,\n        dimension: int,\n        metric: str = \"IP\",\n        **kwargs\n    ) -&gt; None:\n        \"\"\"Create a new collection for vectors.\"\"\"\n        ...\n\n    def insert(\n        self,\n        collection_name: str,\n        vectors: list[list[float]],\n        texts: list[str],\n        metadata: list[dict],\n        **kwargs\n    ) -&gt; list[str]:\n        \"\"\"Insert vectors into collection.\n\n        Returns:\n            List of IDs for inserted vectors\n        \"\"\"\n        ...\n\n    def search(\n        self,\n        collection_name: str,\n        query_vector: list[float],\n        top_k: int = 5,\n        filters: dict | None = None,\n        **kwargs\n    ) -&gt; list[SearchResult]:\n        \"\"\"Search for similar vectors.\n\n        Returns:\n            List of SearchResult objects\n        \"\"\"\n        ...\n</code></pre>"},{"location":"getting_started/architecture/#infrastructure-layer","title":"Infrastructure Layer","text":"<p>Concrete Implementations</p> <p>Real implementations of core protocols for various providers.</p>"},{"location":"getting_started/architecture/#embedding-implementations","title":"Embedding Implementations","text":"OllamaOpenAI Python<pre><code>from rag_toolkit.infra.embedding.ollama import OllamaEmbedding\n\nembedding = OllamaEmbedding(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\"\n)\n\n# Implements EmbeddingClient Protocol \u2713\nvector = embedding.embed(\"Hello world\")\n</code></pre> <p>Features: -  Local embedding models -  Privacy-focused (offline capable) -  No API costs</p> Python<pre><code>from rag_toolkit.infra.embedding.openai_embedding import OpenAIEmbedding\n\nembedding = OpenAIEmbedding(\n    api_key=\"your-api-key\",\n    model=\"text-embedding-3-small\"\n)\n\n# Implements EmbeddingClient Protocol \u2713\nvector = embedding.embed(\"Hello world\")\n</code></pre> <p>Features: -  Cloud-based models -  High performance -  Pay-per-use pricing</p>"},{"location":"getting_started/architecture/#llm-implementations","title":"LLM Implementations","text":"OllamaOpenAI Python<pre><code>from rag_toolkit.infra.llm.ollama import OllamaLLMClient\n\nllm = OllamaLLMClient(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\"\n)\n\n# Implements LLMClient Protocol \u2713\nresponse = llm.generate(\"Explain RAG\")\n</code></pre> Python<pre><code>from rag_toolkit.infra.llm.openai_llm import OpenAILLMClient\n\nllm = OpenAILLMClient(\n    api_key=\"your-api-key\",\n    model=\"gpt-4\"\n)\n\n# Implements LLMClient Protocol \u2713\nresponse = llm.generate(\"Explain RAG\")\n</code></pre>"},{"location":"getting_started/architecture/#vector-store-implementations","title":"Vector Store Implementations","text":"<ul> <li> <p> MilvusVectorStore</p> Python<pre><code>from rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\n\nstore = MilvusVectorStore(\n    host=\"localhost\",\n    port=\"19530\",\n    collection_name=\"my_docs\"\n)\n\n# Implements VectorStoreClient Protocol \u2713\nstore.create_collection(\"docs\", dimension=768)\n</code></pre> <p>Production-ready, high-performance vector database</p> </li> <li> <p> PineconeVectorStore</p> <p>Coming soon! Cloud-native vector database.</p> </li> <li> <p> QdrantVectorStore</p> <p>Coming soon! Vector search engine with extended filtering.</p> </li> </ul>"},{"location":"getting_started/architecture/#material-pipeline-rag-pipeline-layer","title":":material-pipeline: RAG Pipeline Layer","text":"<p>High-Level Orchestration</p> <p>Business logic and workflow orchestration for RAG operations.</p>"},{"location":"getting_started/architecture/#ragpipeline","title":"RagPipeline","text":"<p>Main Entry Point</p> <p>The central orchestrator for all RAG operations.</p> rag/pipeline.py<pre><code>pipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n    chunk_size=512,\n    chunk_overlap=50,\n)\n\n# Orchestrates: chunk \u2192 embed \u2192 store\npipeline.index_documents(documents)\n\n# Orchestrates: embed \u2192 search \u2192 rerank \u2192 generate\nresponse = pipeline.query(\"What is RAG?\")\n</code></pre> <p>Pipeline Workflow:</p> <pre><code>graph LR\n    A[Documents] --&gt;|chunk| B[Chunks]\n    B --&gt;|embed| C[Vectors]\n    C --&gt;|store| D[Vector DB]\n\n    Q[Query] --&gt;|embed| E[Query Vector]\n    E --&gt;|search| D\n    D --&gt;|retrieve| F[Context]\n    F --&gt;|generate| G[Response]\n\n    style A fill:#e3f2fd\n    style G fill:#c8e6c9</code></pre>"},{"location":"getting_started/architecture/#queryrewriter","title":"QueryRewriter","text":"<p>Query Enhancement</p> <p>Improve retrieval quality by rewriting and expanding queries.</p> HyDEMulti-Query Python<pre><code>from rag_toolkit.rag.rewriter import QueryRewriter\n\nrewriter = QueryRewriter(llm_client=llm)\n\n# Hypothetical Document Embeddings\nexpanded = rewriter.rewrite_hyde(\n    query=\"What is RAG?\",\n    num_variations=3\n)\n</code></pre> <p>Generates hypothetical documents that might answer the query.</p> Python<pre><code># Generate multiple query variations\nqueries = rewriter.generate_multi_query(\n    query=\"Explain embeddings\",\n    num_queries=5\n)\n</code></pre> <p>Creates diverse query formulations for better coverage.</p>"},{"location":"getting_started/architecture/#contextassembler","title":"ContextAssembler","text":"<p>Context Management</p> <p>Intelligently assemble retrieved chunks into coherent context.</p> rag/assembler.py<pre><code>from rag_toolkit.rag.assembler import ContextAssembler\n\nassembler = ContextAssembler(\n    max_context_length=2048,\n    include_metadata=True\n)\n\ncontext = assembler.assemble(\n    chunks=retrieved_chunks,\n    query=\"What is RAG?\",\n    format=\"markdown\"\n)\n</code></pre> <p>Features:</p> Feature Description Formatting Output in markdown, text, or JSON Metadata Include source information and scores Truncation Smart truncation to fit token limits Reranking Optional reranking by relevance"},{"location":"getting_started/architecture/#data-flow","title":"Data Flow","text":"<p>Understanding the Pipeline</p>"},{"location":"getting_started/architecture/#indexing-flow","title":"Indexing Flow","text":"<pre><code>graph TB\n    A[\ud83d\udcc4 Documents] --&gt; B{Chunking}\n    B --&gt; C[\ud83d\udcdd Text Chunks]\n    C --&gt; D{Embedding}\n    D --&gt; E[\ud83d\udd22 Vectors]\n    E --&gt; F{Storage}\n    F --&gt; G[\ud83d\udcbe Vector Database]\n\n    style A fill:#e3f2fd\n    style G fill:#c8e6c9\n\n    classDef process fill:#fff3e0\n    class B,D,F process</code></pre> <p>Steps:</p> <ol> <li>Chunking: Split documents into manageable pieces</li> <li>Embedding: Convert text chunks to vector representations</li> <li>Storage: Store vectors with metadata in vector database</li> </ol>"},{"location":"getting_started/architecture/#query-flow","title":"Query Flow","text":"<pre><code>graph TB\n    A[\ud83d\udcac User Query] --&gt; B{Query Rewriting}\n    B --&gt; C[\ud83d\udd04 Enhanced Query]\n    C --&gt; D{Embedding}\n    D --&gt; E[\ud83d\udd22 Query Vector]\n    E --&gt; F{Similarity Search}\n    F --&gt; G[\ud83d\udcda Retrieved Chunks]\n    G --&gt; H{Reranking}\n    H --&gt; I[\u2b50 Ranked Chunks]\n    I --&gt; J{Context Assembly}\n    J --&gt; K[\ud83d\udccb Context]\n    K --&gt; L{LLM Generation}\n    L --&gt; M[\u2728 Response]\n\n    style A fill:#e3f2fd\n    style M fill:#c8e6c9\n\n    classDef process fill:#fff3e0\n    class B,D,F,H,J,L process</code></pre> <p>Steps:</p> <ol> <li>Query Rewriting: Enhance query for better retrieval (optional)</li> <li>Embedding: Convert query to vector representation</li> <li>Similarity Search: Find most similar documents in vector DB</li> <li>Reranking: Optionally reorder results by relevance</li> <li>Context Assembly: Combine chunks into coherent context</li> <li>LLM Generation: Generate final answer using context</li> </ol>"},{"location":"getting_started/architecture/#extension-points","title":"Extension Points","text":"<p>Easy Extensibility</p> <p>Protocol-based design makes adding custom components trivial.</p>"},{"location":"getting_started/architecture/#custom-embedding-provider","title":"Custom Embedding Provider","text":"<p>No Inheritance Needed</p> custom_embedding.py<pre><code>class MyEmbeddingProvider:\n    \"\"\"Custom embedding - no inheritance needed!\"\"\"\n\n    def embed(self, text: str) -&gt; list[float]:\n        # Your implementation\n        return [0.1, 0.2, 0.3, ...]\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        # Your batch implementation\n        return [[0.1, ...], [0.2, ...], ...]\n\n# Works seamlessly with RagPipeline! \u2728\npipeline = RagPipeline(\n    embedding_client=MyEmbeddingProvider(),\n    llm_client=llm,\n    vector_store=store,\n)\n</code></pre> <p>Protocol Checking</p> <p>Use <code>isinstance()</code> to verify protocol implementation: </p>Python<pre><code>from rag_toolkit.core import EmbeddingClient\n\nassert isinstance(MyEmbeddingProvider(), EmbeddingClient)\n</code></pre><p></p>"},{"location":"getting_started/architecture/#custom-vector-store","title":"Custom Vector Store","text":"<p>Bring Your Own Database</p> custom_vectorstore.py<pre><code>class MyVectorDB:\n    \"\"\"Custom vector store - Protocol-based!\"\"\"\n\n    def create_collection(self, name, dimension, **kwargs):\n        # Connect to your DB and create collection\n        pass\n\n    def insert(self, collection, vectors, texts, metadata):\n        # Store vectors in your DB\n        return [\"id1\", \"id2\", \"id3\"]\n\n    def search(self, collection, query_vector, top_k):\n        # Search and return SearchResult objects\n        return [SearchResult(id=\"...\", score=0.95, text=\"...\")]\n\n# Seamless integration! \ud83c\udf89\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=MyVectorDB(),  # \u2713 It just works\n)\n</code></pre>"},{"location":"getting_started/architecture/#custom-llm-client","title":"Custom LLM Client","text":"<p>Integrate Any LLM</p> custom_llm.py<pre><code>class MyCustomLLM:\n    \"\"\"Custom LLM implementation\"\"\"\n\n    def generate(self, prompt: str, max_tokens: int = 512, \n                temperature: float = 0.7, **kwargs) -&gt; str:\n        # Your LLM inference logic\n        response = your_model.generate(prompt)\n        return response\n\n    async def agenerate(self, prompt: str, max_tokens: int = 512,\n                       temperature: float = 0.7, **kwargs) -&gt; str:\n        # Async version\n        return await your_async_model.generate(prompt)\n</code></pre>"},{"location":"getting_started/architecture/#testing-strategy","title":"Testing Strategy","text":"<p>Built for Testability</p> <p>Protocol-based design makes testing straightforward and maintainable.</p>"},{"location":"getting_started/architecture/#unit-tests","title":"Unit Tests","text":"<p>Easy Mocking</p> test_pipeline.py<pre><code># Simple mock implementations\nclass MockEmbedding:\n    def embed(self, text: str) -&gt; list[float]:\n        return [0.0] * 768\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        return [[0.0] * 768] * len(texts)\n\ndef test_pipeline():\n    pipeline = RagPipeline(\n        embedding_client=MockEmbedding(),  # Easy mock! \u2713\n        llm_client=MockLLM(),\n        vector_store=MockVectorStore(),\n    )\n\n    result = pipeline.query(\"test\")\n    assert result is not None\n    assert result.answer != \"\"\n</code></pre> <p>No Complex Mocking Frameworks</p> <p>Simple classes with the right methods are all you need!</p>"},{"location":"getting_started/architecture/#material-integration-integration-tests","title":":material-integration: Integration Tests","text":"<p>Test with Real Services</p> test_integration.py<pre><code>@pytest.mark.integration\ndef test_milvus_integration():\n    # Use real Milvus instance\n    store = MilvusVectorStore(\n        host=\"localhost\",\n        port=\"19530\"\n    )\n\n    # Test real operations\n    store.create_collection(\"test\", dimension=768)\n\n    vectors = [[0.1] * 768, [0.2] * 768]\n    texts = [\"doc1\", \"doc2\"]\n    metadata = [{\"source\": \"test\"}] * 2\n\n    ids = store.insert(\"test\", vectors, texts, metadata)\n    assert len(ids) == 2\n\n    results = store.search(\"test\", [0.15] * 768, top_k=5)\n    assert len(results) &gt; 0\n    assert results[0].score &gt; 0\n</code></pre>"},{"location":"getting_started/architecture/#performance-tests","title":"Performance Tests","text":"<p>Benchmark Operations</p> test_performance.py<pre><code>import time\n\ndef test_batch_embedding_performance():\n    embedding = OllamaEmbedding(model=\"nomic-embed-text\")\n    texts = [\"test document\"] * 100\n\n    start = time.time()\n    vectors = embedding.embed_batch(texts)\n    duration = time.time() - start\n\n    # Should process 100 docs in under 5 seconds\n    assert duration &lt; 5.0\n    assert len(vectors) == 100\n</code></pre>"},{"location":"getting_started/architecture/#performance-considerations","title":"Performance Considerations","text":"<p>Optimization Best Practices</p>"},{"location":"getting_started/architecture/#batch-processing","title":"Batch Processing","text":"<p>Avoid: One-at-a-time Processing</p> Python<pre><code># \u274c Slow: Process individually\nvectors = []\nfor text in texts:\n    vector = embedding.embed(text)\n    vectors.append(vector)\n</code></pre> <p>Prefer: Batch Operations</p> Python<pre><code># \u2705 Fast: Batch processing\nvectors = embedding.embed_batch(texts)\n</code></pre> <p>Performance Gain: Up to 10x faster for large batches!</p>"},{"location":"getting_started/architecture/#async-operations","title":"Async Operations","text":"<p>Use Async for I/O-Bound Tasks</p> Async LLM CallsConcurrent Processing Python<pre><code># Single async call\nresponse = await llm.agenerate(prompt)\n\n# Concurrent queries\nprompts = [\"prompt1\", \"prompt2\", \"prompt3\"]\ntasks = [llm.agenerate(p) for p in prompts]\nresponses = await asyncio.gather(*tasks)\n</code></pre> Python<pre><code>import asyncio\n\nasync def process_documents(documents):\n    # Process multiple documents concurrently\n    tasks = [\n        process_single_doc(doc)\n        for doc in documents\n    ]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"getting_started/architecture/#caching","title":"Caching","text":"<p>Cache Expensive Operations</p> cached_embeddings.py<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef cached_embed(text: str) -&gt; tuple[float, ...]:\n    # Cache up to 1000 embeddings\n    return tuple(embedding.embed(text))\n</code></pre> <p>Use Case: Frequently queried terms or repeated documents.</p>"},{"location":"getting_started/architecture/#vector-store-optimization","title":"Vector Store Optimization","text":"<ul> <li> <p> Index Configuration</p> Python<pre><code>store.create_collection(\n    name=\"docs\",\n    dimension=768,\n    index_type=\"IVF_FLAT\",  # Choose appropriate index\n    metric=\"IP\",            # Inner product for normalized vectors\n    nlist=128,              # Number of clusters\n)\n</code></pre> </li> <li> <p> Metadata Filtering</p> Python<pre><code># Filter before similarity search\nresults = store.search(\n    collection=\"docs\",\n    query_vector=vector,\n    top_k=5,\n    filters={\"category\": \"technical\"}  # Reduce search space\n)\n</code></pre> </li> <li> <p>:material-batch-processing: Batch Insertions</p> Python<pre><code># Insert in batches of 1000\nbatch_size = 1000\nfor i in range(0, len(vectors), batch_size):\n    batch_vectors = vectors[i:i+batch_size]\n    batch_texts = texts[i:i+batch_size]\n    store.insert(\"docs\", batch_vectors, batch_texts, metadata)\n</code></pre> </li> </ul>"},{"location":"getting_started/architecture/#performance-metrics","title":"Performance Metrics","text":"Operation Without Optimization With Optimization Speedup Embed 100 texts 5.2s 0.6s 8.7x Concurrent LLM calls 15s 3s 5x Batch insert 10k vectors 120s 18s 6.7x Cached embeddings 0.5s 0.001s 500x"},{"location":"getting_started/architecture/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Core Concepts</p> <p>Dive deeper into RAG fundamentals and best practices</p> <p> Learn</p> </li> <li> <p> Protocols Guide</p> <p>Master protocol-based design patterns</p> <p> Explore</p> </li> <li> <p> Examples</p> <p>See real-world implementations and patterns</p> <p> Browse</p> </li> <li> <p> API Reference</p> <p>Complete technical documentation</p> <p> Reference</p> </li> </ul>"},{"location":"getting_started/architecture/#key-takeaways","title":"Key Takeaways","text":"<p>Design Philosophy</p> <p>\u2705 Protocol-Based: No inheritance required\u2014duck typing with type safety \u2705 Dependency Injection: Explicit, testable, and flexible \u2705 Layered Architecture: Clear separation of concerns \u2705 Zero Core Dependencies: Core layer has no external dependencies \u2705 Easy Testing: Simple mocking without complex frameworks \u2705 Extensible: Add custom implementations effortlessly  </p> <p>Architecture Goals</p> <p>\"Make the simple easy and the complex possible.\"</p> <p>RAG Toolkit achieves this through protocols, dependency injection, and clean layering\u2014enabling both quick prototypes and production-ready systems.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>Get RAG Toolkit up and running in minutes with flexible installation options tailored to your needs.</p>"},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>System Requirements</p> MinimumRecommendedDevelopment <ul> <li>Python: 3.11 or higher</li> <li>Package Manager: pip or conda</li> <li>Memory: 2GB RAM minimum</li> <li>Disk: 500MB free space</li> </ul> <ul> <li>Python: 3.12+</li> <li>Package Manager: pip 24.0+</li> <li>Memory: 8GB RAM (for LLM operations)</li> <li>Disk: 5GB free space</li> <li>GPU: CUDA-compatible (optional, for faster inference)</li> </ul> <ul> <li>All recommended requirements</li> <li>Git: For version control</li> <li>Docker: For running services</li> <li>Make: For build automation</li> </ul>"},{"location":"getting_started/installation/#basic-installation","title":"Basic Installation","text":"<p>Quick Start</p> <p>Install the core library with minimal dependencies:</p> Bash<pre><code>pip install rag-toolkit\n</code></pre> <p>What's included:</p> <ul> <li> Core protocols and types</li> <li> Pydantic models for validation</li> <li> Basic infrastructure components</li> </ul>"},{"location":"getting_started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Modular Installation</p> <p>RAG Toolkit uses optional dependencies to keep the core library lightweight. Install only what you need!</p>"},{"location":"getting_started/installation/#llm-providers","title":"LLM Providers","text":"<ul> <li> <p> Ollama \u2014 Local LLM server</p> Bash<pre><code>pip install rag-toolkit[ollama]\n</code></pre> <p>Perfect for privacy-focused deployments and offline use.</p> </li> <li> <p>:material-openai: OpenAI \u2014 GPT models</p> Bash<pre><code>pip install rag-toolkit[openai]\n</code></pre> <p>Access state-of-the-art models like GPT-4 and GPT-3.5.</p> </li> </ul>"},{"location":"getting_started/installation/#document-parsing","title":"Document Parsing","text":"<ul> <li> <p> PDF Support</p> Bash<pre><code>pip install rag-toolkit[pdf]\n</code></pre> <p>Parse PDF documents with advanced layout detection.</p> </li> <li> <p> DOCX Support</p> Bash<pre><code>pip install rag-toolkit[docx]\n</code></pre> <p>Extract content from Microsoft Word documents.</p> </li> <li> <p> OCR Support</p> Bash<pre><code>pip install rag-toolkit[ocr]\n</code></pre> <p>Extract text from scanned documents and images.</p> </li> </ul>"},{"location":"getting_started/installation/#language-detection","title":"Language Detection","text":"Bash<pre><code>pip install rag-toolkit[lang]\n</code></pre> <p>Automatic language detection for multilingual document processing.</p>"},{"location":"getting_started/installation/#combined-installations","title":"Combined Installations","text":"<p>Common Combinations</p> All LLM ProvidersAll Document ParsersEverything Bash<pre><code>pip install rag-toolkit[ollama,openai]\n</code></pre> <p>Install support for both local and cloud LLM providers.</p> Bash<pre><code>pip install rag-toolkit[pdf,docx,ocr]\n</code></pre> <p>Handle any document format: PDFs, Word docs, and scanned images.</p> Bash<pre><code>pip install rag-toolkit[all]\n</code></pre> <p>Get the full RAG Toolkit experience with all optional features.</p>"},{"location":"getting_started/installation/#development-installation","title":"Development Installation","text":"<p>For Contributors</p> <p>This installation method is for developers contributing to RAG Toolkit or running from source.</p> Clone and Install<pre><code># Clone the repository\ngit clone https://github.com/gmottola00/rag-toolkit.git\ncd rag-toolkit\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev,docs]\"\n</code></pre> <p>Development tools included:</p> <ul> <li> Testing: pytest, pytest-cov, pytest-asyncio</li> <li> Type Checking: mypy with strict mode</li> <li> Formatting: black, isort for consistent style</li> <li> Linting: ruff for fast Python linting</li> <li> Documentation: MkDocs Material ecosystem</li> <li> All Features: All optional dependencies for comprehensive testing</li> </ul>"},{"location":"getting_started/installation/#verification","title":"Verification","text":"<p>Test Your Installation</p> <p>Run this script to verify everything is working correctly:</p> verify_installation.py<pre><code># Test basic imports\nfrom rag_toolkit import RagPipeline\nfrom rag_toolkit.core import VectorStoreClient, EmbeddingClient, LLMClient\n\nprint(\"\u2705 Core installation verified!\")\n\n# Test optional imports (if installed)\ntry:\n    from rag_toolkit import get_ollama_embedding\n    print(\"\u2705 Ollama support available\")\nexcept ImportError:\n    print(\"\u2139\ufe0f  Ollama not installed (optional)\")\n\ntry:\n    from rag_toolkit import get_openai_embedding\n    print(\"\u2705 OpenAI support available\")\nexcept ImportError:\n    print(\"\u2139\ufe0f  OpenAI not installed (optional)\")\n</code></pre> <p>Expected Output</p> <p>You should see at minimum: </p>Text Only<pre><code>\u2705 Core installation verified!\n</code></pre><p></p> <p>Additional messages indicate which optional features are available.</p>"},{"location":"getting_started/installation/#external-services","title":"External Services","text":""},{"location":"getting_started/installation/#milvus-vector-store","title":"Milvus (Vector Store)","text":"<p>Default Vector Database</p> <p>RAG Toolkit uses Milvus as the default vector store for high-performance similarity search.</p> Docker Compose (Recommended)MakefileMilvus Lite (Embedded) Bash<pre><code>docker-compose up -d milvus\n</code></pre> <p>Starts Milvus with optimal production settings.</p> Bash<pre><code>make docker-milvus\n</code></pre> <p>Convenient shortcut for Docker Compose.</p> Bash<pre><code>pip install milvus\n</code></pre> <p>Lightweight option for development and testing.</p>"},{"location":"getting_started/installation/#ollama-optional","title":"Ollama (Optional)","text":"<p>Local LLM Server</p> <p>Run large language models locally for privacy and cost savings.</p> macOS/LinuxDockerPull Models Bash<pre><code>curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre> Bash<pre><code>docker run -d -p 11434:11434 ollama/ollama\n</code></pre> Bash<pre><code># Language model\nollama pull llama2\n\n# Embedding model\nollama pull nomic-embed-text\n</code></pre>"},{"location":"getting_started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p>"},{"location":"getting_started/installation/#import-errors","title":"Import Errors","text":"<p>Problem</p> Python<pre><code>ModuleNotFoundError: No module named 'ollama'\n</code></pre> <p>Solution</p> <p>Install the required optional dependency: </p>Bash<pre><code>pip install rag-toolkit[ollama]\n</code></pre><p></p>"},{"location":"getting_started/installation/#material-type-check-type-checking-issues","title":":material-type-check: Type Checking Issues","text":"<p>Problem</p> <p>Mypy reports missing type stubs for third-party libraries.</p> <p>Solution</p> Bash<pre><code>pip install types-requests types-urllib3\n</code></pre>"},{"location":"getting_started/installation/#docker-issues","title":"Docker Issues","text":"<p>Problem</p> <p>Milvus container won't start or connection refused.</p> <p>Solution</p> Bash<pre><code># Check if ports are available\nlsof -i :19530\n\n# Clean up and restart\ndocker-compose down -v\ndocker-compose up -d milvus\n\n# Check container logs\ndocker logs milvus-standalone\n</code></pre>"},{"location":"getting_started/installation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start</p> <p>Build your first RAG application in 5 minutes</p> <p> Get Started</p> </li> <li> <p> Core Concepts</p> <p>Learn the fundamental principles of RAG Toolkit</p> <p> Learn More</p> </li> <li> <p> Examples</p> <p>Explore real-world RAG applications</p> <p> View Examples</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> <p> Read Docs</p> </li> </ul>"},{"location":"getting_started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":"macOS Linux Windows <p>Apple Silicon (M1/M2/M3)</p> <p>Some dependencies may need Rosetta 2: </p>Bash<pre><code>softwareupdate --install-rosetta\n</code></pre><p></p> <p>Homebrew Users</p> <p>Consider installing Python via Homebrew for better compatibility: </p>Bash<pre><code>brew install python@3.12\n</code></pre><p></p> <p>Development Headers Required</p> <p>Ensure you have Python development headers:</p> Ubuntu/Debian<pre><code>sudo apt-get update\nsudo apt-get install python3.11-dev build-essential\n</code></pre> CentOS/RHEL<pre><code>sudo yum install python311-devel gcc gcc-c++\n</code></pre> Arch Linux<pre><code>sudo pacman -S python python-pip base-devel\n</code></pre> <p>WSL2 Recommended</p> <p>For best compatibility, use Windows Subsystem for Linux: </p>Bash<pre><code>wsl --install\n</code></pre><p></p> <p>Native Windows</p> <p>If using native Windows:</p> <ol> <li>Install Python from python.org</li> <li>Install Visual C++ Build Tools</li> <li>Use PowerShell or Windows Terminal</li> </ol>"},{"location":"getting_started/installation/#update-instructions","title":"Update Instructions","text":"<p>Stay Up to Date</p> <p>Keep your RAG Toolkit installation current for the latest features and security patches.</p> Core LibraryWith All FeaturesCheck Version Bash<pre><code>pip install --upgrade rag-toolkit\n</code></pre> Bash<pre><code>pip install --upgrade rag-toolkit[all]\n</code></pre> Bash<pre><code>python -c \"import rag_toolkit; print(rag_toolkit.__version__)\"\n</code></pre> <p>Release Notes</p> <p>Check the Changelog for what's new in each version.</p>"},{"location":"getting_started/quickstart/","title":"Quick Start","text":""},{"location":"getting_started/quickstart/#quick-start","title":"Quick Start","text":"<p>Build your first RAG application with RAG Toolkit in less than 5 minutes!</p>"},{"location":"getting_started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before You Begin</p> <p>Ensure you have the following installed:</p> <ul> <li> <p> Python 3.11+</p> <p>Download from python.org</p> </li> <li> <p> pip</p> <p>Python package manager (included with Python)</p> </li> <li> <p> Docker</p> <p>For running Milvus vector store</p> <p>Get Docker</p> </li> </ul>"},{"location":"getting_started/quickstart/#step-1-installation","title":"Step 1: Installation","text":"<p>Install with Ollama Support</p> Bash<pre><code>pip install rag-toolkit[ollama]\n</code></pre> <p>Alternative: OpenAI</p> <p>If you prefer using OpenAI models: </p>Bash<pre><code>pip install rag-toolkit[openai]\n</code></pre><p></p>"},{"location":"getting_started/quickstart/#step-2-start-services","title":"Step 2: Start Services","text":""},{"location":"getting_started/quickstart/#start-milvus-vector-store","title":"Start Milvus (Vector Store)","text":"<p>Vector Database Setup</p> <p>Milvus provides high-performance vector similarity search.</p> Docker ComposeMakefile Bash<pre><code>docker-compose up -d milvus\n</code></pre> Bash<pre><code>make docker-milvus\n</code></pre> <p>Verify Milvus is Running</p> Bash<pre><code>docker ps | grep milvus\n</code></pre> <p>You should see the Milvus container running on port <code>19530</code>.</p>"},{"location":"getting_started/quickstart/#start-ollama-optional","title":"Start Ollama (Optional)","text":"<p>Local LLM Server</p> <p>Ollama allows you to run large language models locally.</p> Install OllamaPull Models Bash<pre><code>curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre> Bash<pre><code># Language model for text generation\nollama pull llama2\n\n# Embedding model for vector representation\nollama pull nomic-embed-text\n</code></pre> <p>Verify Ollama</p> Bash<pre><code>ollama list\n</code></pre> <p>You should see <code>llama2</code> and <code>nomic-embed-text</code> in the list.</p>"},{"location":"getting_started/quickstart/#step-3-your-first-rag-pipeline","title":"Step 3: Your First RAG Pipeline","text":"<p>Create Your First Application</p> <p>Create a file <code>my_first_rag.py</code>:</p> my_first_rag.py<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding.ollama import OllamaEmbedding\nfrom rag_toolkit.infra.llm.ollama import OllamaLLMClient\nfrom rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\n\n# Initialize components\nembedding = OllamaEmbedding(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\"\n)\n\nllm = OllamaLLMClient(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\"\n)\n\nvector_store = MilvusVectorStore(\n    host=\"localhost\",\n    port=\"19530\",\n    collection_name=\"quickstart_docs\"\n)\n\n# Create RAG pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n)\n\n# Index some documents\ndocuments = [\n    \"RAG stands for Retrieval-Augmented Generation. It's a technique that combines information retrieval with text generation.\",\n    \"Vector stores enable semantic search by storing embeddings of text and finding similar content.\",\n    \"rag-toolkit is a Python library that makes it easy to build production-ready RAG applications.\",\n]\n\nprint(\"\ud83d\udcda Indexing documents...\")\npipeline.index_documents(documents)\n\n# Query the system\nprint(\"\\n\ud83d\udd0d Querying the system...\\n\")\nresponse = pipeline.query(\"What is RAG and how does it work?\")\n\nprint(f\"Answer: {response.answer}\")\nprint(f\"\\nSources used: {len(response.sources)}\")\nfor i, source in enumerate(response.sources, 1):\n    print(f\"  {i}. {source[:100]}...\")\n</code></pre> <p>Run Your Application</p> Bash<pre><code>python my_first_rag.py\n</code></pre> <p>Expected Output</p> Text Only<pre><code>\ud83d\udcda Indexing documents...\n\ud83d\udd0d Querying the system...\n\nAnswer: RAG stands for Retrieval-Augmented Generation, which is a \ntechnique that combines information retrieval with text generation...\n\nSources used: 2\n  1. RAG stands for Retrieval-Augmented Generation. It's a technique...\n  2. rag-toolkit is a Python library that makes it easy to build...\n</code></pre>"},{"location":"getting_started/quickstart/#step-4-understanding-the-code","title":"Step 4: Understanding the Code","text":"<p>Code Breakdown</p> <p>Let's understand what each part does:</p>"},{"location":"getting_started/quickstart/#1-component-initialization","title":"1. Component Initialization","text":"Python<pre><code># Initialize embedding model\nembedding = OllamaEmbedding(model=\"nomic-embed-text\")\nllm = OllamaLLMClient(model=\"llama2\")\nvector_store = MilvusVectorStore(collection_name=\"quickstart_docs\")\n</code></pre> <p>We create three core components:</p> Component Purpose Provider Embedding Converts text to vectors Ollama (nomic-embed-text) LLM Generates natural language responses Ollama (llama2) Vector Store Stores and retrieves embeddings Milvus"},{"location":"getting_started/quickstart/#2-pipeline-creation","title":"2. Pipeline Creation","text":"Python<pre><code>pipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=vector_store,\n)\n</code></pre> <p>The <code>RagPipeline</code> orchestrates all components through dependency injection.</p> <p>Protocol-Based Design</p> <p>Any class implementing the protocol can be used\u2014no inheritance required!</p>"},{"location":"getting_started/quickstart/#3-document-indexing","title":"3. Document Indexing","text":"Python<pre><code>pipeline.index_documents(documents)\n</code></pre> <pre><code>graph LR\n    A[Documents] --&gt; B[Split into Chunks]\n    B --&gt; C[Convert to Embeddings]\n    C --&gt; D[Store in Vector DB]\n    style A fill:#e3f2fd\n    style D fill:#c8e6c9</code></pre>"},{"location":"getting_started/quickstart/#4-querying","title":"4. Querying","text":"Python<pre><code>response = pipeline.query(\"What is RAG?\")\n</code></pre> <pre><code>graph LR\n    A[User Query] --&gt; B[Embed Query]\n    B --&gt; C[Search Similar Docs]\n    C --&gt; D[Retrieve Context]\n    D --&gt; E[Generate Response]\n    style A fill:#e3f2fd\n    style E fill:#c8e6c9</code></pre>"},{"location":"getting_started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting_started/quickstart/#customize-your-pipeline","title":"Customize Your Pipeline","text":"<p>Advanced Configuration</p> Python<pre><code># Use different models\nembedding = OllamaEmbedding(model=\"mxbai-embed-large\")\nllm = OllamaLLMClient(model=\"mistral\")\n\n# Configure retrieval parameters\nresponse = pipeline.query(\n    \"What is RAG?\",\n    top_k=5,        # Retrieve top 5 documents\n    rerank=True,    # Use reranking for better results\n)\n\n# Access metadata and scores\nfor source in response.sources:\n    print(f\"Relevance Score: {source.score:.4f}\")\n    print(f\"Content: {source.text}\")\n</code></pre>"},{"location":"getting_started/quickstart/#add-document-parsing","title":"Add Document Parsing","text":"<p>Parse PDF Files</p> Python<pre><code>from rag_toolkit.infra.parsers.pdf import PDFParser\n\n# Parse PDF files\nparser = PDFParser()\ndocuments = parser.parse(\"path/to/document.pdf\")\n\n# Index parsed content\npipeline.index_documents(documents)\n</code></pre>"},{"location":"getting_started/quickstart/#material-openai-use-openai-instead","title":":material-openai: Use OpenAI Instead","text":"<p>Cloud-Based Models</p> Python<pre><code>from rag_toolkit.infra.embedding.openai_embedding import OpenAIEmbedding\nfrom rag_toolkit.infra.llm.openai_llm import OpenAILLMClient\n\nembedding = OpenAIEmbedding(\n    api_key=\"your-api-key\",\n    model=\"text-embedding-3-small\"\n)\n\nllm = OpenAILLMClient(\n    api_key=\"your-api-key\",\n    model=\"gpt-4\"\n)\n</code></pre>"},{"location":"getting_started/quickstart/#implement-custom-components","title":"Implement Custom Components","text":"<p>Protocol-Based Flexibility</p> Python<pre><code># Your own vector store - no inheritance needed!\nclass MyVectorStore:\n    def create_collection(self, name, dimension, **kwargs): ...\n    def insert(self, collection, vectors, texts, metadata): ...\n    def search(self, collection, query_vector, top_k): ...\n\n# It just works! \u2728\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=MyVectorStore(),  # Protocol-based design\n)\n</code></pre>"},{"location":"getting_started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting_started/quickstart/#pattern-1-multi-document-rag","title":"Pattern 1: Multi-Document RAG","text":"<p>Multiple Data Sources</p> Python<pre><code># Index PDF documents\npipeline.index_documents(\n    documents=pdf_docs,\n    metadata=[{\"source\": \"manual.pdf\", \"type\": \"documentation\"}]\n)\n\n# Index website content\npipeline.index_documents(\n    documents=web_docs,\n    metadata=[{\"source\": \"website\", \"type\": \"web\"}]\n)\n\n# Query with filtering\nresponse = pipeline.query(\n    \"How to install?\",\n    filters={\"source\": \"manual.pdf\"}  # Only search in PDF\n)\n</code></pre>"},{"location":"getting_started/quickstart/#pattern-2-conversation-memory","title":"Pattern 2: Conversation Memory","text":"<p>Contextual Conversations</p> Python<pre><code>conversation = []\n\nwhile True:\n    query = input(\"You: \")\n    if query.lower() in ['exit', 'quit']:\n        break\n\n    # Include conversation history\n    context = \"\\n\".join(conversation[-5:])  # Last 5 turns\n    full_query = f\"{context}\\nUser: {query}\"\n\n    response = pipeline.query(full_query)\n    print(f\"Assistant: {response.answer}\")\n\n    # Update history\n    conversation.append(f\"User: {query}\")\n    conversation.append(f\"Assistant: {response.answer}\")\n</code></pre>"},{"location":"getting_started/quickstart/#pattern-3-batch-processing","title":"Pattern 3: Batch Processing","text":"<p>Process Multiple Queries</p> Python<pre><code>queries = [\n    \"What is RAG?\",\n    \"How do vector stores work?\",\n    \"What are embeddings?\",\n]\n\n# Process all queries\nresponses = [pipeline.query(q) for q in queries]\n\n# Display results\nfor q, r in zip(queries, responses):\n    print(f\"Q: {q}\")\n    print(f\"A: {r.answer}\\n\")\n</code></pre>"},{"location":"getting_started/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues &amp; Solutions</p>"},{"location":"getting_started/quickstart/#connection-refused-error","title":"\"Connection refused\" Error","text":"<p>Problem</p> Text Only<pre><code>ConnectionRefusedError: [Errno 61] Connection refused\n</code></pre> <p>Solution</p> <p>Ensure Milvus is running: </p>Bash<pre><code>docker ps | grep milvus\n</code></pre><p></p> <p>If not running, start it: </p>Bash<pre><code>docker-compose up -d milvus\n</code></pre><p></p>"},{"location":"getting_started/quickstart/#model-not-found-error","title":"\"Model not found\" Error","text":"<p>Problem</p> Text Only<pre><code>Error: model 'llama2' not found\n</code></pre> <p>Solution</p> <p>Pull the Ollama model: </p>Bash<pre><code>ollama pull llama2\nollama pull nomic-embed-text\n</code></pre><p></p> <p>Verify installation: </p>Bash<pre><code>ollama list\n</code></pre><p></p>"},{"location":"getting_started/quickstart/#slow-performance","title":"Slow Performance","text":"<p>Problem</p> <p>Queries are taking too long to process.</p> <p>Solutions</p> Use Smaller ModelsReduce top_kEnable Batch Processing Python<pre><code># Smaller embedding model\nembedding = OllamaEmbedding(model=\"all-minilm\")\n\n# Smaller LLM\nllm = OllamaLLMClient(model=\"tinyllama\")\n</code></pre> Python<pre><code># Retrieve fewer documents\nresponse = pipeline.query(\"query\", top_k=3)\n</code></pre> Python<pre><code># Process documents in batches\nvectors = embedding.embed_batch(texts)\n</code></pre>"},{"location":"getting_started/quickstart/#learn-more","title":"Learn More","text":"<ul> <li> <p> Core Concepts</p> <p>Understand the architecture and design principles</p> <p> Learn</p> </li> <li> <p> Protocols Guide</p> <p>Deep dive into protocol-based design</p> <p> Read</p> </li> <li> <p> Examples</p> <p>Explore real-world RAG applications</p> <p> Browse</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> <p> Explore</p> </li> </ul>"},{"location":"getting_started/quickstart/#getting-help","title":"Getting Help","text":"<p>Need Assistance?</p> <ul> <li> <p> Documentation   Comprehensive guides and tutorials</p> </li> <li> <p> GitHub Discussions   Ask questions and share ideas</p> </li> <li> <p> Issue Tracker   Report bugs and request features</p> </li> <li> <p> Email Support   Direct contact for enterprise inquiries</p> </li> </ul> <p>Quick Tips</p> <ul> <li>Start with the installation guide if you haven't already</li> <li>Check out architecture overview to understand the design</li> <li>Browse examples for inspiration</li> <li>Join our community discussions</li> </ul>"},{"location":"guides/","title":"User Guide","text":""},{"location":"guides/#user-guide","title":"User Guide","text":"<p>Welcome to the rag-toolkit user guide! This section covers everything you need to know to build production-ready RAG applications.</p>"},{"location":"guides/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Core Concepts</p> <p>Fundamental concepts and terminology used throughout rag-toolkit.</p> </li> <li> <p> Protocols</p> <p>Understanding the Protocol-based architecture and how to implement custom providers.</p> </li> <li> <p> Vector Stores</p> <p>Working with different vector databases and implementing custom stores.</p> </li> <li> <p> Embeddings</p> <p>Using embedding models and creating custom embedding providers.</p> </li> <li> <p> LLMs</p> <p>Integrating language models and implementing custom LLM clients.</p> </li> <li> <p>:material-pipeline: RAG Pipeline</p> <p>Building and customizing complete RAG pipelines.</p> </li> <li> <p> Chunking</p> <p>Document chunking strategies and custom chunkers.</p> </li> <li> <p> Reranking</p> <p>Improving retrieval quality with reranking techniques.</p> </li> </ul>"},{"location":"guides/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Check the API Reference</li> <li>\ud83d\udca1 See Examples</li> <li>\ud83d\udcac Ask in GitHub Discussions</li> <li>\ud83d\udc1b Report issues on GitHub</li> </ul>"},{"location":"guides/chunking/","title":"Chunking","text":""},{"location":"guides/chunking/#chunking-strategies","title":"Chunking Strategies","text":"<p>Chunking is critical for RAG quality. Learn how to split documents effectively for optimal embedding and retrieval.</p>"},{"location":"guides/chunking/#why-chunking-matters","title":"Why Chunking Matters","text":"<p>The Problem with Large Documents</p> <p>Issues:</p> <ul> <li> Embeddings lose semantic focus</li> <li> Context windows overflow</li> <li>:material-target-off: Poor retrieval precision</li> <li> Slow processing</li> </ul> <p>Benefits of Proper Chunking</p> <ul> <li> Focused semantic embeddings</li> <li> Precise retrieval</li> <li> Better LLM context utilization</li> <li> Efficient processing</li> </ul> <pre><code>graph LR\n    A[\ud83d\udcc4 Large Document] --&gt; B[\u2702\ufe0f Chunking]\n    B --&gt; C[\ud83d\udcdd Chunk 1]\n    B --&gt; D[\ud83d\udcdd Chunk 2]\n    B --&gt; E[\ud83d\udcdd Chunk 3]\n\n    C --&gt; F[\ud83d\udd22 Focused Embedding]\n    D --&gt; G[\ud83d\udd22 Focused Embedding]\n    E --&gt; H[\ud83d\udd22 Focused Embedding]\n\n    style A fill:#ffebee\n    style F fill:#c8e6c9\n    style G fill:#c8e6c9\n    style H fill:#c8e6c9</code></pre>"},{"location":"guides/chunking/#available-chunking-strategies","title":"Available Chunking Strategies","text":"<p>Two Complementary Approaches</p> <p>RAG Toolkit provides two chunking strategies that work together for optimal results.</p>"},{"location":"guides/chunking/#1-dynamicchunker-structural-chunking","title":"1.  DynamicChunker - Structural Chunking","text":"<p>Document Structure-Based</p> <p>Creates chunks based on document structure using heading hierarchy.</p> <p>Best for: Structured documents with clear sections (PDFs, documentation, reports)</p> dynamic_chunking.py<pre><code>from rag_toolkit.core.chunking import DynamicChunker\n\n# Create dynamic chunker\nchunker = DynamicChunker(\n    include_tables=True,        # Include table blocks\n    max_heading_level=6,        # Maximum heading depth\n    allow_preamble=False        # Handle content before first heading\n)\n\n# Build chunks from parsed pages\n# Input: pages from document parser with blocks\nchunks = chunker.build_chunks(pages)\n\nfor chunk in chunks:\n    print(f\"Title: {chunk.title}\")\n    print(f\"Level: {chunk.heading_level}\")\n    print(f\"Pages: {chunk.page_numbers}\")\n    print(f\"Text length: {len(chunk.text)}\")\n</code></pre> <p>How it works:</p> <pre><code>graph TB\n    A[\ud83d\udcc4 Document] --&gt; B{Level-1 Headings}\n    B --&gt; C[\ud83d\udcd1 Section 1]\n    B --&gt; D[\ud83d\udcd1 Section 2]\n    B --&gt; E[\ud83d\udcd1 Section 3]\n\n    C --&gt; F[h2, h3, paragraphs, tables]\n    D --&gt; G[h2, h3, paragraphs, tables]\n    E --&gt; H[h2, h3, paragraphs, tables]\n\n    style A fill:#e3f2fd\n    style C fill:#c8e6c9\n    style D fill:#c8e6c9\n    style E fill:#c8e6c9</code></pre> <ol> <li>Splits document at level-1 headings (h1)</li> <li>Each chunk includes:<ul> <li>Level-1 heading as title</li> <li>All sub-headings (h2-h6) under that section</li> <li>Paragraphs, lists, and optionally tables</li> <li>Continues until the next level-1 heading</li> </ul> </li> </ol> <p>Configuration options:</p> Include TablesLimit Heading DepthHandle Preamble Python<pre><code>chunker = DynamicChunker(include_tables=True)\n</code></pre> <p>Tables are included as structured text within chunks.</p> Python<pre><code>chunker = DynamicChunker(max_heading_level=3)  # Only h1-h3\n</code></pre> <p>Ignore deeper heading levels for simpler structure.</p> Python<pre><code>chunker = DynamicChunker(allow_preamble=True)\n</code></pre> <p>Creates a \"Preamble\" chunk for content before the first heading.</p>"},{"location":"guides/chunking/#2-tokenchunker-token-based-chunking","title":"2.  TokenChunker - Token-Based Chunking","text":"<p>Token Budget Control</p> <p>Splits larger chunks into smaller token-based pieces with intelligent overlap.</p> <p>Best for: Controlling token budget, optimal embedding sizes, LLM context limits</p> token_chunking.py<pre><code>from rag_toolkit.core.chunking import TokenChunker\n\n# Create token chunker\nchunker = TokenChunker(\n    max_tokens=800,         # Maximum tokens per chunk\n    min_tokens=400,         # Minimum tokens per chunk\n    overlap_tokens=120,     # Overlap between chunks\n)\n\n# Chunk structured chunks into token chunks\n# Input: Chunk objects from DynamicChunker\ntoken_chunks = chunker.chunk(structured_chunks)\n\nfor chunk in token_chunks:\n    print(f\"ID: {chunk.id}\")\n    print(f\"Section: {chunk.section_path}\")\n    print(f\"Text: {chunk.text[:100]}...\")\n    print(f\"Metadata: {chunk.metadata}\")\n</code></pre> <p>How it works:</p> <pre><code>graph LR\n    A[\ud83d\udcdd Large Chunk&lt;br/&gt;2400 tokens] --&gt; B{TokenChunker}\n    B --&gt; C[\ud83d\udcc4 Chunk 1&lt;br/&gt;tokens 0-800]\n    B --&gt; D[\ud83d\udcc4 Chunk 2&lt;br/&gt;tokens 680-1480]\n    B --&gt; E[\ud83d\udcc4 Chunk 3&lt;br/&gt;tokens 1360-2160]\n\n    C -.120 overlap.-&gt; D\n    D -.120 overlap.-&gt; E\n\n    style A fill:#ffebee\n    style C fill:#c8e6c9\n    style D fill:#c8e6c9\n    style E fill:#c8e6c9</code></pre> <ol> <li>Takes chunks from DynamicChunker</li> <li>Splits them by token count (whitespace tokenizer)</li> <li>Applies overlap to preserve context at boundaries</li> <li>Maintains section hierarchy and metadata</li> </ol> <p>Token overlap visualization:</p> Text Only<pre><code>Chunk 1: tokens[0:800]\n         \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\nChunk 2:           \u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591  # 120 overlap\nChunk 3:                      \u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  # 120 overlap\n</code></pre> <p>Configuration:</p> Small Chunks (Precise)Large Chunks (Context)Custom Tokenizer Python<pre><code>chunker = TokenChunker(\n    max_tokens=512,\n    min_tokens=200,\n    overlap_tokens=50\n)\n</code></pre> <p>Best for precise retrieval, shorter contexts.</p> Python<pre><code>chunker = TokenChunker(\n    max_tokens=1500,\n    min_tokens=800,\n    overlap_tokens=200\n)\n</code></pre> <p>Best for more context, longer documents.</p> Python<pre><code>def my_tokenizer(text: str) -&gt; list[str]:\n    # Your tokenization logic\n    return text.split()\n\nchunker = TokenChunker(\n    max_tokens=800,\n    tokenizer=my_tokenizer\n)\n</code></pre> <p>Use your own tokenization strategy.</p>"},{"location":"guides/chunking/#two-stage-chunking-pipeline","title":"Two-Stage Chunking Pipeline","text":"<p>The recommended approach is to use both chunkers together:</p> Python<pre><code>from rag_toolkit.core.chunking import DynamicChunker, TokenChunker\n\n# Stage 1: Structural chunking\ndynamic_chunker = DynamicChunker(\n    include_tables=True,\n    max_heading_level=6,\n    allow_preamble=False\n)\n\n# Stage 2: Token-based chunking\ntoken_chunker = TokenChunker(\n    max_tokens=800,\n    min_tokens=400,\n    overlap_tokens=120\n)\n\n# Process document\n# 1. Parse document (PDF, DOCX, etc.)\npages = parser.parse(\"document.pdf\")\n\n# 2. Create structural chunks\nstructural_chunks = dynamic_chunker.build_chunks(pages)\n\n# 3. Split into token chunks\nfinal_chunks = token_chunker.chunk(structural_chunks)\n\n# Now ready for embedding and indexing\nfor chunk in final_chunks:\n    embedding = await embed(chunk.text)\n    await vector_store.insert(embedding, chunk.text, chunk.metadata)\n</code></pre>"},{"location":"guides/chunking/#chunk-types","title":"Chunk Types","text":""},{"location":"guides/chunking/#chunk-from-dynamicchunker","title":"Chunk (from DynamicChunker)","text":"<p>Basic structural chunk with heading information:</p> Python<pre><code>@dataclass\nclass Chunk:\n    id: str                           # Unique identifier\n    title: str                        # Section heading\n    heading_level: int                # Heading depth (1-6)\n    text: str                         # Full text content\n    blocks: List[Dict[str, Any]]     # Structured blocks\n    page_numbers: List[int]          # Source pages\n</code></pre>"},{"location":"guides/chunking/#tokenchunk-from-tokenchunker","title":"TokenChunk (from TokenChunker)","text":"<p>Token-based chunk with metadata:</p> Python<pre><code>@dataclass\nclass TokenChunk:\n    id: str                           # Unique ID (parent:start-end)\n    text: str                         # Chunk text\n    section_path: str                 # Full section hierarchy\n    metadata: Dict[str, str]         # Extracted metadata\n    page_numbers: List[int]          # Source pages\n    source_chunk_id: str             # Parent chunk ID\n</code></pre>"},{"location":"guides/chunking/#metadata-extraction","title":"Metadata Extraction","text":"<p>TokenChunker automatically extracts metadata from text:</p> Python<pre><code># Metadata patterns (configurable in source)\nTENDER_CODE_PATTERN = r\"\\b\\d{6}-\\d{4}\\b\"\nLOT_ID_PATTERN = r\"\\bLOT[-_\\s]?\\w+\\b\"\n\n# Document type keywords\nDOC_TYPE_KEYWORDS = {\n    \"bando\": \"tender_notice\",\n    \"avviso\": \"notice\",\n    \"rettifica\": \"corrigendum\",\n    \"capitolato\": \"specs\",\n    \"disciplinare\": \"disciplinare\",\n}\n\n# Example extracted metadata\n{\n    \"tender_code\": \"123456-2024\",\n    \"lot_id\": \"lot_1\",\n    \"document_type\": \"tender_notice\",\n    \"clause_type\": \"article\"\n}\n</code></pre>"},{"location":"guides/chunking/#section-path-hierarchy","title":"Section Path Hierarchy","text":"<p>TokenChunker builds a hierarchical section path:</p> Python<pre><code># Example document structure:\n# H1: Chapter 1\n#   H2: Section 1.1\n#     H3: Subsection 1.1.1\n\n# Resulting section_path:\n\"Chapter 1 &gt; Section 1.1 &gt; Subsection 1.1.1\"\n</code></pre> <p>This helps maintain context when chunks are retrieved.</p>"},{"location":"guides/chunking/#chunk-size-guidelines","title":"Chunk Size Guidelines","text":"Document Type max_tokens min_tokens overlap Reasoning Technical docs 800 400 120 Balanced context Short Q&amp;A 512 200 50 Precise answers Long reports 1500 800 200 More context Code files 600 300 60 Function-level"},{"location":"guides/chunking/#best-practices","title":"Best Practices","text":""},{"location":"guides/chunking/#1-use-two-stage-pipeline","title":"1. Use Two-Stage Pipeline","text":"<p>Always use DynamicChunker \u2192 TokenChunker for best results:</p> Python<pre><code># \u2705 Good: Two-stage chunking\nstructural = dynamic_chunker.build_chunks(pages)\nfinal = token_chunker.chunk(structural)\n\n# \u274c Bad: Only one chunker\nfinal = token_chunker.chunk(raw_text)  # Loses structure\n</code></pre>"},{"location":"guides/chunking/#2-configure-overlap","title":"2. Configure Overlap","text":"<p>Always use overlap (15-20% of max_tokens):</p> Python<pre><code># \u2705 Good: 15% overlap\nTokenChunker(max_tokens=800, overlap_tokens=120)\n\n# \u274c Bad: No overlap\nTokenChunker(max_tokens=800, overlap_tokens=0)\n</code></pre>"},{"location":"guides/chunking/#3-preserve-tables","title":"3. Preserve Tables","text":"<p>Include tables for complete information:</p> Python<pre><code># \u2705 Good: Include tables\nDynamicChunker(include_tables=True)\n\n# \u26a0\ufe0f  Careful: May lose important data\nDynamicChunker(include_tables=False)\n</code></pre>"},{"location":"guides/chunking/#4-use-metadata-for-filtering","title":"4. Use Metadata for Filtering","text":"<p>Leverage extracted metadata:</p> Python<pre><code># Search with metadata filters\nresults = await vector_store.search(\n    query_embedding,\n    filter={\n        \"document_type\": \"tender_notice\",\n        \"lot_id\": \"lot_1\"\n    }\n)\n</code></pre>"},{"location":"guides/chunking/#integration-with-rag-pipeline","title":"Integration with RAG Pipeline","text":"<p>Complete example with rag-toolkit:</p> Python<pre><code>from rag_toolkit.core.chunking import DynamicChunker, TokenChunker\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\nfrom rag_toolkit.core.vectorstore import MilvusVectorStore\n\n# Initialize components\ndynamic_chunker = DynamicChunker()\ntoken_chunker = TokenChunker(max_tokens=800)\nembedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\nvector_store = MilvusVectorStore(collection_name=\"documents\")\n\n# Process and index document\nasync def index_document(document_path: str):\n    # 1. Parse\n    pages = parser.parse(document_path)\n\n    # 2. Structural chunking\n    structural_chunks = dynamic_chunker.build_chunks(pages)\n\n    # 3. Token chunking\n    token_chunks = token_chunker.chunk(structural_chunks)\n\n    # 4. Embed and index\n    for chunk in token_chunks:\n        embedding_vector = await embedding.embed(chunk.text)\n\n        await vector_store.insert(\n            id=chunk.id,\n            vector=embedding_vector,\n            text=chunk.text,\n            metadata={\n                **chunk.metadata,\n                \"section_path\": chunk.section_path,\n                \"pages\": chunk.page_numbers\n            }\n        )\n\n    print(f\"Indexed {len(token_chunks)} chunks\")\n\n# Index\nawait index_document(\"report.pdf\")\n</code></pre>"},{"location":"guides/chunking/#customization","title":"Customization","text":""},{"location":"guides/chunking/#custom-tokenizer","title":"Custom Tokenizer","text":"Python<pre><code>import tiktoken\n\n# Use OpenAI tokenizer\ndef openai_tokenizer(text: str) -&gt; list[str]:\n    enc = tiktoken.get_encoding(\"cl100k_base\")\n    tokens = enc.encode(text)\n    return [enc.decode([t]) for t in tokens]\n\nchunker = TokenChunker(\n    max_tokens=800,\n    tokenizer=openai_tokenizer\n)\n</code></pre>"},{"location":"guides/chunking/#domain-specific-metadata","title":"Domain-Specific Metadata","text":"<p>Extend metadata extraction by modifying patterns:</p> Python<pre><code># In chunking.py, add your patterns:\nCUSTOM_PATTERN = re.compile(r\"YOUR_REGEX\")\n\ndef _extract_metadata(text: str, title: str | None = None) -&gt; Dict[str, str]:\n    metadata = {}\n\n    # Add your extraction logic\n    if CUSTOM_PATTERN.search(text):\n        metadata[\"custom_field\"] = ...\n\n    return metadata\n</code></pre>"},{"location":"guides/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/chunking/#chunks-too-large","title":"Chunks Too Large","text":"Python<pre><code># Reduce max_tokens\nTokenChunker(max_tokens=512, min_tokens=200)\n</code></pre>"},{"location":"guides/chunking/#chunks-too-small","title":"Chunks Too Small","text":"Python<pre><code># Increase min_tokens\nTokenChunker(max_tokens=1200, min_tokens=600)\n</code></pre>"},{"location":"guides/chunking/#missing-context-at-boundaries","title":"Missing Context at Boundaries","text":"Python<pre><code># Increase overlap\nTokenChunker(overlap_tokens=200)  # 25% overlap\n</code></pre>"},{"location":"guides/chunking/#lost-document-structure","title":"Lost Document Structure","text":"Python<pre><code># Ensure DynamicChunker is used first\nstructural = dynamic_chunker.build_chunks(pages)\nfinal = token_chunker.chunk(structural)\n</code></pre>"},{"location":"guides/chunking/#next-steps","title":"Next Steps","text":"<ul> <li>Embeddings Guide - Embed your chunks</li> <li>Vector Stores - Store and retrieve chunks</li> <li>RAG Pipeline - Complete RAG setup</li> <li>Production Setup - Deploy to production</li> </ul>"},{"location":"guides/chunking/#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Chunking fundamentals</li> <li>Architecture - System design</li> <li>API Reference - Complete API docs</li> </ul> <p>Split text into fixed-size chunks with optional overlap.</p> <p>Best for: General documents, simple use cases</p> Python<pre><code>from rag_toolkit.core.chunking import FixedSizeChunker\n\n# Create chunker\nchunker = FixedSizeChunker(\n    chunk_size=500,  # Characters per chunk\n    chunk_overlap=50,  # Overlap between chunks\n)\n\n# Chunk document\ndocument = \"Long document text...\" * 100\nchunks = await chunker.chunk(document)\n\nprint(f\"Created {len(chunks)} chunks\")\nfor chunk in chunks[:3]:\n    print(f\"Chunk {chunk.id}: {len(chunk.text)} chars\")\n</code></pre> <p>Configuration:</p> Python<pre><code># Small chunks for precise retrieval\nchunker = FixedSizeChunker(chunk_size=300, chunk_overlap=30)\n\n# Large chunks for more context\nchunker = FixedSizeChunker(chunk_size=1000, chunk_overlap=100)\n\n# No overlap (faster but may miss context)\nchunker = FixedSizeChunker(chunk_size=500, chunk_overlap=0)\n</code></pre>"},{"location":"guides/chunking/#2-token-aware-chunking","title":"2. Token-Aware Chunking","text":"<p>Split by token count instead of characters, respecting LLM limits.</p> <p>Best for: Production systems, token budget control</p> Python<pre><code>from rag_toolkit.core.chunking import TokenChunker\n\n# Create token-aware chunker\nchunker = TokenChunker(\n    chunk_size=512,  # Tokens per chunk\n    chunk_overlap=50,  # Token overlap\n    model=\"gpt-4\",  # Model for tokenization\n)\n\n# Chunk with token precision\nchunks = await chunker.chunk(document)\n\nfor chunk in chunks:\n    print(f\"Tokens: {chunk.token_count}\")\n</code></pre> <p>Benefits: - Precise token control - Optimal context window usage - Model-specific tokenization</p>"},{"location":"guides/chunking/#3-semantic-chunking","title":"3. Semantic Chunking","text":"<p>Split at natural semantic boundaries (sentences, paragraphs).</p> <p>Best for: Maintaining context integrity</p> Python<pre><code>from rag_toolkit.core.chunking import SemanticChunker\n\n# Create semantic chunker\nchunker = SemanticChunker(\n    mode=\"sentence\",  # or \"paragraph\"\n    max_chunk_size=500,\n    similarity_threshold=0.7,  # Merge similar sentences\n)\n\n# Chunk at semantic boundaries\nchunks = await chunker.chunk(document)\n</code></pre> <p>Modes: - <code>sentence</code>: Split at sentence boundaries - <code>paragraph</code>: Split at paragraph boundaries - <code>section</code>: Split at section headers</p>"},{"location":"guides/chunking/#4-recursive-chunking","title":"4. Recursive Chunking","text":"<p>Hierarchical chunking with fallback strategies.</p> <p>Best for: Complex documents, robust processing</p> Python<pre><code>from rag_toolkit.core.chunking import RecursiveChunker\n\n# Create recursive chunker\nchunker = RecursiveChunker(\n    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \" \"],  # Try in order\n    chunk_size=500,\n    chunk_overlap=50,\n)\n\n# Chunks with intelligent splitting\nchunks = await chunker.chunk(document)\n</code></pre> <p>How it works: 1. Try splitting by first separator (<code>\\n\\n\\n</code>) 2. If chunks too large, try next separator (<code>\\n\\n</code>) 3. Continue until target size reached</p>"},{"location":"guides/chunking/#5-markdown-aware-chunking","title":"5. Markdown-Aware Chunking","text":"<p>Respect Markdown structure (headers, lists, code blocks).</p> <p>Best for: Documentation, technical content</p> Python<pre><code>from rag_toolkit.core.chunking import MarkdownChunker\n\n# Create Markdown chunker\nchunker = MarkdownChunker(\n    chunk_size=500,\n    preserve_code_blocks=True,  # Keep code blocks intact\n    header_hierarchy=True,  # Include parent headers\n)\n\n# Chunk Markdown\nmarkdown_doc = \"\"\"\n# Chapter 1\n## Section 1.1\nContent here...\n\n## Section 1.2\nMore content...\n\"\"\"\n\nchunks = await chunker.chunk(markdown_doc)\n\nfor chunk in chunks:\n    print(f\"Headers: {chunk.metadata['headers']}\")\n    print(f\"Content: {chunk.text[:50]}...\")\n</code></pre>"},{"location":"guides/chunking/#6-dynamic-chunking","title":"6. Dynamic Chunking","text":"<p>Adapt chunk size based on content characteristics.</p> <p>Best for: Mixed content types, optimal retrieval</p> Python<pre><code>from rag_toolkit.core.chunking import DynamicChunker\n\n# Create dynamic chunker\nchunker = DynamicChunker(\n    min_chunk_size=200,\n    max_chunk_size=800,\n    target_chunk_size=500,\n    adapt_to_content=True,  # Adjust based on content\n)\n\n# Automatically optimizes chunk sizes\nchunks = await chunker.chunk(document)\n</code></pre>"},{"location":"guides/chunking/#advanced-chunking","title":"Advanced Chunking","text":""},{"location":"guides/chunking/#metadata-enrichment","title":"Metadata Enrichment","text":"<p>Add metadata to chunks for better filtering:</p> Python<pre><code>from rag_toolkit.core.chunking import MetadataEnricher\n\n# Enrich chunks with metadata\nenricher = MetadataEnricher()\n\nchunks = await chunker.chunk(document)\nenriched_chunks = await enricher.enrich(\n    chunks,\n    metadata={\n        \"source\": \"research_paper.pdf\",\n        \"author\": \"John Doe\",\n        \"date\": \"2024-12-20\",\n        \"category\": \"AI\"\n    }\n)\n\n# Each chunk now has metadata\nfor chunk in enriched_chunks:\n    print(chunk.metadata)\n</code></pre>"},{"location":"guides/chunking/#hierarchical-chunking","title":"Hierarchical Chunking","text":"<p>Create parent-child relationships:</p> Python<pre><code>from rag_toolkit.core.chunking import HierarchicalChunker\n\n# Create hierarchical chunks\nchunker = HierarchicalChunker(\n    parent_chunk_size=1000,\n    child_chunk_size=200,\n    overlap=50,\n)\n\n# Returns parents and children\nparents, children = await chunker.chunk(document)\n\n# Children reference parents\nfor child in children:\n    print(f\"Child: {child.id}\")\n    print(f\"Parent: {child.parent_id}\")\n</code></pre>"},{"location":"guides/chunking/#context-preserving-chunking","title":"Context-Preserving Chunking","text":"<p>Include surrounding context in chunks:</p> Python<pre><code>from rag_toolkit.core.chunking import ContextChunker\n\n# Add context to chunks\nchunker = ContextChunker(\n    chunk_size=500,\n    context_before=100,  # Characters before\n    context_after=100,  # Characters after\n)\n\nchunks = await chunker.chunk(document)\n\nfor chunk in chunks:\n    print(f\"Main: {chunk.text}\")\n    print(f\"Context before: {chunk.context_before}\")\n    print(f\"Context after: {chunk.context_after}\")\n</code></pre>"},{"location":"guides/chunking/#chunk-size-selection","title":"Chunk Size Selection","text":""},{"location":"guides/chunking/#guidelines","title":"Guidelines","text":"Document Type Recommended Size Reasoning Short Q&amp;A 200-300 chars Precise answers General docs 500-800 chars Balanced Long-form content 1000-1500 chars More context Code 300-500 chars Function-level Academic papers 800-1200 chars Paragraph-level"},{"location":"guides/chunking/#finding-optimal-size","title":"Finding Optimal Size","text":"Python<pre><code>from rag_toolkit.core.chunking import ChunkSizeOptimizer\n\n# Optimize chunk size for your data\noptimizer = ChunkSizeOptimizer(\n    pipeline=rag_pipeline,\n    test_queries=[\"Q1\", \"Q2\", \"Q3\"],\n    test_documents=documents,\n)\n\n# Test different sizes\nresults = await optimizer.optimize(\n    chunk_sizes=[200, 500, 800, 1000],\n    metric=\"retrieval_precision\"\n)\n\nbest_size = results.best_chunk_size\nprint(f\"Optimal chunk size: {best_size}\")\n</code></pre>"},{"location":"guides/chunking/#overlap-configuration","title":"Overlap Configuration","text":""},{"location":"guides/chunking/#why-overlap","title":"Why Overlap?","text":"<p>Overlap prevents information loss at chunk boundaries:</p> Text Only<pre><code>Without overlap:\n[Chunk 1: \"...end of sentence.\"] [Chunk 2: \"Start of new...\"]\n\u274c Context break\n\nWith overlap:\n[Chunk 1: \"...end of sentence. Start of\"] [Chunk 2: \"sentence. Start of new...\"]\n\u2705 Context preserved\n</code></pre>"},{"location":"guides/chunking/#recommended-overlap","title":"Recommended Overlap","text":"Python<pre><code># General rule: 10-20% of chunk size\nchunk_size = 500\noverlap = int(chunk_size * 0.15)  # 75 characters\n\nchunker = FixedSizeChunker(\n    chunk_size=chunk_size,\n    chunk_overlap=overlap\n)\n</code></pre>"},{"location":"guides/chunking/#document-type-specific-chunking","title":"Document Type-Specific Chunking","text":""},{"location":"guides/chunking/#pdf-documents","title":"PDF Documents","text":"Python<pre><code>from rag_toolkit.infra.parsers.pdf import PDFParser\nfrom rag_toolkit.core.chunking import SemanticChunker\n\n# Parse PDF\nparser = PDFParser()\ndocument = await parser.parse(\"document.pdf\")\n\n# Chunk with page awareness\nchunker = SemanticChunker(\n    mode=\"paragraph\",\n    preserve_page_numbers=True,\n)\n\nchunks = await chunker.chunk(document)\n\n# Chunks include page numbers\nfor chunk in chunks:\n    print(f\"Pages: {chunk.page_numbers}\")\n</code></pre>"},{"location":"guides/chunking/#code-files","title":"Code Files","text":"Python<pre><code>from rag_toolkit.core.chunking import CodeChunker\n\n# Chunk code by functions/classes\nchunker = CodeChunker(\n    language=\"python\",\n    chunk_by=\"function\",  # or \"class\", \"method\"\n    include_docstrings=True,\n)\n\ncode = \"\"\"\ndef function1():\n    '''Docstring'''\n    pass\n\ndef function2():\n    '''Another docstring'''\n    pass\n\"\"\"\n\nchunks = await chunker.chunk(code)\n\n# Each chunk is a function\nfor chunk in chunks:\n    print(f\"Function: {chunk.metadata['function_name']}\")\n</code></pre>"},{"location":"guides/chunking/#structured-data","title":"Structured Data","text":"Python<pre><code>from rag_toolkit.core.chunking import StructuredChunker\n\n# Chunk JSON/CSV\nchunker = StructuredChunker(\n    format=\"json\",\n    chunk_by=\"record\",  # Each record is a chunk\n)\n\njson_data = [\n    {\"id\": 1, \"text\": \"Record 1\"},\n    {\"id\": 2, \"text\": \"Record 2\"},\n]\n\nchunks = await chunker.chunk(json_data)\n</code></pre>"},{"location":"guides/chunking/#chunking-pipeline-integration","title":"Chunking Pipeline Integration","text":""},{"location":"guides/chunking/#with-rag-pipeline","title":"With RAG Pipeline","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.core.chunking import TokenChunker\n\n# Create chunker\nchunker = TokenChunker(chunk_size=512, chunk_overlap=50)\n\n# Create pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    chunker=chunker,  # Add chunker\n)\n\n# Automatically chunks before indexing\nawait pipeline.index(\n    texts=[long_document],  # Single long document\n    # Automatically chunked into smaller pieces\n)\n</code></pre>"},{"location":"guides/chunking/#custom-preprocessing","title":"Custom Preprocessing","text":"Python<pre><code>from rag_toolkit.core.chunking import Preprocessor\n\n# Create preprocessor\npreprocessor = Preprocessor(\n    lowercase=False,\n    remove_extra_whitespace=True,\n    remove_urls=True,\n    remove_emails=True,\n)\n\n# Preprocess before chunking\ncleaned_text = await preprocessor.process(raw_text)\nchunks = await chunker.chunk(cleaned_text)\n</code></pre>"},{"location":"guides/chunking/#quality-evaluation","title":"Quality Evaluation","text":""},{"location":"guides/chunking/#chunk-quality-metrics","title":"Chunk Quality Metrics","text":"Python<pre><code>from rag_toolkit.core.chunking import ChunkQualityEvaluator\n\n# Evaluate chunk quality\nevaluator = ChunkQualityEvaluator()\n\nmetrics = await evaluator.evaluate(chunks)\n\nprint(f\"Average chunk size: {metrics.avg_size}\")\nprint(f\"Size variance: {metrics.size_variance}\")\nprint(f\"Semantic coherence: {metrics.coherence:.2f}\")\nprint(f\"Information density: {metrics.density:.2f}\")\n</code></pre>"},{"location":"guides/chunking/#ab-testing","title":"A/B Testing","text":"Python<pre><code># Compare two chunking strategies\nchunker_a = FixedSizeChunker(chunk_size=500)\nchunker_b = TokenChunker(chunk_size=512)\n\n# Test both\nresults_a = await test_retrieval(chunker_a, test_queries)\nresults_b = await test_retrieval(chunker_b, test_queries)\n\nprint(f\"Strategy A precision: {results_a.precision:.2f}\")\nprint(f\"Strategy B precision: {results_b.precision:.2f}\")\n</code></pre>"},{"location":"guides/chunking/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/chunking/#parallel-chunking","title":"Parallel Chunking","text":"Python<pre><code>import asyncio\n\nasync def chunk_documents_parallel(\n    documents: list[str],\n    chunker,\n    max_concurrent: int = 10\n):\n    \"\"\"Chunk multiple documents in parallel.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def chunk_one(doc: str):\n        async with semaphore:\n            return await chunker.chunk(doc)\n\n    tasks = [chunk_one(doc) for doc in documents]\n    return await asyncio.gather(*tasks)\n\n# Usage\nall_chunks = await chunk_documents_parallel(documents, chunker)\n</code></pre>"},{"location":"guides/chunking/#caching","title":"Caching","text":"Python<pre><code>from functools import lru_cache\n\nclass CachedChunker:\n    \"\"\"Chunker with caching.\"\"\"\n\n    def __init__(self, chunker):\n        self.chunker = chunker\n        self._cache = {}\n\n    async def chunk(self, text: str):\n        \"\"\"Chunk with caching.\"\"\"\n        # Use hash as cache key\n        key = hash(text)\n\n        if key not in self._cache:\n            self._cache[key] = await self.chunker.chunk(text)\n\n        return self._cache[key]\n\n# Usage\ncached_chunker = CachedChunker(chunker)\n</code></pre>"},{"location":"guides/chunking/#best-practices_1","title":"Best Practices","text":"<ol> <li>Choose the Right Strategy</li> <li>Start with TokenChunker for production</li> <li>Use SemanticChunker for maintaining context</li> <li> <p>Use specialized chunkers for specific content</p> </li> <li> <p>Optimize Chunk Size</p> </li> <li>Test different sizes with your data</li> <li>Consider your embedding model's optimal input</li> <li> <p>Balance precision vs context</p> </li> <li> <p>Use Overlap</p> </li> <li>Always use 10-20% overlap</li> <li>Increases retrieval quality significantly</li> <li> <p>Small performance cost, big quality gain</p> </li> <li> <p>Enrich with Metadata</p> </li> <li>Add source, page, section metadata</li> <li>Enables powerful filtering</li> <li> <p>Improves traceability</p> </li> <li> <p>Preprocess Text</p> </li> <li>Remove noise (extra whitespace, etc.)</li> <li>Normalize text encoding</li> <li> <p>Handle special characters</p> </li> <li> <p>Test and Evaluate</p> </li> <li>A/B test different strategies</li> <li>Measure retrieval quality</li> <li>Iterate based on results</li> </ol>"},{"location":"guides/chunking/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"guides/chunking/#chunks-too-large_1","title":"Chunks Too Large","text":"Python<pre><code># Reduce chunk size\nchunker = TokenChunker(\n    chunk_size=256,  # Smaller chunks\n    chunk_overlap=25\n)\n</code></pre>"},{"location":"guides/chunking/#chunks-too-small_1","title":"Chunks Too Small","text":"Python<pre><code># Increase chunk size\nchunker = TokenChunker(\n    chunk_size=1024,  # Larger chunks\n    chunk_overlap=100\n)\n</code></pre>"},{"location":"guides/chunking/#context-loss-at-boundaries","title":"Context Loss at Boundaries","text":"Python<pre><code># Increase overlap\nchunker = FixedSizeChunker(\n    chunk_size=500,\n    chunk_overlap=100  # 20% overlap\n)\n</code></pre>"},{"location":"guides/chunking/#poor-semantic-coherence","title":"Poor Semantic Coherence","text":"Python<pre><code># Use semantic chunker\nchunker = SemanticChunker(\n    mode=\"paragraph\",\n    max_chunk_size=800\n)\n</code></pre>"},{"location":"guides/chunking/#next-steps_1","title":"Next Steps","text":"<ul> <li>RAG Pipeline - Integrate chunking</li> <li>Embeddings Guide - Optimal embedding sizes</li> <li>Vector Stores - Store chunks</li> <li>Production Setup</li> </ul>"},{"location":"guides/chunking/#see-also_1","title":"See Also","text":"<ul> <li>Core Concepts - Chunking fundamentals</li> <li>Architecture - System design</li> <li>Token Limits - Model context windows</li> </ul>"},{"location":"guides/core_concepts/","title":"Core Concepts","text":""},{"location":"guides/core_concepts/#core-concepts","title":"Core Concepts","text":"<p>Master these fundamental concepts to unlock the full potential of RAG Toolkit.</p>"},{"location":"guides/core_concepts/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<p>What is RAG?</p> <p>RAG combines information retrieval with text generation to create more accurate, factual, and contextual responses.</p>"},{"location":"guides/core_concepts/#traditional-llm","title":"Traditional LLM","text":"<pre><code>graph LR\n    A[\ud83d\udc64 User Query] --&gt; B[\ud83e\udd16 LLM] --&gt; C[\ud83d\udcdd Response]\n    style A fill:#e3f2fd\n    style C fill:#ffebee</code></pre> <p>Limitations</p> <ul> <li> Limited to training data cutoff</li> <li> Can hallucinate facts</li> <li> No access to private/recent data</li> <li> No source citations</li> </ul>"},{"location":"guides/core_concepts/#material-brain-circuit-rag-enhanced-llm","title":":material-brain-circuit: RAG-Enhanced LLM","text":"<pre><code>graph LR\n    A[\ud83d\udc64 User Query] --&gt; B[\ud83d\udd0d Retrieval]\n    B --&gt; C[\ud83d\udcbe Vector Store]\n    C --&gt; D[\ud83d\udcda Relevant Docs]\n    D --&gt; E[\ud83d\udccb Context + Query]\n    E --&gt; F[\ud83e\udd16 LLM]\n    F --&gt; G[\u2728 Response]\n    style A fill:#e3f2fd\n    style G fill:#c8e6c9</code></pre> <p>Benefits</p> <ul> <li> Access to current information</li> <li> Reduced hallucinations</li> <li> Citations and sources</li> <li> Private data integration</li> <li> Always up-to-date knowledge</li> </ul>"},{"location":"guides/core_concepts/#key-components","title":"Key Components","text":"<p>The Building Blocks of RAG</p>"},{"location":"guides/core_concepts/#1-embeddings","title":"1.  Embeddings","text":"<p>Semantic Vector Representations</p> <p>Embeddings convert text into numerical vectors that capture semantic meaning.</p> embedding_example.py<pre><code>text = \"Hello world\"\nvector = embedding.embed(text)\n# [0.23, -0.45, 0.12, ...]  # 768 dimensions\n</code></pre> <p>Semantic Similarity:</p> Python<pre><code>embed(\"dog\") \u2248 embed(\"puppy\")       # High similarity \u2713\nembed(\"dog\") \u2260 embed(\"computer\")    # Low similarity \u2717\n</code></pre> <p>Key Property</p> <p>Similar texts produce similar vectors \u2014 enabling semantic search beyond keyword matching!</p> <ul> <li> <p> Dense Vectors</p> <p>Typically 384-4096 dimensions per text</p> </li> <li> <p> Semantic Understanding</p> <p>Captures context, synonyms, relationships</p> </li> <li> <p> Efficient Search</p> <p>Fast vector similarity operations</p> </li> <li> <p>:material-language: Language Agnostic</p> <p>Works across multiple languages</p> </li> </ul>"},{"location":"guides/core_concepts/#2-vector-stores","title":"2.  Vector Stores","text":"<p>Efficient Similarity Search</p> <p>Vector stores enable lightning-fast similarity search over millions of embeddings.</p> vector_store_example.py<pre><code># Store documents as vectors\nstore.insert(\n    collection=\"docs\",\n    vectors=[vector1, vector2, vector3],\n    texts=[\"text1\", \"text2\", \"text3\"],\n    metadata=[{\"page\": 1}, {\"page\": 2}, {\"page\": 3}],\n)\n\n# Find similar documents\nresults = store.search(\n    collection=\"docs\",\n    query_vector=query_vector,\n    top_k=5,\n)\n</code></pre> <p>Popular Vector Stores:</p> MilvusPineconeQdrant Python<pre><code># High-performance, scalable\nstore = MilvusVectorStore(\n    host=\"localhost\",\n    port=\"19530\",\n    collection_name=\"docs\"\n)\n</code></pre> <ul> <li> Production-ready performance</li> <li> Horizontal scalability</li> <li>:material-gpu: GPU acceleration support</li> </ul> Python<pre><code># Managed cloud service\nstore = PineconeVectorStore(\n    api_key=\"your-key\",\n    environment=\"us-west1-gcp\"\n)\n</code></pre> <ul> <li> Fully managed</li> <li> Auto-scaling</li> <li> Enterprise security</li> </ul> Python<pre><code># Developer-friendly\nstore = QdrantVectorStore(\n    url=\"http://localhost:6333\",\n    collection_name=\"docs\"\n)\n</code></pre> <ul> <li> Rich filtering capabilities</li> <li> Easy Docker setup</li> <li> JSON-based schema</li> </ul>"},{"location":"guides/core_concepts/#3-llms-large-language-models","title":"3.  LLMs (Large Language Models)","text":"<p>Natural Language Generation</p> <p>LLMs generate human-like text based on input prompts and retrieved context.</p> llm_example.py<pre><code>prompt = \"\"\"\nContext: {retrieved_context}\n\nQuestion: {user_question}\n\nAnswer:\"\"\"\n\nresponse = llm.generate(prompt)\n</code></pre> <p>Popular LLMs:</p> Model Provider Context Quality Cost GPT-4 OpenAI 128K $$$ Claude 3 Anthropic 200K $$$ Llama 3 Meta (Ollama) 8K Free Mistral Mistral (Ollama) 32K Free <p>Choosing an LLM</p> <ul> <li>Production: GPT-4 or Claude 3 for best quality</li> <li>Development: Ollama models for free local testing</li> <li>Cost-sensitive: GPT-3.5 Turbo for balanced performance</li> </ul>"},{"location":"guides/core_concepts/#4-chunking","title":"4.  Chunking","text":"<p>Document Segmentation</p> <p>Splitting documents into manageable pieces for optimal embedding and retrieval.</p> chunking_example.py<pre><code>document = \"Very long document with lots of content...\"\n\nchunks = chunker.chunk(\n    document,\n    chunk_size=512,\n    chunk_overlap=50,\n)\n# [\"chunk1...\", \"chunk2...\", \"chunk3...\"]\n</code></pre> <p>Chunking Strategies:</p> <ul> <li> <p> Fixed Size</p> Python<pre><code>chunks = fixed_chunker.chunk(\n    text, chunk_size=512\n)\n</code></pre> <p>Simple and consistent chunk sizes</p> </li> <li> <p> Sentence-Based</p> Python<pre><code>chunks = sentence_chunker.chunk(\n    text, max_sentences=10\n)\n</code></pre> <p>Respects semantic boundaries</p> </li> <li> <p> Recursive</p> Python<pre><code>chunks = recursive_chunker.chunk(\n    text, separators=[\"\\n\\n\", \"\\n\", \". \"]\n)\n</code></pre> <p>Hierarchical splitting strategy</p> </li> <li> <p> Token-Aware</p> Python<pre><code>chunks = token_chunker.chunk(\n    text, max_tokens=800\n)\n</code></pre> <p>Respects LLM token limits</p> </li> </ul> <p>Chunking Best Practices</p> <ul> <li>Chunk size: 300-800 tokens optimal for most use cases</li> <li>Overlap: 10-20% overlap preserves context at boundaries</li> <li>Strategy: Match chunking to document structure (headings, paragraphs)</li> </ul>"},{"location":"guides/core_concepts/#material-workflow-the-rag-workflow","title":":material-workflow: The RAG Workflow","text":"<p>End-to-End Process</p>"},{"location":"guides/core_concepts/#indexing-phase","title":"Indexing Phase","text":"<pre><code>graph TB\n    A[\ud83d\udcc1 Documents] --&gt; B[\u2702\ufe0f Chunking]\n    B --&gt; C[\ud83d\udcdd Text Chunks]\n    C --&gt; D[\ud83d\udd22 Embedding]\n    D --&gt; E[\ud83d\udcca Vectors]\n    E --&gt; F[\ud83d\udcbe Vector Store]\n\n    style A fill:#e3f2fd\n    style F fill:#c8e6c9</code></pre> indexing_pipeline.py<pre><code># 1. Load documents\ndocuments = load_documents(\"./data/\")\n\n# 2. Chunk documents\nchunks = chunker.chunk_documents(documents)\n\n# 3. Generate embeddings\nvectors = embedding.embed_batch(chunks)\n\n# 4. Store in vector database\nstore.insert(\n    collection=\"knowledge_base\",\n    vectors=vectors,\n    texts=chunks,\n    metadata=[{\"source\": doc.source, \"page\": doc.page} \n              for doc in documents],\n)\n</code></pre>"},{"location":"guides/core_concepts/#query-phase","title":"Query Phase","text":"<pre><code>graph TB\n    A[\ud83d\udcac User Query] --&gt; B[\ud83d\udd22 Embed Query]\n    B --&gt; C[\ud83d\udcca Query Vector]\n    C --&gt; D[\ud83d\udd0d Similarity Search]\n    D --&gt; E[\ud83d\udcbe Vector Store]\n    E --&gt; F[\ud83d\udcda Retrieved Docs]\n    F --&gt; G[\ud83d\udccb Build Context]\n    G --&gt; H[\ud83e\udd16 LLM Generate]\n    H --&gt; I[\u2728 Final Answer]\n\n    style A fill:#e3f2fd\n    style I fill:#c8e6c9</code></pre> query_pipeline.py<pre><code># 1. Embed user query\nquery = \"What is RAG?\"\nquery_vector = embedding.embed(query)\n\n# 2. Search for similar documents\nresults = store.search(\n    collection=\"knowledge_base\",\n    query_vector=query_vector,\n    top_k=5,\n)\n\n# 3. Build context from results\ncontext = \"\\n\\n\".join([r.text for r in results])\n\n# 4. Generate answer with LLM\nprompt = f\"\"\"\nContext: {context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n\nanswer = llm.generate(prompt)\nprint(answer)\n</code></pre>"},{"location":"guides/core_concepts/#2-retrieve-similar-chunks","title":"2. Retrieve similar chunks","text":"<p>results = store.search(     collection=\"knowledge_base\",     query_vector=query_vector,     top_k=5, )</p>"},{"location":"guides/core_concepts/#3-assemble-context","title":"3. Assemble context","text":"<p>context = \"\\n\\n\".join([r.text for r in results])</p>"},{"location":"guides/core_concepts/#4-generate-response","title":"4. Generate response","text":"<p>prompt = f\"\"\" Use the following context to answer the question.</p> <p>Context:</p> <p>Question: {query}</p> <p>Answer:\"\"\"</p> <p>answer = llm.generate(prompt) </p>Text Only<pre><code>## Semantic Search vs Keyword Search\n\n### Keyword Search\n\n```python\nquery = \"dog care\"\nresults = keyword_search(query)\n# Matches: \"dog\", \"care\", \"dogs\"\n</code></pre><p></p> <p>Limitations: - Exact word matching - Misses synonyms - No context understanding</p>"},{"location":"guides/core_concepts/#semantic-search-embeddings","title":"Semantic Search (Embeddings)","text":"Python<pre><code>query = \"dog care\"\nquery_vector = embed(query)\nresults = vector_search(query_vector)\n# Matches: \"pet grooming\", \"canine health\", \"puppy training\"\n</code></pre> <p>Advantages: - \u2705 Understands meaning - \u2705 Finds related concepts - \u2705 Language-agnostic - \u2705 Context-aware</p>"},{"location":"guides/core_concepts/#protocols-vs-inheritance","title":"Protocols vs Inheritance","text":""},{"location":"guides/core_concepts/#traditional-approach-inheritance","title":"Traditional Approach (Inheritance)","text":"Python<pre><code>from abc import ABC, abstractmethod\n\nclass BaseEmbedding(ABC):\n    @abstractmethod\n    def embed(self, text: str) -&gt; list[float]:\n        pass\n\nclass MyEmbedding(BaseEmbedding):  # Must inherit!\n    def embed(self, text: str) -&gt; list[float]:\n        return [0.0] * 768\n</code></pre>"},{"location":"guides/core_concepts/#rag-toolkit-approach-protocols","title":"rag-toolkit Approach (Protocols)","text":"Python<pre><code>from typing import Protocol\n\nclass EmbeddingClient(Protocol):\n    def embed(self, text: str) -&gt; list[float]: ...\n\nclass MyEmbedding:  # No inheritance needed!\n    def embed(self, text: str) -&gt; list[float]:\n        return [0.0] * 768\n\n# Works seamlessly\npipeline = RagPipeline(embedding_client=MyEmbedding())\n</code></pre> <p>Benefits: - \u2705 No inheritance required - \u2705 Duck typing with type safety - \u2705 Easy testing with mocks - \u2705 More flexibility</p>"},{"location":"guides/core_concepts/#dependency-injection","title":"Dependency Injection","text":"<p>Components receive their dependencies explicitly:</p> Python<pre><code># \u274c Bad: Hidden dependencies\nclass Pipeline:\n    def __init__(self):\n        self.embedding = OllamaEmbedding()  # Hardcoded!\n        self.llm = OllamaLLM()              # Can't test!\n\n# \u2705 Good: Explicit dependencies\nclass RagPipeline:\n    def __init__(\n        self,\n        embedding_client: EmbeddingClient,\n        llm_client: LLMClient,\n        vector_store: VectorStoreClient,\n    ):\n        self.embedding = embedding_client\n        self.llm = llm_client\n        self.store = vector_store\n</code></pre> <p>Benefits: - \u2705 Easy to test (inject mocks) - \u2705 Flexible (swap implementations) - \u2705 Clear dependencies - \u2705 No global state</p>"},{"location":"guides/core_concepts/#metadata-and-filtering","title":"Metadata and Filtering","text":"<p>Attach metadata to chunks for filtering:</p> Python<pre><code># Index with metadata\nstore.insert(\n    collection=\"docs\",\n    vectors=vectors,\n    texts=chunks,\n    metadata=[\n        {\"source\": \"manual.pdf\", \"page\": 1, \"section\": \"intro\"},\n        {\"source\": \"manual.pdf\", \"page\": 2, \"section\": \"setup\"},\n        {\"source\": \"faq.txt\", \"topic\": \"installation\"},\n    ],\n)\n\n# Query with filters\nresults = store.search(\n    query_vector=query_vector,\n    top_k=5,\n    filters={\"source\": \"manual.pdf\"},  # Only from manual\n)\n</code></pre>"},{"location":"guides/core_concepts/#retrieval-strategies","title":"Retrieval Strategies","text":""},{"location":"guides/core_concepts/#simple-retrieval","title":"Simple Retrieval","text":"Python<pre><code>results = store.search(query_vector, top_k=5)\n</code></pre>"},{"location":"guides/core_concepts/#hybrid-search-vector-keyword","title":"Hybrid Search (Vector + Keyword)","text":"Python<pre><code>results = store.hybrid_search(\n    query_vector=query_vector,\n    query_text=query_text,\n    alpha=0.5,  # 50% vector, 50% keyword\n)\n</code></pre>"},{"location":"guides/core_concepts/#multi-query-retrieval","title":"Multi-Query Retrieval","text":"Python<pre><code># Generate multiple query variations\nqueries = [\n    \"What is RAG?\",\n    \"How does RAG work?\",\n    \"Explain retrieval augmented generation\",\n]\n\nall_results = []\nfor q in queries:\n    vector = embed(q)\n    results = store.search(vector, top_k=3)\n    all_results.extend(results)\n\n# Deduplicate and rerank\nfinal_results = deduplicate_and_rerank(all_results)\n</code></pre>"},{"location":"guides/core_concepts/#context-window-management","title":"Context Window Management","text":"<p>LLMs have token limits. Manage context carefully:</p> Python<pre><code># Check token count\ndef count_tokens(text: str) -&gt; int:\n    # Approximate: 1 token \u2248 4 characters\n    return len(text) // 4\n\n# Fit within context window\nmax_context_tokens = 4096\nprompt_tokens = 100\nanswer_tokens = 512\n\navailable_tokens = max_context_tokens - prompt_tokens - answer_tokens\n\n# Select chunks that fit\nselected_chunks = []\ntotal_tokens = 0\n\nfor chunk in ranked_chunks:\n    chunk_tokens = count_tokens(chunk.text)\n    if total_tokens + chunk_tokens &lt;= available_tokens:\n        selected_chunks.append(chunk)\n        total_tokens += chunk_tokens\n</code></pre>"},{"location":"guides/core_concepts/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"guides/core_concepts/#retrieval-metrics","title":"Retrieval Metrics","text":"<p>Precision@K: Relevant results in top K </p>Python<pre><code>relevant_in_topk = sum(1 for r in results[:k] if r.is_relevant)\nprecision = relevant_in_topk / k\n</code></pre><p></p> <p>Recall@K: Relevant results found </p>Python<pre><code>recall = relevant_in_topk / total_relevant\n</code></pre><p></p> <p>Mean Reciprocal Rank (MRR): Rank of first relevant result </p>Python<pre><code>for i, result in enumerate(results, 1):\n    if result.is_relevant:\n        mrr = 1 / i\n        break\n</code></pre><p></p>"},{"location":"guides/core_concepts/#generation-metrics","title":"Generation Metrics","text":"<p>BLEU: N-gram overlap ROUGE: Recall of n-grams BERTScore: Semantic similarity</p>"},{"location":"guides/core_concepts/#best-practices","title":"Best Practices","text":""},{"location":"guides/core_concepts/#1-chunk-size","title":"1. Chunk Size","text":"Python<pre><code># \u274c Too small: Loses context\nchunk_size = 50\n\n# \u274c Too large: Too much irrelevant info\nchunk_size = 5000\n\n# \u2705 Just right: Balanced\nchunk_size = 512  # Adjust based on your needs\n</code></pre>"},{"location":"guides/core_concepts/#2-overlap","title":"2. Overlap","text":"Python<pre><code># \u2705 Add overlap to preserve context\nchunk_overlap = chunk_size // 10  # 10% overlap\n</code></pre>"},{"location":"guides/core_concepts/#3-metadata","title":"3. Metadata","text":"Python<pre><code># \u2705 Include rich metadata\nmetadata = {\n    \"source\": doc.filename,\n    \"page\": page_num,\n    \"section\": section_title,\n    \"timestamp\": doc.created_at,\n    \"author\": doc.author,\n}\n</code></pre>"},{"location":"guides/core_concepts/#4-error-handling","title":"4. Error Handling","text":"Python<pre><code># \u2705 Handle errors gracefully\ntry:\n    results = store.search(query_vector, top_k=5)\nexcept Exception as e:\n    logger.error(f\"Search failed: {e}\")\n    results = []  # Fallback to empty results\n</code></pre>"},{"location":"guides/core_concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Protocols</li> <li>Explore Vector Stores</li> <li>Read about Embeddings</li> <li>See RAG Pipeline for complete workflows</li> </ul>"},{"location":"guides/embeddings/","title":"Embeddings","text":""},{"location":"guides/embeddings/#embeddings","title":"Embeddings","text":"<p>Embeddings are the cornerstone of semantic search and RAG systems. Master embeddings to build powerful, accurate retrieval systems.</p>"},{"location":"guides/embeddings/#what-are-embeddings","title":"What are Embeddings?","text":"<p>Semantic Vector Representations</p> <p>Embeddings are dense vector representations of text that capture semantic meaning. Similar texts produce similar embeddings, enabling semantic search beyond keyword matching.</p> Python<pre><code>embed(\"dog\") \u2248 embed(\"puppy\")       # High similarity score: 0.92\nembed(\"dog\") \u2260 embed(\"computer\")    # Low similarity score: 0.15\n</code></pre>"},{"location":"guides/embeddings/#key-properties","title":"Key Properties","text":"<ul> <li> <p> Dense Vectors</p> <p>Typically 384-4096 dimensions</p> Python<pre><code>vector = embed(\"Hello\")\nlen(vector)  # 768 dimensions\n</code></pre> </li> <li> <p> Semantic Similarity</p> <p>Similar meaning \u2192 similar vectors</p> Python<pre><code>cosine_similarity(\n    embed(\"car\"),\n    embed(\"automobile\")\n)  # 0.89\n</code></pre> </li> <li> <p>:material-language: Language Understanding</p> <p>Captures context, synonyms, relationships</p> <ul> <li>\"bank\" (financial) \u2260 \"bank\" (river)</li> <li>Context-aware representations</li> </ul> </li> <li> <p> Efficient Search</p> <p>Fast vector similarity operations</p> <ul> <li>Millions of vectors in milliseconds</li> <li>Approximate nearest neighbor (ANN)</li> </ul> </li> </ul>"},{"location":"guides/embeddings/#supported-embedding-providers","title":"Supported Embedding Providers","text":"<p>Choose Your Provider</p>"},{"location":"guides/embeddings/#material-openai-openai-recommended","title":":material-openai: OpenAI (Recommended)","text":"<p>State-of-the-Art Quality</p> <p>OpenAI provides industry-leading embedding models with excellent quality and speed.</p> <p>Available Models:</p> Model Dimensions Cost (per 1M tokens) Use Case <code>text-embedding-3-small</code> 1536 $0.02  Fast, cost-effective <code>text-embedding-3-large</code> 3072 $0.13  Highest quality <code>text-embedding-ada-002</code> 1536 $0.10  Previous gen (still good) <p>Installation:</p> Install OpenAI Support<pre><code>pip install rag-toolkit[openai]\nexport OPENAI_API_KEY=\"your-api-key\"\n</code></pre> <p>Usage:</p> BasicBatch ProcessingAdvanced Python<pre><code>from rag_toolkit.infra.embedding import OpenAIEmbedding\n\n# Initialize\nembedding = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"your-api-key\",  # Or use OPENAI_API_KEY env var\n)\n\n# Embed single text\nvector = await embedding.embed(\"Hello world\")\nprint(f\"Dimension: {len(vector)}\")  # 1536\n</code></pre> Python<pre><code># Embed multiple texts (batched for efficiency)\nvectors = await embedding.embed_batch([\n    \"First document\",\n    \"Second document\",\n    \"Third document\"\n])\nprint(f\"Embedded {len(vectors)} documents\")\n</code></pre> Python<pre><code>embedding = OpenAIEmbedding(\n    model=\"text-embedding-3-large\",\n    api_key=\"your-api-key\",\n    batch_size=100,      # Process 100 at a time\n    timeout=30.0,        # 30 second timeout\n    max_retries=3,       # Retry failed requests\n)\n</code></pre> <p>Pricing (as of January 2026)</p> <ul> <li><code>text-embedding-3-small</code>: $0.02 / 1M tokens \u2014 Best value</li> <li><code>text-embedding-3-large</code>: $0.13 / 1M tokens \u2014 Best quality</li> <li><code>text-embedding-ada-002</code>: $0.10 / 1M tokens \u2014 Legacy</li> </ul>"},{"location":"guides/embeddings/#ollama-local-free","title":"Ollama (Local, Free)","text":"<p>Privacy &amp; Cost-Free</p> <p>Run powerful embedding models locally with Ollama \u2014 perfect for privacy-focused deployments and zero API costs.</p> <p>Popular Models:</p> Model Dimensions Speed Quality Size <code>nomic-embed-text</code> 768  Fast 274MB <code>mxbai-embed-large</code> 1024  Medium 669MB <code>all-minilm</code> 384  Very fast 46MB <p>Installation:</p> macOS/LinuxDocker Bash<pre><code># Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull embedding model\nollama pull nomic-embed-text\n\n# Install RAG Toolkit with Ollama support\npip install rag-toolkit[ollama]\n</code></pre> Bash<pre><code># Run Ollama in Docker\ndocker run -d -p 11434:11434 ollama/ollama\n\n# Pull model\ndocker exec ollama ollama pull nomic-embed-text\n</code></pre> <p>Usage:</p> ollama_embedding.py<pre><code>from rag_toolkit.infra.embedding import OllamaEmbedding\n\n# Initialize\nembedding = OllamaEmbedding(\n    model=\"nomic-embed-text\",\n    base_url=\"http://localhost:11434\",  # Default Ollama URL\n)\n\n# Embed text\nvector = await embedding.embed(\"Hello world\")\nprint(f\"Dimension: {len(vector)}\")  # 768\n\n# Batch embedding\nvectors = await embedding.embed_batch([\n    \"First document\",\n    \"Second document\",\n    \"Third document\"\n])\n</code></pre> <p>Model Comparison:</p> <ul> <li> <p> nomic-embed-text</p> <p>768 dimensions | 274MB</p> Bash<pre><code>ollama pull nomic-embed-text\n</code></pre> <p>Best for: General purpose, balanced quality/speed</p> </li> <li> <p> mxbai-embed-large</p> <p>1024 dimensions | 669MB</p> Bash<pre><code>ollama pull mxbai-embed-large\n</code></pre> <p>Best for: High quality requirements</p> </li> <li> <p> all-minilm</p> <p>384 dimensions | 46MB</p> Bash<pre><code>ollama pull all-minilm\n</code></pre> <p>Best for: Speed-critical applications, low memory</p> </li> </ul> <p>Choosing an Ollama Model</p> <ul> <li>General use: <code>nomic-embed-text</code> \u2014 excellent balance</li> <li>High quality: <code>mxbai-embed-large</code> \u2014 best results</li> <li>Fast &amp; lightweight: <code>all-minilm</code> \u2014 minimal resources</li> </ul>"},{"location":"guides/embeddings/#configuration","title":"Configuration","text":""},{"location":"guides/embeddings/#batch-size","title":"Batch Size","text":"<p>Control how many texts are embedded at once:</p> Python<pre><code># OpenAI (handles batching automatically)\nembedding = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    batch_size=100,  # Default: 100\n)\n\n# Ollama\nembedding = OllamaEmbedding(\n    model=\"nomic-embed-text\",\n    batch_size=32,  # Smaller batches for local GPU\n)\n</code></pre>"},{"location":"guides/embeddings/#timeouts","title":"Timeouts","text":"<p>Set timeouts for embedding requests:</p> Python<pre><code>embedding = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    timeout=30.0,  # 30 seconds (default: 60)\n)\n</code></pre>"},{"location":"guides/embeddings/#retry-logic","title":"Retry Logic","text":"<p>Handle transient failures with retries:</p> Python<pre><code>embedding = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    max_retries=3,  # Default: 3\n    retry_delay=1.0,  # Seconds between retries\n)\n</code></pre>"},{"location":"guides/embeddings/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/embeddings/#dimension-reduction","title":"Dimension Reduction","text":"<p>Reduce embedding dimensions for memory efficiency (OpenAI only):</p> Python<pre><code># text-embedding-3-* models support dimension reduction\nembedding = OpenAIEmbedding(\n    model=\"text-embedding-3-large\",\n    dimensions=1024,  # Reduce from 3072 to 1024\n)\n\n# Maintains ~98% of quality at ~33% of dimensions\n</code></pre>"},{"location":"guides/embeddings/#custom-prefixes","title":"Custom Prefixes","text":"<p>Add prefixes for retrieval vs document embeddings:</p> Python<pre><code># For asymmetric search (query \u2260 documents)\nembedding = OllamaEmbedding(\n    model=\"nomic-embed-text\",\n    query_prefix=\"search_query: \",\n    document_prefix=\"search_document: \",\n)\n\n# Query embedding (with prefix)\nquery_vector = await embedding.embed(\n    \"What is machine learning?\",\n    is_query=True\n)\n\n# Document embedding (with prefix)\ndoc_vector = await embedding.embed(\n    \"Machine learning is a subset of AI...\",\n    is_query=False\n)\n</code></pre>"},{"location":"guides/embeddings/#normalize-embeddings","title":"Normalize Embeddings","text":"<p>Normalize vectors for cosine similarity:</p> Python<pre><code>embedding = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    normalize=True,  # L2 normalization (default: True)\n)\n\n# With normalization: cosine similarity = dot product\n# Without: need to compute cosine explicitly\n</code></pre>"},{"location":"guides/embeddings/#batch-processing","title":"Batch Processing","text":""},{"location":"guides/embeddings/#basic-batch-embedding","title":"Basic Batch Embedding","text":"Python<pre><code># Embed multiple documents efficiently\ndocuments = [\n    \"First document text\",\n    \"Second document text\",\n    # ... thousands more\n]\n\n# Automatically batched\nembeddings = await embedding.embed_batch(documents)\nprint(f\"Embedded {len(embeddings)} documents\")\n</code></pre>"},{"location":"guides/embeddings/#progress-tracking","title":"Progress Tracking","text":"Python<pre><code>from tqdm import tqdm\n\n# With progress bar\nbatch_size = 100\nall_embeddings = []\n\nfor i in tqdm(range(0, len(documents), batch_size)):\n    batch = documents[i:i+batch_size]\n    batch_embeddings = await embedding.embed_batch(batch)\n    all_embeddings.extend(batch_embeddings)\n</code></pre>"},{"location":"guides/embeddings/#error-handling","title":"Error Handling","text":"Python<pre><code>from rag_toolkit.core.embedding import EmbeddingError\n\ntry:\n    embeddings = await embedding.embed_batch(documents)\nexcept EmbeddingError as e:\n    print(f\"Embedding failed: {e}\")\n    # Handle error (retry, skip, etc.)\n</code></pre>"},{"location":"guides/embeddings/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"guides/embeddings/#by-quality","title":"By Quality","text":"<p>Best Quality (OpenAI): </p>Python<pre><code>embedding = OpenAIEmbedding(model=\"text-embedding-3-large\")\n# 3072 dimensions, highest quality\n# Use for: Production systems, critical applications\n</code></pre><p></p> <p>Balanced Quality (OpenAI): </p>Python<pre><code>embedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n# 1536 dimensions, excellent quality/cost ratio\n# Use for: Most applications (recommended default)\n</code></pre><p></p> <p>Good Quality (Ollama, Free): </p>Python<pre><code>embedding = OllamaEmbedding(model=\"nomic-embed-text\")\n# 768 dimensions, no API costs\n# Use for: Privacy-sensitive, development, offline\n</code></pre><p></p>"},{"location":"guides/embeddings/#by-speed","title":"By Speed","text":"<p>Fastest (Ollama, Local): </p>Python<pre><code>embedding = OllamaEmbedding(model=\"all-minilm\")\n# 384 dimensions, very fast\n# Use for: Real-time applications, large datasets\n</code></pre><p></p> <p>Fast (OpenAI): </p>Python<pre><code>embedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n# 1536 dimensions, fast API\n# Use for: Most applications\n</code></pre><p></p>"},{"location":"guides/embeddings/#by-cost","title":"By Cost","text":"<p>Free (Ollama): </p>Python<pre><code>embedding = OllamaEmbedding(model=\"nomic-embed-text\")\n# Zero API costs, requires local compute\n# Cost: GPU/CPU time only\n</code></pre><p></p> <p>Cost-Effective (OpenAI): </p>Python<pre><code>embedding = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    dimensions=512,  # Reduce dimensions = lower cost\n)\n# $0.02 / 1M tokens\n# Use for: Budget-conscious applications\n</code></pre><p></p>"},{"location":"guides/embeddings/#integration-with-rag","title":"Integration with RAG","text":""},{"location":"guides/embeddings/#basic-rag-pipeline","title":"Basic RAG Pipeline","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\nfrom rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\nfrom rag_toolkit.infra.llm import OpenAILLM\n\n# Setup embedding\nembedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Create RAG pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=MilvusVectorStore(\n        collection_name=\"documents\",\n        embedding_client=embedding,\n        dimension=1536,  # Match embedding dimension\n    ),\n    llm_client=OpenAILLM(model=\"gpt-4\"),\n)\n\n# Index documents\nawait pipeline.index(\n    texts=[\"Document 1\", \"Document 2\"],\n    metadatas=[{\"source\": \"doc1\"}, {\"source\": \"doc2\"}]\n)\n\n# Query\nresult = await pipeline.query(\"What is in the documents?\")\nprint(result.answer)\n</code></pre>"},{"location":"guides/embeddings/#hybrid-embedding-strategy","title":"Hybrid Embedding Strategy","text":"<p>Use different embeddings for different purposes:</p> Python<pre><code># Fast embedding for initial retrieval\nfast_embedding = OllamaEmbedding(model=\"all-minilm\")\n\n# High-quality embedding for reranking\nquality_embedding = OpenAIEmbedding(model=\"text-embedding-3-large\")\n\n# Two-stage retrieval\n# Stage 1: Fast search with all-minilm (1000 candidates)\nfast_vector_store = MilvusVectorStore(\n    collection_name=\"fast_search\",\n    embedding_client=fast_embedding,\n)\n\n# Stage 2: Rerank with text-embedding-3-large (top 10)\nquality_vector_store = MilvusVectorStore(\n    collection_name=\"quality_rerank\",\n    embedding_client=quality_embedding,\n)\n</code></pre>"},{"location":"guides/embeddings/#custom-embedding-clients","title":"Custom Embedding Clients","text":"<p>Implement your own embedding provider following the protocol:</p> Python<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass EmbeddingClient(Protocol):\n    \"\"\"Protocol for embedding clients.\"\"\"\n\n    @property\n    def dimension(self) -&gt; int:\n        \"\"\"Return embedding dimension.\"\"\"\n        ...\n\n    async def embed(self, text: str) -&gt; list[float]:\n        \"\"\"Embed a single text.\"\"\"\n        ...\n\n    async def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Embed multiple texts (batched).\"\"\"\n        ...\n</code></pre>"},{"location":"guides/embeddings/#example-huggingface-embeddings","title":"Example: HuggingFace Embeddings","text":"Python<pre><code>from sentence_transformers import SentenceTransformer\n\nclass HuggingFaceEmbedding:\n    \"\"\"HuggingFace sentence-transformers embedding client.\"\"\"\n\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        self.model = SentenceTransformer(model_name)\n        self._dimension = self.model.get_sentence_embedding_dimension()\n\n    @property\n    def dimension(self) -&gt; int:\n        return self._dimension\n\n    async def embed(self, text: str) -&gt; list[float]:\n        \"\"\"Embed single text.\"\"\"\n        embedding = self.model.encode(text, convert_to_tensor=False)\n        return embedding.tolist()\n\n    async def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Embed multiple texts.\"\"\"\n        embeddings = self.model.encode(\n            texts,\n            batch_size=32,\n            show_progress_bar=True,\n            convert_to_tensor=False\n        )\n        return embeddings.tolist()\n\n# Usage\nembedding = HuggingFaceEmbedding(\"all-MiniLM-L6-v2\")\nvector = await embedding.embed(\"Hello world\")\n</code></pre> <p>See Custom Vector Store Example for more details.</p>"},{"location":"guides/embeddings/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/embeddings/#caching-embeddings","title":"Caching Embeddings","text":"<p>Cache embeddings to avoid recomputing:</p> Python<pre><code>from functools import lru_cache\n\nclass CachedEmbedding:\n    \"\"\"Embedding client with caching.\"\"\"\n\n    def __init__(self, embedding_client):\n        self.client = embedding_client\n        self._embed_cached = lru_cache(maxsize=10000)(self._embed_single)\n\n    async def _embed_single(self, text: str) -&gt; tuple[float, ...]:\n        \"\"\"Cached embedding (must return tuple for hashability).\"\"\"\n        vector = await self.client.embed(text)\n        return tuple(vector)\n\n    async def embed(self, text: str) -&gt; list[float]:\n        \"\"\"Embed with caching.\"\"\"\n        return list(await self._embed_cached(text))\n\n    @property\n    def dimension(self) -&gt; int:\n        return self.client.dimension\n\n# Usage\nbase_embedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\ncached_embedding = CachedEmbedding(base_embedding)\n\n# First call: computes embedding\nv1 = await cached_embedding.embed(\"Hello\")  # API call\n\n# Second call: uses cache\nv2 = await cached_embedding.embed(\"Hello\")  # No API call\n</code></pre>"},{"location":"guides/embeddings/#parallel-processing","title":"Parallel Processing","text":"<p>Process documents in parallel:</p> Python<pre><code>import asyncio\n\nasync def embed_documents_parallel(\n    documents: list[str],\n    embedding_client,\n    max_concurrent: int = 10\n):\n    \"\"\"Embed documents with concurrency limit.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def embed_with_limit(text: str):\n        async with semaphore:\n            return await embedding_client.embed(text)\n\n    tasks = [embed_with_limit(doc) for doc in documents]\n    return await asyncio.gather(*tasks)\n\n# Usage\nembeddings = await embed_documents_parallel(\n    documents=large_document_list,\n    embedding_client=embedding,\n    max_concurrent=20  # 20 concurrent requests\n)\n</code></pre>"},{"location":"guides/embeddings/#batch-size-tuning","title":"Batch Size Tuning","text":"<p>Find optimal batch size for your use case:</p> Python<pre><code>import time\n\nasync def benchmark_batch_size(\n    documents: list[str],\n    embedding_client,\n    batch_sizes: list[int] = [10, 50, 100, 200]\n):\n    \"\"\"Benchmark different batch sizes.\"\"\"\n    for batch_size in batch_sizes:\n        embedding_client.batch_size = batch_size\n\n        start = time.time()\n        await embedding_client.embed_batch(documents)\n        duration = time.time() - start\n\n        docs_per_sec = len(documents) / duration\n        print(f\"Batch size {batch_size}: {docs_per_sec:.1f} docs/sec\")\n\n# Find best batch size\nawait benchmark_batch_size(test_documents, embedding)\n</code></pre>"},{"location":"guides/embeddings/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"guides/embeddings/#token-usage-tracking","title":"Token Usage Tracking","text":"<p>Track embedding costs:</p> Python<pre><code>class TokenTrackedEmbedding:\n    \"\"\"Wrapper to track token usage.\"\"\"\n\n    def __init__(self, embedding_client):\n        self.client = embedding_client\n        self.total_tokens = 0\n\n    async def embed(self, text: str) -&gt; list[float]:\n        # Rough estimate: 1 token \u2248 4 characters\n        tokens = len(text) // 4\n        self.total_tokens += tokens\n        return await self.client.embed(text)\n\n    async def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        tokens = sum(len(text) // 4 for text in texts)\n        self.total_tokens += tokens\n        return await self.client.embed_batch(texts)\n\n    @property\n    def dimension(self) -&gt; int:\n        return self.client.dimension\n\n    def get_cost(self, model: str = \"text-embedding-3-small\") -&gt; float:\n        \"\"\"Estimate cost in USD.\"\"\"\n        # Pricing per 1M tokens\n        prices = {\n            \"text-embedding-3-small\": 0.02,\n            \"text-embedding-3-large\": 0.13,\n            \"text-embedding-ada-002\": 0.10,\n        }\n        price_per_million = prices.get(model, 0.02)\n        return (self.total_tokens / 1_000_000) * price_per_million\n\n# Usage\ntracked = TokenTrackedEmbedding(OpenAIEmbedding())\nawait tracked.embed_batch(documents)\nprint(f\"Tokens used: {tracked.total_tokens:,}\")\nprint(f\"Estimated cost: ${tracked.get_cost():.4f}\")\n</code></pre>"},{"location":"guides/embeddings/#embedding-quality-check","title":"Embedding Quality Check","text":"<p>Verify embedding similarity:</p> Python<pre><code>import numpy as np\n\ndef cosine_similarity(v1: list[float], v2: list[float]) -&gt; float:\n    \"\"\"Compute cosine similarity between vectors.\"\"\"\n    a = np.array(v1)\n    b = np.array(v2)\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Test similarity\nv1 = await embedding.embed(\"dog\")\nv2 = await embedding.embed(\"puppy\")\nv3 = await embedding.embed(\"computer\")\n\nprint(f\"dog &lt;-&gt; puppy: {cosine_similarity(v1, v2):.3f}\")  # ~0.8-0.9\nprint(f\"dog &lt;-&gt; computer: {cosine_similarity(v1, v3):.3f}\")  # ~0.2-0.3\n</code></pre>"},{"location":"guides/embeddings/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/embeddings/#api-key-issues","title":"API Key Issues","text":"Python<pre><code>from rag_toolkit.infra.embedding import OpenAIEmbedding\n\ntry:\n    embedding = OpenAIEmbedding()\n    await embedding.embed(\"test\")\nexcept Exception as e:\n    if \"api_key\" in str(e).lower():\n        print(\"\u274c Invalid or missing API key\")\n        print(\"Set OPENAI_API_KEY environment variable\")\n</code></pre>"},{"location":"guides/embeddings/#ollama-not-running","title":"Ollama Not Running","text":"Python<pre><code>from rag_toolkit.infra.embedding import OllamaEmbedding\n\ntry:\n    embedding = OllamaEmbedding()\n    await embedding.embed(\"test\")\nexcept Exception as e:\n    if \"connection\" in str(e).lower():\n        print(\"\u274c Ollama not running\")\n        print(\"Start Ollama: ollama serve\")\n</code></pre>"},{"location":"guides/embeddings/#model-not-found","title":"Model Not Found","text":"Python<pre><code>try:\n    embedding = OllamaEmbedding(model=\"nomic-embed-text\")\n    await embedding.embed(\"test\")\nexcept Exception as e:\n    if \"not found\" in str(e).lower():\n        print(\"\u274c Model not found\")\n        print(\"Pull model: ollama pull nomic-embed-text\")\n</code></pre>"},{"location":"guides/embeddings/#dimension-mismatch","title":"Dimension Mismatch","text":"Python<pre><code># Ensure embedding and vector store dimensions match\nembedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\nprint(f\"Embedding dimension: {embedding.dimension}\")  # 1536\n\nvector_store = MilvusVectorStore(\n    collection_name=\"docs\",\n    dimension=embedding.dimension,  # Must match!\n)\n</code></pre>"},{"location":"guides/embeddings/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Model</li> <li>Production: <code>text-embedding-3-small</code> (balanced quality/cost)</li> <li>High quality: <code>text-embedding-3-large</code></li> <li> <p>Privacy/offline: <code>nomic-embed-text</code> (Ollama)</p> </li> <li> <p>Batch Processing</p> </li> <li>Always use <code>embed_batch()</code> for multiple texts</li> <li>Tune batch size based on your infrastructure</li> <li> <p>Use async for parallel requests</p> </li> <li> <p>Error Handling</p> </li> <li>Implement retries for transient failures</li> <li>Log failed embeddings for debugging</li> <li> <p>Have fallback embedding strategy</p> </li> <li> <p>Cost Optimization</p> </li> <li>Cache frequently embedded texts</li> <li>Use dimension reduction when possible</li> <li> <p>Consider Ollama for development</p> </li> <li> <p>Quality Assurance</p> </li> <li>Test embeddings with known similar/dissimilar texts</li> <li>Monitor embedding quality over time</li> <li> <p>Validate dimension consistency</p> </li> <li> <p>Performance</p> </li> <li>Use appropriate batch sizes</li> <li>Implement connection pooling</li> <li>Consider caching layer</li> </ol>"},{"location":"guides/embeddings/#next-steps","title":"Next Steps","text":"<ul> <li>Vector Stores Guide - Store and search embeddings</li> <li>RAG Pipeline - Build complete RAG systems</li> <li>Custom Vector Store Example</li> <li>Production Setup - Deploy to production</li> </ul>"},{"location":"guides/embeddings/#see-also","title":"See Also","text":"<ul> <li>OpenAI Embeddings Documentation</li> <li>Ollama Documentation</li> <li>Embedding Protocol</li> </ul>"},{"location":"guides/llms/","title":"LLMs","text":""},{"location":"guides/llms/#llm-clients","title":"LLM Clients","text":"<p>Large Language Models (LLMs) power the generation phase of RAG systems. Master LLM integration for high-quality, contextual responses.</p>"},{"location":"guides/llms/#overview","title":"Overview","text":"<p>What LLM Clients Do</p> <p>LLM clients in RAG Toolkit handle the entire text generation pipeline.</p> <p>Capabilities:</p> <ul> <li> <p> Text Generation</p> <p>Generate natural language answers from prompts</p> </li> <li> <p> Context Integration</p> <p>Combine retrieved documents with user queries</p> </li> <li> <p> Streaming</p> <p>Real-time response generation</p> </li> <li> <p> Error Handling</p> <p>Automatic retries, rate limiting, fallbacks</p> </li> </ul>"},{"location":"guides/llms/#supported-llm-providers","title":"Supported LLM Providers","text":"<p>Choose Your Provider</p>"},{"location":"guides/llms/#openai-recommended","title":"OpenAI (Recommended)","text":"<p>OpenAI provides state-of-the-art models with excellent quality and reliability.</p> <p>Models: - <code>gpt-4-turbo</code>: Latest GPT-4, best quality, 128k context - <code>gpt-4</code>: Standard GPT-4, 8k context - <code>gpt-3.5-turbo</code>: Fast and cost-effective, 16k context</p> <p>Installation:</p> Bash<pre><code>pip install rag-toolkit[openai]\nexport OPENAI_API_KEY=\"your-api-key\"\n</code></pre> <p>Usage:</p> Python<pre><code>from rag_toolkit.infra.llm import OpenAILLM\n\n# Initialize\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    api_key=\"your-api-key\",  # Or set OPENAI_API_KEY env var\n    temperature=0.7,\n    max_tokens=1000,\n)\n\n# Generate response\nresponse = await llm.generate(\n    prompt=\"What is machine learning?\",\n)\nprint(response)\n\n# Generate with system message\nresponse = await llm.generate(\n    prompt=\"Explain quantum computing\",\n    system_message=\"You are a helpful physics teacher.\"\n)\n</code></pre> <p>Pricing (as of Dec 2024): - <code>gpt-4-turbo</code>: $10 / 1M input tokens, $30 / 1M output tokens - <code>gpt-4</code>: $30 / 1M input tokens, $60 / 1M output tokens - <code>gpt-3.5-turbo</code>: $0.50 / 1M input tokens, $1.50 / 1M output tokens</p>"},{"location":"guides/llms/#ollama-local-free","title":"Ollama (Local, Free)","text":"<p>Run powerful LLMs locally with Ollama for privacy and zero API costs.</p> <p>Popular Models: - <code>llama3</code>: Meta's Llama 3, excellent quality - <code>mistral</code>: Mistral 7B, fast and capable - <code>phi3</code>: Microsoft Phi-3, efficient small model - <code>gemma</code>: Google Gemma, strong performance</p> <p>Installation:</p> Bash<pre><code># Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull a model\nollama pull llama3\n\n# Install rag-toolkit with Ollama support\npip install rag-toolkit[ollama]\n</code></pre> <p>Usage:</p> Python<pre><code>from rag_toolkit.infra.llm import OllamaLLM\n\n# Initialize\nllm = OllamaLLM(\n    model=\"llama3\",\n    base_url=\"http://localhost:11434\",  # Default Ollama URL\n    temperature=0.7,\n)\n\n# Generate response\nresponse = await llm.generate(\n    prompt=\"What is machine learning?\",\n)\nprint(response)\n\n# With system message\nresponse = await llm.generate(\n    prompt=\"Explain neural networks\",\n    system_message=\"You are a helpful AI teacher.\"\n)\n</code></pre> <p>Model Comparison:</p> Model Size Speed Quality Context Use Case <code>llama3</code> 8B Medium Excellent 8k General purpose <code>mistral</code> 7B Fast Very good 32k Long context <code>phi3</code> 3.8B Very fast Good 4k Speed critical <code>gemma</code> 7B Medium Very good 8k Balanced"},{"location":"guides/llms/#configuration","title":"Configuration","text":""},{"location":"guides/llms/#temperature","title":"Temperature","text":"<p>Control randomness in responses:</p> Python<pre><code># Deterministic (factual responses)\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    temperature=0.0,  # No randomness\n)\n\n# Balanced (default)\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    temperature=0.7,  # Some creativity\n)\n\n# Creative\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    temperature=1.0,  # Maximum creativity\n)\n</code></pre>"},{"location":"guides/llms/#max-tokens","title":"Max Tokens","text":"<p>Limit response length:</p> Python<pre><code>llm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    max_tokens=500,  # Maximum 500 tokens in response\n)\n\n# For summaries\nllm_summary = OpenAILLM(model=\"gpt-4-turbo\", max_tokens=200)\n\n# For detailed explanations\nllm_detailed = OpenAILLM(model=\"gpt-4-turbo\", max_tokens=2000)\n</code></pre>"},{"location":"guides/llms/#timeout","title":"Timeout","text":"<p>Set request timeouts:</p> Python<pre><code>llm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    timeout=60.0,  # 60 seconds (default: 120)\n)\n</code></pre>"},{"location":"guides/llms/#retry-logic","title":"Retry Logic","text":"<p>Handle failures gracefully:</p> Python<pre><code>llm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    max_retries=3,  # Retry failed requests\n    retry_delay=1.0,  # Wait 1 second between retries\n)\n</code></pre>"},{"location":"guides/llms/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/llms/#streaming-responses","title":"Streaming Responses","text":"<p>Stream responses for real-time display:</p> Python<pre><code># Stream response tokens as they're generated\nasync for chunk in llm.generate_stream(\n    prompt=\"Explain quantum mechanics in detail\",\n):\n    print(chunk, end=\"\", flush=True)\nprint()  # New line after streaming\n</code></pre>"},{"location":"guides/llms/#chat-history","title":"Chat History","text":"<p>Maintain conversation context:</p> Python<pre><code>from rag_toolkit.core.llm import Message\n\n# Build chat history\nmessages = [\n    Message(role=\"system\", content=\"You are a helpful assistant.\"),\n    Message(role=\"user\", content=\"What is Python?\"),\n    Message(role=\"assistant\", content=\"Python is a programming language.\"),\n    Message(role=\"user\", content=\"Give me an example.\"),\n]\n\n# Generate with history\nresponse = await llm.generate_with_history(messages=messages)\nprint(response)\n</code></pre>"},{"location":"guides/llms/#function-calling","title":"Function Calling","text":"<p>Use structured outputs (OpenAI only):</p> Python<pre><code># Define function\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search_documents\",\n            \"description\": \"Search for relevant documents\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"Search query\"\n                    },\n                    \"limit\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Number of results\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }\n]\n\n# Generate with function calling\nresponse = await llm.generate(\n    prompt=\"Find documents about machine learning\",\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\n# Check if function was called\nif response.tool_calls:\n    for tool_call in response.tool_calls:\n        print(f\"Function: {tool_call.function.name}\")\n        print(f\"Arguments: {tool_call.function.arguments}\")\n</code></pre>"},{"location":"guides/llms/#json-mode","title":"JSON Mode","text":"<p>Force structured JSON output (OpenAI only):</p> Python<pre><code>llm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    response_format={\"type\": \"json_object\"}\n)\n\nresponse = await llm.generate(\n    prompt=\"\"\"\n    Extract information from this text as JSON:\n    \"John Smith is 30 years old and works as a software engineer in San Francisco.\"\n\n    Return JSON with fields: name, age, occupation, location\n    \"\"\"\n)\n\nimport json\ndata = json.loads(response)\nprint(data)  # {\"name\": \"John Smith\", \"age\": 30, ...}\n</code></pre>"},{"location":"guides/llms/#integration-with-rag","title":"Integration with RAG","text":""},{"location":"guides/llms/#basic-rag-query","title":"Basic RAG Query","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\nfrom rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\nfrom rag_toolkit.infra.llm import OpenAILLM\n\n# Setup LLM\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    temperature=0.7,\n)\n\n# Create RAG pipeline\npipeline = RagPipeline(\n    embedding_client=OpenAIEmbedding(),\n    vector_store=MilvusVectorStore(\n        collection_name=\"documents\",\n        embedding_client=OpenAIEmbedding(),\n    ),\n    llm_client=llm,\n)\n\n# Query with automatic context retrieval\nresult = await pipeline.query(\n    \"What are the key findings in the research papers?\"\n)\n\nprint(f\"Answer: {result.answer}\")\nprint(f\"Sources: {len(result.sources)} documents used\")\n</code></pre>"},{"location":"guides/llms/#custom-prompts","title":"Custom Prompts","text":"<p>Customize how context is presented to the LLM:</p> Python<pre><code># Custom prompt template\ncustom_prompt = \"\"\"\nYou are a research assistant analyzing scientific papers.\n\nContext from papers:\n{context}\n\nQuestion: {question}\n\nProvide a detailed answer with citations.\n\"\"\"\n\npipeline = RagPipeline(\n    llm_client=llm,\n    prompt_template=custom_prompt,\n    # ... other config\n)\n</code></pre>"},{"location":"guides/llms/#multi-step-reasoning","title":"Multi-Step Reasoning","text":"<p>Break complex queries into steps:</p> Python<pre><code># Step 1: Decompose query\ndecomposition_prompt = \"\"\"\nBreak this complex question into simpler sub-questions:\n{question}\n\"\"\"\n\nsub_questions = await llm.generate(\n    prompt=decomposition_prompt.format(question=query)\n)\n\n# Step 2: Answer each sub-question\nanswers = []\nfor sub_q in sub_questions:\n    answer = await pipeline.query(sub_q)\n    answers.append(answer)\n\n# Step 3: Synthesize final answer\nsynthesis_prompt = \"\"\"\nSynthesize a final answer from these sub-answers:\n{answers}\n\nOriginal question: {question}\n\"\"\"\n\nfinal_answer = await llm.generate(\n    prompt=synthesis_prompt.format(\n        answers=\"\\n\\n\".join(answers),\n        question=query\n    )\n)\n</code></pre>"},{"location":"guides/llms/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"guides/llms/#by-quality","title":"By Quality","text":"<p>Best Quality (OpenAI): </p>Python<pre><code>llm = OpenAILLM(model=\"gpt-4-turbo\")\n# 128k context, best reasoning\n# Use for: Complex tasks, production systems\n</code></pre><p></p> <p>Good Quality (Ollama, Free): </p>Python<pre><code>llm = OllamaLLM(model=\"llama3\")\n# 8k context, excellent for most tasks\n# Use for: General purpose, privacy-sensitive\n</code></pre><p></p>"},{"location":"guides/llms/#by-speed","title":"By Speed","text":"<p>Fastest (Ollama, Local): </p>Python<pre><code>llm = OllamaLLM(model=\"phi3\")\n# Very fast, good quality\n# Use for: Real-time applications\n</code></pre><p></p> <p>Fast (OpenAI): </p>Python<pre><code>llm = OpenAILLM(model=\"gpt-3.5-turbo\")\n# Fast API, good quality\n# Use for: Most applications\n</code></pre><p></p>"},{"location":"guides/llms/#by-cost","title":"By Cost","text":"<p>Free (Ollama): </p>Python<pre><code>llm = OllamaLLM(model=\"llama3\")\n# Zero API costs\n# Cost: GPU/CPU time only\n</code></pre><p></p> <p>Cost-Effective (OpenAI): </p>Python<pre><code>llm = OpenAILLM(model=\"gpt-3.5-turbo\")\n# $0.50 / 1M input tokens\n# Use for: Budget-conscious applications\n</code></pre><p></p>"},{"location":"guides/llms/#by-context-window","title":"By Context Window","text":"<p>Longest Context: </p>Python<pre><code># OpenAI GPT-4-turbo: 128k tokens\nllm = OpenAILLM(model=\"gpt-4-turbo\")\n\n# Ollama Mistral: 32k tokens\nllm = OllamaLLM(model=\"mistral\")\n</code></pre><p></p>"},{"location":"guides/llms/#custom-llm-clients","title":"Custom LLM Clients","text":"<p>Implement your own LLM provider following the protocol:</p> Python<pre><code>from typing import Protocol, runtime_checkable, AsyncIterator\n\n@runtime_checkable\nclass LLMClient(Protocol):\n    \"\"\"Protocol for LLM clients.\"\"\"\n\n    async def generate(\n        self,\n        prompt: str,\n        system_message: str | None = None,\n        temperature: float | None = None,\n        max_tokens: int | None = None,\n    ) -&gt; str:\n        \"\"\"Generate text completion.\"\"\"\n        ...\n\n    async def generate_stream(\n        self,\n        prompt: str,\n        system_message: str | None = None,\n        temperature: float | None = None,\n        max_tokens: int | None = None,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Generate text completion with streaming.\"\"\"\n        ...\n</code></pre>"},{"location":"guides/llms/#example-anthropic-claude","title":"Example: Anthropic Claude","text":"Python<pre><code>from anthropic import AsyncAnthropic\n\nclass AnthropicLLM:\n    \"\"\"Anthropic Claude LLM client.\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"claude-3-opus-20240229\",\n        api_key: str | None = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n    ):\n        self.model = model\n        self.client = AsyncAnthropic(api_key=api_key)\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\n    async def generate(\n        self,\n        prompt: str,\n        system_message: str | None = None,\n        temperature: float | None = None,\n        max_tokens: int | None = None,\n    ) -&gt; str:\n        \"\"\"Generate completion.\"\"\"\n        response = await self.client.messages.create(\n            model=self.model,\n            max_tokens=max_tokens or self.max_tokens,\n            temperature=temperature or self.temperature,\n            system=system_message or \"\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        return response.content[0].text\n\n    async def generate_stream(\n        self,\n        prompt: str,\n        system_message: str | None = None,\n        temperature: float | None = None,\n        max_tokens: int | None = None,\n    ):\n        \"\"\"Generate with streaming.\"\"\"\n        async with self.client.messages.stream(\n            model=self.model,\n            max_tokens=max_tokens or self.max_tokens,\n            temperature=temperature or self.temperature,\n            system=system_message or \"\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        ) as stream:\n            async for chunk in stream.text_stream:\n                yield chunk\n\n# Usage\nllm = AnthropicLLM()\nresponse = await llm.generate(\"Explain quantum computing\")\n</code></pre>"},{"location":"guides/llms/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/llms/#caching-responses","title":"Caching Responses","text":"<p>Cache common queries:</p> Python<pre><code>from functools import lru_cache\n\nclass CachedLLM:\n    \"\"\"LLM client with response caching.\"\"\"\n\n    def __init__(self, llm_client):\n        self.client = llm_client\n        self._generate_cached = lru_cache(maxsize=1000)(\n            self._generate_sync\n        )\n\n    def _generate_sync(self, prompt: str) -&gt; str:\n        \"\"\"Synchronous wrapper for caching.\"\"\"\n        import asyncio\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(\n            self.client.generate(prompt)\n        )\n\n    async def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate with caching.\"\"\"\n        # Only cache without extra parameters\n        if not kwargs:\n            return self._generate_cached(prompt)\n        return await self.client.generate(prompt, **kwargs)\n\n# Usage\nbase_llm = OpenAILLM(model=\"gpt-4-turbo\")\ncached_llm = CachedLLM(base_llm)\n\n# First call: API request\nr1 = await cached_llm.generate(\"What is AI?\")  # API call\n\n# Second call: from cache\nr2 = await cached_llm.generate(\"What is AI?\")  # No API call\n</code></pre>"},{"location":"guides/llms/#batch-processing","title":"Batch Processing","text":"<p>Process multiple prompts efficiently:</p> Python<pre><code>import asyncio\n\nasync def batch_generate(\n    prompts: list[str],\n    llm_client,\n    max_concurrent: int = 5\n):\n    \"\"\"Generate responses in parallel with concurrency limit.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def generate_with_limit(prompt: str):\n        async with semaphore:\n            return await llm_client.generate(prompt)\n\n    tasks = [generate_with_limit(prompt) for prompt in prompts]\n    return await asyncio.gather(*tasks)\n\n# Usage\nprompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks\",\n    \"Describe gradient descent\"\n]\n\nresponses = await batch_generate(\n    prompts=prompts,\n    llm_client=llm,\n    max_concurrent=10\n)\n</code></pre>"},{"location":"guides/llms/#token-estimation","title":"Token Estimation","text":"<p>Estimate costs before calling API:</p> Python<pre><code>import tiktoken\n\ndef estimate_tokens(text: str, model: str = \"gpt-4\") -&gt; int:\n    \"\"\"Estimate token count for text.\"\"\"\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\ndef estimate_cost(\n    prompt: str,\n    expected_response_length: int,\n    model: str = \"gpt-4-turbo\"\n) -&gt; float:\n    \"\"\"Estimate API cost in USD.\"\"\"\n    # Pricing per 1M tokens\n    prices = {\n        \"gpt-4-turbo\": {\"input\": 10.0, \"output\": 30.0},\n        \"gpt-4\": {\"input\": 30.0, \"output\": 60.0},\n        \"gpt-3.5-turbo\": {\"input\": 0.5, \"output\": 1.5},\n    }\n\n    input_tokens = estimate_tokens(prompt, model)\n    output_tokens = expected_response_length\n\n    price = prices.get(model, prices[\"gpt-4-turbo\"])\n    input_cost = (input_tokens / 1_000_000) * price[\"input\"]\n    output_cost = (output_tokens / 1_000_000) * price[\"output\"]\n\n    return input_cost + output_cost\n\n# Usage\nprompt = \"Explain quantum computing in detail...\"\ncost = estimate_cost(prompt, expected_response_length=500)\nprint(f\"Estimated cost: ${cost:.4f}\")\n</code></pre>"},{"location":"guides/llms/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"guides/llms/#response-tracking","title":"Response Tracking","text":"<p>Track API usage and costs:</p> Python<pre><code>class TrackedLLM:\n    \"\"\"LLM client with usage tracking.\"\"\"\n\n    def __init__(self, llm_client):\n        self.client = llm_client\n        self.total_calls = 0\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n\n    async def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate with tracking.\"\"\"\n        self.total_calls += 1\n\n        # Estimate input tokens\n        self.total_input_tokens += len(prompt) // 4\n\n        response = await self.client.generate(prompt, **kwargs)\n\n        # Estimate output tokens\n        self.total_output_tokens += len(response) // 4\n\n        return response\n\n    def get_stats(self) -&gt; dict:\n        \"\"\"Get usage statistics.\"\"\"\n        return {\n            \"total_calls\": self.total_calls,\n            \"input_tokens\": self.total_input_tokens,\n            \"output_tokens\": self.total_output_tokens,\n            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n        }\n\n    def estimate_cost(self, model: str = \"gpt-4-turbo\") -&gt; float:\n        \"\"\"Estimate total cost.\"\"\"\n        prices = {\n            \"gpt-4-turbo\": {\"input\": 10.0, \"output\": 30.0},\n        }\n        price = prices.get(model, prices[\"gpt-4-turbo\"])\n\n        input_cost = (self.total_input_tokens / 1_000_000) * price[\"input\"]\n        output_cost = (self.total_output_tokens / 1_000_000) * price[\"output\"]\n\n        return input_cost + output_cost\n\n# Usage\ntracked = TrackedLLM(OpenAILLM())\nawait tracked.generate(\"What is AI?\")\nawait tracked.generate(\"Explain ML\")\n\nstats = tracked.get_stats()\nprint(f\"Total calls: {stats['total_calls']}\")\nprint(f\"Total tokens: {stats['total_tokens']:,}\")\nprint(f\"Estimated cost: ${tracked.estimate_cost():.4f}\")\n</code></pre>"},{"location":"guides/llms/#response-quality-check","title":"Response Quality Check","text":"<p>Validate response quality:</p> Python<pre><code>async def generate_with_validation(\n    prompt: str,\n    llm_client,\n    max_retries: int = 3\n) -&gt; str:\n    \"\"\"Generate with quality validation.\"\"\"\n    for attempt in range(max_retries):\n        response = await llm_client.generate(prompt)\n\n        # Validation checks\n        if len(response) &lt; 10:\n            print(f\"Attempt {attempt + 1}: Response too short, retrying...\")\n            continue\n\n        if \"I don't know\" in response and attempt &lt; max_retries - 1:\n            print(f\"Attempt {attempt + 1}: Uncertain response, retrying...\")\n            continue\n\n        return response\n\n    raise ValueError(\"Failed to generate valid response\")\n\n# Usage\nresponse = await generate_with_validation(\n    \"What is quantum computing?\",\n    llm_client=llm\n)\n</code></pre>"},{"location":"guides/llms/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/llms/#api-key-issues","title":"API Key Issues","text":"Python<pre><code>from rag_toolkit.infra.llm import OpenAILLM\n\ntry:\n    llm = OpenAILLM()\n    await llm.generate(\"test\")\nexcept Exception as e:\n    if \"api_key\" in str(e).lower():\n        print(\"\u274c Invalid or missing API key\")\n        print(\"Set OPENAI_API_KEY environment variable\")\n</code></pre>"},{"location":"guides/llms/#rate-limiting","title":"Rate Limiting","text":"Python<pre><code>from tenacity import retry, wait_exponential, stop_after_attempt\n\n@retry(\n    wait=wait_exponential(min=1, max=60),\n    stop=stop_after_attempt(5)\n)\nasync def generate_with_retry(prompt: str, llm_client):\n    \"\"\"Generate with exponential backoff for rate limits.\"\"\"\n    try:\n        return await llm_client.generate(prompt)\n    except Exception as e:\n        if \"rate limit\" in str(e).lower():\n            print(\"Rate limited, backing off...\")\n            raise\n        raise\n\n# Usage\nresponse = await generate_with_retry(\"What is AI?\", llm)\n</code></pre>"},{"location":"guides/llms/#context-length-errors","title":"Context Length Errors","text":"Python<pre><code>def truncate_to_context_limit(\n    text: str,\n    max_tokens: int = 8000,\n    model: str = \"gpt-4\"\n) -&gt; str:\n    \"\"\"Truncate text to fit context window.\"\"\"\n    import tiktoken\n\n    encoding = tiktoken.encoding_for_model(model)\n    tokens = encoding.encode(text)\n\n    if len(tokens) &lt;= max_tokens:\n        return text\n\n    # Truncate and decode\n    truncated_tokens = tokens[:max_tokens]\n    return encoding.decode(truncated_tokens)\n\n# Usage\nlong_prompt = \"...\" * 10000  # Very long prompt\nsafe_prompt = truncate_to_context_limit(long_prompt, max_tokens=7000)\nresponse = await llm.generate(safe_prompt)\n</code></pre>"},{"location":"guides/llms/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Model</li> <li>Production: <code>gpt-4-turbo</code> (best quality)</li> <li>Cost-effective: <code>gpt-3.5-turbo</code></li> <li> <p>Privacy/offline: <code>llama3</code> (Ollama)</p> </li> <li> <p>Prompt Engineering</p> </li> <li>Be specific and clear</li> <li>Provide examples (few-shot)</li> <li>Use system messages for role/context</li> <li> <p>Structure with clear sections</p> </li> <li> <p>Error Handling</p> </li> <li>Implement retries with exponential backoff</li> <li>Handle rate limits gracefully</li> <li>Validate response quality</li> <li> <p>Have fallback strategies</p> </li> <li> <p>Cost Optimization</p> </li> <li>Cache common queries</li> <li>Use cheaper models when appropriate</li> <li>Estimate costs before making calls</li> <li> <p>Monitor usage regularly</p> </li> <li> <p>Performance</p> </li> <li>Use streaming for better UX</li> <li>Batch requests when possible</li> <li>Set appropriate timeouts</li> <li> <p>Implement concurrency limits</p> </li> <li> <p>Quality Assurance</p> </li> <li>Use temperature=0 for consistency</li> <li>Validate responses</li> <li>A/B test different models</li> <li>Monitor response quality</li> </ol>"},{"location":"guides/llms/#next-steps","title":"Next Steps","text":"<ul> <li>RAG Pipeline - Build complete RAG systems</li> <li>Embeddings Guide - Learn about embeddings</li> <li>Advanced Pipeline Example</li> <li>Production Setup</li> </ul>"},{"location":"guides/llms/#see-also","title":"See Also","text":"<ul> <li>OpenAI API Documentation</li> <li>Ollama Documentation</li> <li>LLM Protocol</li> </ul>"},{"location":"guides/protocols/","title":"Protocols","text":""},{"location":"guides/protocols/#protocols-guide","title":"Protocols Guide","text":"<p>RAG Toolkit uses Python Protocols for maximum flexibility and type safety. This guide explains protocols and how to leverage them effectively.</p>"},{"location":"guides/protocols/#what-are-protocols","title":"What Are Protocols?","text":"<p>Structural Typing (Duck Typing)</p> <p>Protocols define interfaces using structural typing rather than inheritance\u2014if it walks like a duck and quacks like a duck, it's a duck!</p>"},{"location":"guides/protocols/#traditional-approach","title":"Traditional Approach","text":"<p>Inheritance-Based</p> Python<pre><code>from abc import ABC, abstractmethod\n\nclass VectorStore(ABC):\n    @abstractmethod\n    def search(self, query: list[float]) -&gt; list:\n        pass\n\nclass MyStore(VectorStore):  # Must inherit \u274c\n    def search(self, query: list[float]) -&gt; list:\n        return []\n</code></pre> <p>Problems: - Requires inheritance - Tight coupling - Difficult to mock - Limited flexibility</p>"},{"location":"guides/protocols/#protocol-approach","title":"Protocol Approach","text":"<p>Structural Typing</p> Python<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass VectorStoreClient(Protocol):\n    def search(self, query: list[float]) -&gt; list: ...\n\nclass MyStore:  # No inheritance! \u2713\n    def search(self, query: list[float]) -&gt; list:\n        return []\n\n# Works! MyStore matches the protocol structure\nstore: VectorStoreClient = MyStore()\n</code></pre> <p>Benefits: -  No inheritance required -  Duck typing with type safety -  Easy mocking for tests -  Flexible implementations</p> <p>Runtime Checking</p> <p>The <code>@runtime_checkable</code> decorator enables <code>isinstance()</code> checks: </p>Python<pre><code>assert isinstance(MyStore(), VectorStoreClient)  # \u2713 True\n</code></pre><p></p>"},{"location":"guides/protocols/#core-protocols","title":"Core Protocols","text":"<p>The Three Pillars</p> <p>RAG Toolkit defines three core protocols that form the foundation of the system.</p>"},{"location":"guides/protocols/#embeddingclient","title":"EmbeddingClient","text":"<p>Text-to-Vector Transformation</p> core/embedding/base.py<pre><code>@runtime_checkable\nclass EmbeddingClient(Protocol):\n    \"\"\"Protocol for text embedding models.\"\"\"\n\n    def embed(self, text: str) -&gt; list[float]:\n        \"\"\"\n        Embed a single text into a vector.\n\n        Args:\n            text: Input text to embed\n\n        Returns:\n            Vector representation as list of floats\n\n        Example:\n            &gt;&gt;&gt; embedding = OllamaEmbedding()\n            &gt;&gt;&gt; vector = embedding.embed(\"Hello world\")\n            &gt;&gt;&gt; len(vector)\n            768\n        \"\"\"\n        ...\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"\n        Embed multiple texts efficiently.\n\n        Args:\n            texts: List of input texts\n\n        Returns:\n            List of vectors\n\n        Example:\n            &gt;&gt;&gt; texts = [\"Hello\", \"World\"]\n            &gt;&gt;&gt; vectors = embedding.embed_batch(texts)\n            &gt;&gt;&gt; len(vectors)\n            2\n        \"\"\"\n        ...\n</code></pre> <p>Implementation Example</p> Python<pre><code>class MyEmbedding:\n    def embed(self, text: str) -&gt; list[float]:\n        # Your embedding logic\n        return [0.1, 0.2, 0.3, ...]\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        # Batch processing\n        return [self.embed(t) for t in texts]\n\n# Works with RAG Pipeline! \u2728\npipeline = RagPipeline(\n    embedding_client=MyEmbedding(),\n    llm_client=llm,\n    vector_store=store,\n)\n</code></pre>"},{"location":"guides/protocols/#llmclient","title":"LLMClient","text":"Python<pre><code>@runtime_checkable\nclass LLMClient(Protocol):\n    \"\"\"Protocol for language model clients.\"\"\"\n\n    def generate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"\n        Generate text from a prompt.\n\n        Args:\n            prompt: Input prompt\n            max_tokens: Maximum tokens to generate\n            temperature: Sampling temperature (0.0 = deterministic, 2.0 = creative)\n            **kwargs: Provider-specific options\n\n        Returns:\n            Generated text\n\n        Example:\n            &gt;&gt;&gt; llm = OllamaLLMClient()\n            &gt;&gt;&gt; response = llm.generate(\"Explain RAG in one sentence\")\n            &gt;&gt;&gt; print(response)\n            \"RAG combines retrieval and generation...\"\n        \"\"\"\n        ...\n\n    async def agenerate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Async version of generate.\"\"\"\n        ...\n</code></pre>"},{"location":"guides/protocols/#vectorstoreclient","title":"VectorStoreClient","text":"Python<pre><code>@runtime_checkable\nclass VectorStoreClient(Protocol):\n    \"\"\"Protocol for vector store operations.\"\"\"\n\n    def create_collection(\n        self,\n        name: str,\n        dimension: int,\n        metric: str = \"IP\",\n        **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Create a new collection for storing vectors.\n\n        Args:\n            name: Collection name\n            dimension: Vector dimension\n            metric: Distance metric (\"IP\", \"L2\", \"COSINE\")\n            **kwargs: Store-specific options\n\n        Example:\n            &gt;&gt;&gt; store.create_collection(\"docs\", dimension=768)\n        \"\"\"\n        ...\n\n    def insert(\n        self,\n        collection_name: str,\n        vectors: list[list[float]],\n        texts: list[str],\n        metadata: list[dict],\n        **kwargs\n    ) -&gt; list[str]:\n        \"\"\"\n        Insert vectors into collection.\n\n        Args:\n            collection_name: Target collection\n            vectors: List of vector embeddings\n            texts: Original texts\n            metadata: Associated metadata\n            **kwargs: Store-specific options\n\n        Returns:\n            List of IDs for inserted vectors\n\n        Example:\n            &gt;&gt;&gt; ids = store.insert(\n            ...     \"docs\",\n            ...     vectors=[[0.1, ...], [0.2, ...]],\n            ...     texts=[\"text1\", \"text2\"],\n            ...     metadata=[{\"source\": \"doc1\"}, {\"source\": \"doc2\"}]\n            ... )\n        \"\"\"\n        ...\n\n    def search(\n        self,\n        collection_name: str,\n        query_vector: list[float],\n        top_k: int = 5,\n        filters: dict | None = None,\n        **kwargs\n    ) -&gt; list[SearchResult]:\n        \"\"\"\n        Search for similar vectors.\n\n        Args:\n            collection_name: Collection to search\n            query_vector: Query embedding\n            top_k: Number of results to return\n            filters: Metadata filters\n            **kwargs: Store-specific options\n\n        Returns:\n            List of SearchResult objects sorted by relevance\n\n        Example:\n            &gt;&gt;&gt; results = store.search(\n            ...     \"docs\",\n            ...     query_vector=[0.1, ...],\n            ...     top_k=5,\n            ...     filters={\"source\": \"manual\"}\n            ... )\n        \"\"\"\n        ...\n\n    def hybrid_search(\n        self,\n        collection_name: str,\n        query_vector: list[float],\n        query_text: str,\n        top_k: int = 5,\n        alpha: float = 0.5,\n        **kwargs\n    ) -&gt; list[SearchResult]:\n        \"\"\"\n        Hybrid search combining vector and keyword search.\n\n        Args:\n            collection_name: Collection to search\n            query_vector: Query embedding\n            query_text: Query text for keyword search\n            top_k: Number of results\n            alpha: Weight between vector (0.0) and keyword (1.0) search\n            **kwargs: Store-specific options\n\n        Returns:\n            List of SearchResult objects\n        \"\"\"\n        ...\n\n    def delete(self, collection_name: str, ids: list[str]) -&gt; None:\n        \"\"\"Delete vectors by ID.\"\"\"\n        ...\n\n    def get_stats(self, collection_name: str) -&gt; dict:\n        \"\"\"Get collection statistics.\"\"\"\n        ...\n</code></pre>"},{"location":"guides/protocols/#implementing-custom-protocols","title":"Implementing Custom Protocols","text":""},{"location":"guides/protocols/#example-custom-embedding-provider","title":"Example: Custom Embedding Provider","text":"Python<pre><code>class HuggingFaceEmbedding:\n    \"\"\"Custom embedding using HuggingFace models.\"\"\"\n\n    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n        from sentence_transformers import SentenceTransformer\n        self.model = SentenceTransformer(model_name)\n\n    def embed(self, text: str) -&gt; list[float]:\n        \"\"\"Implements EmbeddingClient.embed\"\"\"\n        embedding = self.model.encode(text)\n        return embedding.tolist()\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Implements EmbeddingClient.embed_batch\"\"\"\n        embeddings = self.model.encode(texts)\n        return embeddings.tolist()\n\n# Use it with RagPipeline - no inheritance needed!\nembedding = HuggingFaceEmbedding()\npipeline = RagPipeline(\n    embedding_client=embedding,  # \u2705 Works!\n    llm_client=llm,\n    vector_store=store,\n)\n</code></pre>"},{"location":"guides/protocols/#example-custom-vector-store","title":"Example: Custom Vector Store","text":"Python<pre><code>import chromadb\nfrom rag_toolkit.core.types import SearchResult\n\nclass ChromaVectorStore:\n    \"\"\"Custom vector store using ChromaDB.\"\"\"\n\n    def __init__(self, path: str = \"./chroma_db\"):\n        self.client = chromadb.PersistentClient(path=path)\n        self.collections = {}\n\n    def create_collection(\n        self,\n        name: str,\n        dimension: int,\n        metric: str = \"IP\",\n        **kwargs\n    ) -&gt; None:\n        \"\"\"Implements VectorStoreClient.create_collection\"\"\"\n        self.collections[name] = self.client.create_collection(\n            name=name,\n            metadata={\"dimension\": dimension, \"metric\": metric}\n        )\n\n    def insert(\n        self,\n        collection_name: str,\n        vectors: list[list[float]],\n        texts: list[str],\n        metadata: list[dict],\n        **kwargs\n    ) -&gt; list[str]:\n        \"\"\"Implements VectorStoreClient.insert\"\"\"\n        collection = self.collections[collection_name]\n        ids = [f\"{collection_name}_{i}\" for i in range(len(vectors))]\n\n        collection.add(\n            embeddings=vectors,\n            documents=texts,\n            metadatas=metadata,\n            ids=ids,\n        )\n        return ids\n\n    def search(\n        self,\n        collection_name: str,\n        query_vector: list[float],\n        top_k: int = 5,\n        filters: dict | None = None,\n        **kwargs\n    ) -&gt; list[SearchResult]:\n        \"\"\"Implements VectorStoreClient.search\"\"\"\n        collection = self.collections[collection_name]\n\n        results = collection.query(\n            query_embeddings=[query_vector],\n            n_results=top_k,\n            where=filters,\n        )\n\n        # Convert to SearchResult objects\n        search_results = []\n        for i in range(len(results['ids'][0])):\n            search_results.append(\n                SearchResult(\n                    id=results['ids'][0][i],\n                    score=results['distances'][0][i],\n                    text=results['documents'][0][i],\n                    metadata=results['metadatas'][0][i],\n                )\n            )\n\n        return search_results\n\n    def hybrid_search(self, *args, **kwargs) -&gt; list[SearchResult]:\n        \"\"\"Optional: implement hybrid search\"\"\"\n        raise NotImplementedError(\"ChromaDB doesn't support hybrid search\")\n\n    def delete(self, collection_name: str, ids: list[str]) -&gt; None:\n        \"\"\"Implements VectorStoreClient.delete\"\"\"\n        collection = self.collections[collection_name]\n        collection.delete(ids=ids)\n\n    def get_stats(self, collection_name: str) -&gt; dict:\n        \"\"\"Implements VectorStoreClient.get_stats\"\"\"\n        collection = self.collections[collection_name]\n        return {\n            \"count\": collection.count(),\n            \"name\": collection_name,\n        }\n\n# Use with RagPipeline seamlessly!\nstore = ChromaVectorStore()\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,\n    vector_store=store,  # \u2705 Works perfectly!\n)\n</code></pre>"},{"location":"guides/protocols/#example-custom-llm-client","title":"Example: Custom LLM Client","text":"Python<pre><code>class AnthropicLLMClient:\n    \"\"\"Custom LLM client for Anthropic Claude.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"claude-3-sonnet\"):\n        import anthropic\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self.model = model\n\n    def generate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Implements LLMClient.generate\"\"\"\n        response = self.client.messages.create(\n            model=self.model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.content[0].text\n\n    async def agenerate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Implements LLMClient.agenerate\"\"\"\n        # Anthropic has async client\n        async_client = anthropic.AsyncAnthropic(api_key=self.api_key)\n        response = await async_client.messages.create(\n            model=self.model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.content[0].text\n\n# Works with RagPipeline!\nllm = AnthropicLLMClient(api_key=\"your-key\")\npipeline = RagPipeline(\n    embedding_client=embedding,\n    llm_client=llm,  # \u2705 Anthropic support!\n    vector_store=store,\n)\n</code></pre>"},{"location":"guides/protocols/#runtime-type-checking","title":"Runtime Type Checking","text":"<p>Use <code>@runtime_checkable</code> for runtime validation:</p> Python<pre><code>from typing import runtime_checkable\n\n@runtime_checkable\nclass EmbeddingClient(Protocol):\n    def embed(self, text: str) -&gt; list[float]: ...\n\n# Check if object implements protocol\nclass MyEmbedding:\n    def embed(self, text: str) -&gt; list[float]:\n        return [0.0] * 768\n\nembedding = MyEmbedding()\nprint(isinstance(embedding, EmbeddingClient))  # True \u2705\n\n# Missing method\nclass BadEmbedding:\n    pass\n\nbad = BadEmbedding()\nprint(isinstance(bad, EmbeddingClient))  # False \u274c\n</code></pre>"},{"location":"guides/protocols/#testing-with-protocols","title":"Testing with Protocols","text":"<p>Protocols make testing incredibly easy:</p> Python<pre><code>import pytest\nfrom rag_toolkit import RagPipeline\n\nclass MockEmbedding:\n    \"\"\"Simple mock - no complex setup needed!\"\"\"\n    def embed(self, text: str) -&gt; list[float]:\n        return [0.0] * 768\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        return [[0.0] * 768 for _ in texts]\n\nclass MockLLM:\n    \"\"\"Mock LLM that returns predictable responses.\"\"\"\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        return \"Mock response\"\n\n    async def agenerate(self, prompt: str, **kwargs) -&gt; str:\n        return \"Mock async response\"\n\nclass MockVectorStore:\n    \"\"\"Mock vector store for testing.\"\"\"\n    def create_collection(self, name, dimension, **kwargs): pass\n    def insert(self, *args, **kwargs): return [\"id1\", \"id2\"]\n    def search(self, *args, **kwargs): \n        return [\n            SearchResult(id=\"1\", score=0.9, text=\"result 1\", metadata={}),\n            SearchResult(id=\"2\", score=0.8, text=\"result 2\", metadata={}),\n        ]\n    def hybrid_search(self, *args, **kwargs): return []\n    def delete(self, *args, **kwargs): pass\n    def get_stats(self, *args, **kwargs): return {}\n\ndef test_rag_pipeline():\n    \"\"\"Test pipeline with mocks - super easy!\"\"\"\n    pipeline = RagPipeline(\n        embedding_client=MockEmbedding(),\n        llm_client=MockLLM(),\n        vector_store=MockVectorStore(),\n    )\n\n    response = pipeline.query(\"test query\")\n    assert response.answer == \"Mock response\"\n</code></pre>"},{"location":"guides/protocols/#best-practices","title":"Best Practices","text":""},{"location":"guides/protocols/#1-keep-protocols-minimal","title":"1. Keep Protocols Minimal","text":"Python<pre><code># \u2705 Good: Minimal required interface\nclass EmbeddingClient(Protocol):\n    def embed(self, text: str) -&gt; list[float]: ...\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]: ...\n\n# \u274c Bad: Too many requirements\nclass EmbeddingClient(Protocol):\n    def embed(self, text: str) -&gt; list[float]: ...\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]: ...\n    def get_model_name(self) -&gt; str: ...\n    def get_dimension(self) -&gt; int: ...\n    def fine_tune(self, data): ...  # Too specific!\n</code></pre>"},{"location":"guides/protocols/#2-use-type-hints","title":"2. Use Type Hints","text":"Python<pre><code># \u2705 Good: Clear type hints\ndef embed(self, text: str) -&gt; list[float]: ...\n\n# \u274c Bad: No type hints\ndef embed(self, text): ...\n</code></pre>"},{"location":"guides/protocols/#3-document-expected-behavior","title":"3. Document Expected Behavior","text":"Python<pre><code>class VectorStoreClient(Protocol):\n    def search(\n        self,\n        collection_name: str,\n        query_vector: list[float],\n        top_k: int = 5,\n    ) -&gt; list[SearchResult]:\n        \"\"\"\n        Search for similar vectors.\n\n        Expected behavior:\n        - Results should be sorted by relevance (highest first)\n        - Should return at most `top_k` results\n        - Empty list if no results found\n        - Raise ValueError if collection doesn't exist\n        \"\"\"\n        ...\n</code></pre>"},{"location":"guides/protocols/#4-provide-default-implementations","title":"4. Provide Default Implementations","text":"Python<pre><code># Provide base classes for common use cases\nclass BaseEmbedding:\n    \"\"\"Optional base class with common functionality.\"\"\"\n\n    def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Default batch implementation using single embed.\"\"\"\n        return [self.embed(text) for text in texts]\n\n# Users can inherit if useful, but don't have to!\nclass MyEmbedding(BaseEmbedding):  # Optional!\n    def embed(self, text: str) -&gt; list[float]:\n        # Only implement embed, get embed_batch for free\n        return [0.0] * 768\n</code></pre>"},{"location":"guides/protocols/#protocol-composition","title":"Protocol Composition","text":"<p>Compose protocols for complex interfaces:</p> Python<pre><code>class Searchable(Protocol):\n    def search(self, query: str) -&gt; list: ...\n\nclass Indexable(Protocol):\n    def index(self, documents: list[str]) -&gt; None: ...\n\nclass VectorStore(Searchable, Indexable, Protocol):\n    \"\"\"Combines both interfaces.\"\"\"\n    pass\n</code></pre>"},{"location":"guides/protocols/#next-steps","title":"Next Steps","text":"<ul> <li>See Vector Stores for implementation examples</li> <li>Learn about Embeddings</li> <li>Explore LLMs</li> <li>Read API Reference for complete protocol definitions</li> </ul>"},{"location":"guides/rag_pipeline/","title":"RAG Pipeline","text":""},{"location":"guides/rag_pipeline/#material-pipeline-rag-pipeline","title":":material-pipeline: RAG Pipeline","text":"<p>The RAG Pipeline is the orchestration engine of RAG Toolkit, coordinating the entire flow from document indexing to query answering.</p>"},{"location":"guides/rag_pipeline/#overview","title":"Overview","text":"<p>The Heart of RAG Toolkit</p> <p>A RAG pipeline combines three core components to deliver contextual, accurate answers.</p> <pre><code>graph LR\n    A[\ud83d\udcc4 Documents] --&gt; B[\ud83d\udd22 Embedding&lt;br/&gt;Client]\n    B --&gt; C[\ud83d\udcbe Vector&lt;br/&gt;Store]\n    D[\ud83d\udcac Query] --&gt; B\n    B --&gt; C\n    C --&gt; E[\ud83d\udcda Retrieved&lt;br/&gt;Context]\n    E --&gt; F[\ud83e\udd16 LLM&lt;br/&gt;Client]\n    D --&gt; F\n    F --&gt; G[\u2728 Answer]\n\n    style A fill:#e3f2fd\n    style G fill:#c8e6c9</code></pre> <p>The Three Pillars:</p> <ul> <li> <p> Embedding Client</p> <p>Converts text to semantic vectors</p> Python<pre><code>embedding.embed(\"text\")\n</code></pre> </li> <li> <p> Vector Store</p> <p>Stores and searches embeddings</p> Python<pre><code>store.search(vector, top_k=5)\n</code></pre> </li> <li> <p> LLM Client</p> <p>Generates natural language answers</p> Python<pre><code>llm.generate(prompt)\n</code></pre> </li> </ul>"},{"location":"guides/rag_pipeline/#quick-start","title":"Quick Start","text":""},{"location":"guides/rag_pipeline/#basic-pipeline","title":"Basic Pipeline","text":"basic_pipeline.py<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\nfrom rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\nfrom rag_toolkit.infra.llm import OpenAILLM\n\n# Create pipeline\npipeline = RagPipeline(\n    embedding_client=OpenAIEmbedding(\n        model=\"text-embedding-3-small\"\n    ),\n    vector_store=MilvusVectorStore(\n        collection_name=\"my_docs\",\n        embedding_client=OpenAIEmbedding(),\n        dimension=1536,\n    ),\n    llm_client=OpenAILLM(model=\"gpt-4-turbo\"),\n)\n\n# Index documents\nawait pipeline.index(\n    texts=[\n        \"RAG combines retrieval with generation.\",\n        \"Vector databases enable semantic search.\"\n    ],\n    metadatas=[\n        {\"source\": \"doc1.txt\", \"page\": 1},\n        {\"source\": \"doc2.txt\", \"page\": 1}\n    ]\n)\n\n# Query\nresult = await pipeline.query(\"What is RAG?\")\nprint(f\"Answer: {result.answer}\")\nprint(f\"Sources: {[s.metadata for s in result.sources]}\")\n</code></pre>"},{"location":"guides/rag_pipeline/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"guides/rag_pipeline/#constructor-parameters","title":"Constructor Parameters","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.rag.models import RagConfig\n\n# Full configuration\npipeline = RagPipeline(\n    # Required components\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n\n    # Optional configuration\n    config=RagConfig(\n        retrieval_k=5,  # Number of documents to retrieve\n        rerank=True,  # Enable reranking\n        rerank_k=3,  # Final number after reranking\n        temperature=0.7,  # LLM temperature\n        max_tokens=1000,  # LLM max response length\n    ),\n\n    # Custom prompt template\n    prompt_template=\"\"\"\n    Answer the question based on the context below.\n\n    Context:\n    {context}\n\n    Question: {question}\n\n    Answer:\n    \"\"\",\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#retrieval-configuration","title":"Retrieval Configuration","text":"Python<pre><code># Configure retrieval behavior\nconfig = RagConfig(\n    retrieval_k=10,  # Retrieve 10 documents\n    retrieval_threshold=0.7,  # Minimum similarity score\n    use_metadata_filter=True,  # Enable metadata filtering\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    config=config,\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#llm-configuration","title":"LLM Configuration","text":"Python<pre><code># Configure LLM behavior\nconfig = RagConfig(\n    temperature=0.0,  # Deterministic responses\n    max_tokens=500,  # Shorter responses\n    system_message=\"You are a helpful research assistant.\",\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    config=config,\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#indexing-documents","title":"Indexing Documents","text":""},{"location":"guides/rag_pipeline/#basic-indexing","title":"Basic Indexing","text":"Python<pre><code># Index text documents\nids = await pipeline.index(\n    texts=[\"Document 1 content\", \"Document 2 content\"],\n    metadatas=[\n        {\"source\": \"doc1.pdf\", \"page\": 1},\n        {\"source\": \"doc2.pdf\", \"page\": 1}\n    ]\n)\nprint(f\"Indexed {len(ids)} documents\")\n</code></pre>"},{"location":"guides/rag_pipeline/#batch-indexing","title":"Batch Indexing","text":"Python<pre><code># Index large datasets in batches\ndocuments = load_large_dataset()  # Returns list of documents\n\nbatch_size = 100\nfor i in range(0, len(documents), batch_size):\n    batch = documents[i:i+batch_size]\n\n    await pipeline.index(\n        texts=[doc.text for doc in batch],\n        metadatas=[doc.metadata for doc in batch]\n    )\n\n    print(f\"Indexed {i+batch_size}/{len(documents)} documents\")\n</code></pre>"},{"location":"guides/rag_pipeline/#indexing-with-custom-ids","title":"Indexing with Custom IDs","text":"Python<pre><code># Provide your own document IDs\nawait pipeline.index(\n    ids=[\"doc-1\", \"doc-2\", \"doc-3\"],\n    texts=[\"Text 1\", \"Text 2\", \"Text 3\"],\n    metadatas=[{\"source\": \"file1\"}, {\"source\": \"file2\"}, {\"source\": \"file3\"}]\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#incremental-indexing","title":"Incremental Indexing","text":"Python<pre><code># Add new documents without replacing existing ones\nawait pipeline.index(\n    texts=[\"New document\"],\n    metadatas=[{\"source\": \"new.pdf\", \"date\": \"2024-12-20\"}]\n)\n\n# Update existing document (same ID)\nawait pipeline.index(\n    ids=[\"doc-1\"],\n    texts=[\"Updated content for doc-1\"],\n    metadatas=[{\"source\": \"doc1.pdf\", \"updated\": True}]\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#querying","title":"Querying","text":""},{"location":"guides/rag_pipeline/#basic-query","title":"Basic Query","text":"Python<pre><code># Simple question answering\nresult = await pipeline.query(\"What is machine learning?\")\n\nprint(f\"Answer: {result.answer}\")\nprint(f\"Confidence: {result.confidence}\")\nprint(f\"Sources used: {len(result.sources)}\")\n</code></pre>"},{"location":"guides/rag_pipeline/#query-with-metadata-filtering","title":"Query with Metadata Filtering","text":"Python<pre><code># Filter by document source\nresult = await pipeline.query(\n    \"What are the key findings?\",\n    metadata_filter={\"source\": \"research_paper.pdf\"}\n)\n\n# Filter by multiple criteria\nresult = await pipeline.query(\n    \"Summarize chapter 3\",\n    metadata_filter={\n        \"source\": \"textbook.pdf\",\n        \"chapter\": 3,\n        \"verified\": True\n    }\n)\n\n# Complex filtering\nresult = await pipeline.query(\n    \"Recent developments\",\n    metadata_filter={\n        \"date\": {\"$gte\": \"2024-01-01\"},  # After Jan 1, 2024\n        \"category\": {\"$in\": [\"AI\", \"ML\"]},  # In category list\n    }\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#query-with-custom-k","title":"Query with Custom K","text":"Python<pre><code># Retrieve more/fewer documents\nresult = await pipeline.query(\n    \"Explain neural networks\",\n    k=10  # Retrieve 10 documents instead of default\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#streaming-responses","title":"Streaming Responses","text":"Python<pre><code># Stream answer tokens in real-time\nasync for chunk in pipeline.query_stream(\n    \"Explain quantum computing in detail\"\n):\n    print(chunk, end=\"\", flush=True)\nprint()  # New line\n</code></pre>"},{"location":"guides/rag_pipeline/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/rag_pipeline/#query-rewriting","title":"Query Rewriting","text":"<p>Improve retrieval by rewriting queries:</p> Python<pre><code>from rag_toolkit.rag.rewriter import QueryRewriter\n\n# Create rewriter\nrewriter = QueryRewriter(llm_client=llm)\n\n# Rewrite query\noriginal = \"What's ML?\"\nrewritten = await rewriter.rewrite(original)\nprint(f\"Original: {original}\")\nprint(f\"Rewritten: {rewritten}\")  # \"What is machine learning?\"\n\n# Use in pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    query_rewriter=rewriter,  # Enable rewriting\n)\n\n# Queries automatically rewritten\nresult = await pipeline.query(\"What's ML?\")\n</code></pre>"},{"location":"guides/rag_pipeline/#reranking","title":"Reranking","text":"<p>Improve result quality with reranking:</p> Python<pre><code>from rag_toolkit.rag.rerankers import CrossEncoderReranker\n\n# Create reranker\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n)\n\n# Configure pipeline with reranking\nconfig = RagConfig(\n    retrieval_k=20,  # Retrieve 20 candidates\n    rerank=True,  # Enable reranking\n    rerank_k=5,  # Rerank to top 5\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    config=config,\n    reranker=reranker,\n)\n\n# Queries use two-stage retrieval\nresult = await pipeline.query(\"Important information\")\n# 1. Vector search: 20 candidates\n# 2. Rerank: top 5 most relevant\n# 3. LLM generation with top 5\n</code></pre>"},{"location":"guides/rag_pipeline/#hybrid-search","title":"Hybrid Search","text":"<p>Combine vector and keyword search:</p> Python<pre><code>from rag_toolkit.rag.models import RagConfig\n\n# Enable hybrid search\nconfig = RagConfig(\n    retrieval_k=10,\n    use_hybrid_search=True,  # Enable hybrid search\n    hybrid_alpha=0.7,  # 0.7 vector + 0.3 keyword\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    config=config,\n)\n\n# Queries use both vector and keyword matching\nresult = await pipeline.query(\"machine learning algorithms\")\n</code></pre>"},{"location":"guides/rag_pipeline/#context-assembly","title":"Context Assembly","text":"<p>Control how context is assembled for the LLM:</p> Python<pre><code>from rag_toolkit.rag.assembler import ContextAssembler\n\n# Custom assembler\nassembler = ContextAssembler(\n    max_tokens=2000,  # Maximum context tokens\n    deduplication=True,  # Remove duplicate chunks\n    sorting=\"relevance\",  # Sort by relevance (or \"source\")\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    context_assembler=assembler,\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#multi-query","title":"Multi-Query","text":"<p>Generate multiple query variations for better retrieval:</p> Python<pre><code># Enable multi-query\nconfig = RagConfig(\n    multi_query=True,  # Generate query variations\n    multi_query_count=3,  # Number of variations\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    config=config,\n)\n\n# Single query becomes multiple queries internally\nresult = await pipeline.query(\"What is AI?\")\n# Internally queries:\n# - \"What is artificial intelligence?\"\n# - \"Define AI and its applications\"\n# - \"Explain the concept of AI\"\n</code></pre>"},{"location":"guides/rag_pipeline/#conversational-rag","title":"Conversational RAG","text":"<p>Maintain conversation history for multi-turn interactions:</p> Python<pre><code>from rag_toolkit.rag.models import ConversationHistory\n\n# Initialize conversation history\nhistory = ConversationHistory()\n\n# First question\nresult1 = await pipeline.query(\n    \"What is machine learning?\",\n    conversation_history=history\n)\nprint(result1.answer)\n\n# Follow-up (uses history for context)\nresult2 = await pipeline.query(\n    \"What are its applications?\",  # \"its\" refers to ML from previous question\n    conversation_history=history\n)\nprint(result2.answer)\n\n# Another follow-up\nresult3 = await pipeline.query(\n    \"Give me an example\",  # Context from previous turns\n    conversation_history=history\n)\nprint(result3.answer)\n</code></pre>"},{"location":"guides/rag_pipeline/#custom-prompt-templates","title":"Custom Prompt Templates","text":""},{"location":"guides/rag_pipeline/#template-structure","title":"Template Structure","text":"Python<pre><code># Define custom template\ncustom_template = \"\"\"\nYou are an expert {role}.\n\nContext information:\n{context}\n\nUser question: {question}\n\nInstructions:\n- Provide detailed explanations\n- Use examples when helpful\n- Cite sources using [Source: X]\n\nYour answer:\n\"\"\"\n\n# Use in pipeline\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    prompt_template=custom_template,\n)\n\n# Query with custom role\nresult = await pipeline.query(\n    \"Explain neural networks\",\n    template_variables={\"role\": \"AI researcher\"}\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#multi-language-templates","title":"Multi-Language Templates","text":"Python<pre><code># English template\nen_template = \"\"\"\nContext: {context}\nQuestion: {question}\nAnswer:\n\"\"\"\n\n# Italian template\nit_template = \"\"\"\nContesto: {context}\nDomanda: {question}\nRisposta:\n\"\"\"\n\n# Create pipelines for each language\nen_pipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    prompt_template=en_template,\n)\n\nit_pipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    prompt_template=it_template,\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#pipeline-evaluation","title":"Pipeline Evaluation","text":""},{"location":"guides/rag_pipeline/#basic-metrics","title":"Basic Metrics","text":"Python<pre><code>from rag_toolkit.rag.evaluation import RagEvaluator\n\n# Create evaluator\nevaluator = RagEvaluator(pipeline=pipeline)\n\n# Evaluate on test set\ntest_questions = [\n    \"What is machine learning?\",\n    \"Explain neural networks\",\n    \"What is gradient descent?\"\n]\n\ntest_answers = [\n    \"Machine learning is...\",\n    \"Neural networks are...\",\n    \"Gradient descent is...\"\n]\n\n# Run evaluation\nmetrics = await evaluator.evaluate(\n    questions=test_questions,\n    expected_answers=test_answers\n)\n\nprint(f\"Accuracy: {metrics.accuracy:.2f}\")\nprint(f\"Relevance: {metrics.relevance:.2f}\")\nprint(f\"Faithfulness: {metrics.faithfulness:.2f}\")\n</code></pre>"},{"location":"guides/rag_pipeline/#retrieval-quality","title":"Retrieval Quality","text":"Python<pre><code># Evaluate retrieval only\nretrieval_metrics = await evaluator.evaluate_retrieval(\n    questions=test_questions,\n    relevant_doc_ids=[\n        [\"doc-1\", \"doc-2\"],  # Relevant for Q1\n        [\"doc-3\", \"doc-4\"],  # Relevant for Q2\n        [\"doc-5\"],  # Relevant for Q3\n    ]\n)\n\nprint(f\"Precision@5: {retrieval_metrics.precision_at_5:.2f}\")\nprint(f\"Recall@5: {retrieval_metrics.recall_at_5:.2f}\")\nprint(f\"MRR: {retrieval_metrics.mrr:.2f}\")\n</code></pre>"},{"location":"guides/rag_pipeline/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/rag_pipeline/#caching","title":"Caching","text":"Python<pre><code>from rag_toolkit.rag.cache import QueryCache\n\n# Create cache\ncache = QueryCache(max_size=1000)\n\n# Wrap pipeline\ncached_pipeline = cache.wrap(pipeline)\n\n# First query: normal execution\nresult1 = await cached_pipeline.query(\"What is AI?\")  # Executes query\n\n# Second query: from cache\nresult2 = await cached_pipeline.query(\"What is AI?\")  # Returns cached result\n</code></pre>"},{"location":"guides/rag_pipeline/#batch-processing","title":"Batch Processing","text":"Python<pre><code># Process multiple queries in parallel\nqueries = [\n    \"What is ML?\",\n    \"Explain neural networks\",\n    \"Define gradient descent\"\n]\n\n# Parallel execution\nresults = await pipeline.query_batch(queries)\n\nfor query, result in zip(queries, results):\n    print(f\"Q: {query}\")\n    print(f\"A: {result.answer}\\n\")\n</code></pre>"},{"location":"guides/rag_pipeline/#connection-pooling","title":"Connection Pooling","text":"Python<pre><code># Pipeline automatically manages connections\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=MilvusVectorStore(\n        collection_name=\"docs\",\n        pool_size=10,  # Connection pool\n    ),\n    llm_client=llm,\n)\n</code></pre>"},{"location":"guides/rag_pipeline/#error-handling","title":"Error Handling","text":""},{"location":"guides/rag_pipeline/#graceful-degradation","title":"Graceful Degradation","text":"Python<pre><code>try:\n    result = await pipeline.query(\"What is AI?\")\nexcept Exception as e:\n    # Log error\n    logger.error(f\"Query failed: {e}\")\n\n    # Fallback response\n    result = RagResult(\n        answer=\"I apologize, but I'm unable to answer that question right now.\",\n        sources=[],\n        confidence=0.0\n    )\n</code></pre>"},{"location":"guides/rag_pipeline/#retry-logic","title":"Retry Logic","text":"Python<pre><code>from tenacity import retry, wait_exponential, stop_after_attempt\n\n@retry(\n    wait=wait_exponential(min=1, max=60),\n    stop=stop_after_attempt(3)\n)\nasync def robust_query(query: str):\n    \"\"\"Query with automatic retries.\"\"\"\n    return await pipeline.query(query)\n\n# Usage\nresult = await robust_query(\"What is machine learning?\")\n</code></pre>"},{"location":"guides/rag_pipeline/#monitoring","title":"Monitoring","text":""},{"location":"guides/rag_pipeline/#query-logging","title":"Query Logging","text":"Python<pre><code>import logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Log queries\nclass LoggedPipeline:\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n\n    async def query(self, query: str, **kwargs):\n        logger.info(f\"Query received: {query}\")\n\n        start = time.time()\n        result = await self.pipeline.query(query, **kwargs)\n        duration = time.time() - start\n\n        logger.info(f\"Query completed in {duration:.2f}s\")\n        logger.info(f\"Sources used: {len(result.sources)}\")\n\n        return result\n\n# Usage\nlogged_pipeline = LoggedPipeline(pipeline)\nresult = await logged_pipeline.query(\"What is AI?\")\n</code></pre>"},{"location":"guides/rag_pipeline/#metrics-collection","title":"Metrics Collection","text":"Python<pre><code>from prometheus_client import Counter, Histogram\n\n# Define metrics\nquery_counter = Counter('rag_queries_total', 'Total RAG queries')\nquery_duration = Histogram('rag_query_duration_seconds', 'Query duration')\n\n# Instrument pipeline\nclass MetricsPipeline:\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n\n    async def query(self, query: str, **kwargs):\n        query_counter.inc()\n\n        with query_duration.time():\n            result = await self.pipeline.query(query, **kwargs)\n\n        return result\n\n# Usage\nmetrics_pipeline = MetricsPipeline(pipeline)\n</code></pre>"},{"location":"guides/rag_pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Component Selection</li> <li>Use OpenAI embeddings for best quality</li> <li>Use GPT-4-turbo for complex reasoning</li> <li> <p>Use Ollama for privacy/offline needs</p> </li> <li> <p>Configuration</p> </li> <li>Start with <code>retrieval_k=5</code></li> <li>Enable reranking for critical applications</li> <li> <p>Use temperature=0.0 for consistency</p> </li> <li> <p>Indexing</p> </li> <li>Batch large datasets</li> <li>Include rich metadata</li> <li> <p>Update documents incrementally</p> </li> <li> <p>Querying</p> </li> <li>Use metadata filters to narrow scope</li> <li>Enable query rewriting for better retrieval</li> <li> <p>Implement conversation history for multi-turn</p> </li> <li> <p>Performance</p> </li> <li>Cache common queries</li> <li>Use connection pooling</li> <li> <p>Monitor query latency</p> </li> <li> <p>Quality</p> </li> <li>Evaluate regularly with test sets</li> <li>A/B test different configurations</li> <li>Collect user feedback</li> </ol>"},{"location":"guides/rag_pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Vector Stores Guide - Deep dive into vector stores</li> <li>Embeddings Guide - Learn about embeddings</li> <li>LLMs Guide - Master LLM configuration</li> <li>Advanced Pipeline Example</li> <li>Production Setup</li> </ul>"},{"location":"guides/rag_pipeline/#see-also","title":"See Also","text":"<ul> <li>Core Concepts - RAG fundamentals</li> <li>Protocols - Understand the protocol system</li> <li>Architecture - System design</li> </ul>"},{"location":"guides/reranking/","title":"Reranking","text":""},{"location":"guides/reranking/#reranking","title":"Reranking","text":"<p>Reranking is a game-changing technique that dramatically improves retrieval quality. Master reranking for production-grade RAG systems.</p>"},{"location":"guides/reranking/#why-reranking","title":"Why Reranking?","text":"<p>The Problem with Single-Stage Retrieval</p> <p>Limitations:</p> <ul> <li> Embedding-based search is fast but imperfect</li> <li>:material-target-off: May retrieve semantically similar but irrelevant documents</li> <li> Limited by embedding quality</li> </ul> <p>Solution: Two-Stage Retrieval</p> <ol> <li>Stage 1 (Fast): Vector search retrieves many candidates (e.g., 50-100)</li> <li>Stage 2 (Accurate): Reranker scores candidates and selects best (e.g., top 5)</li> </ol> <p>Results:</p> <ul> <li> 10-30% improvement in retrieval quality</li> <li> Better context for LLM</li> <li> More relevant answers</li> <li> Minimal latency increase</li> </ul> <pre><code>graph LR\n    A[\ud83d\udcac Query] --&gt; B[\ud83d\udd0d Vector Search]\n    B --&gt; C[\ud83d\udcda 50 Candidates]\n    C --&gt; D[\u2b50 Reranker]\n    D --&gt; E[\ud83c\udfaf Top 5 Best]\n    E --&gt; F[\ud83e\udd16 LLM]\n    F --&gt; G[\u2728 Answer]\n\n    style A fill:#e3f2fd\n    style E fill:#fff3e0\n    style G fill:#c8e6c9</code></pre>"},{"location":"guides/reranking/#reranker-types","title":"Reranker Types","text":"<p>Choose Your Reranking Strategy</p>"},{"location":"guides/reranking/#1-cross-encoder-reranker","title":"1. Cross-Encoder Reranker","text":"<p>Uses deep learning to score query-document pairs.</p> <p>Best for: High accuracy, production systems</p> Python<pre><code>from rag_toolkit.rag.rerankers import CrossEncoderReranker\n\n# Create reranker\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n)\n\n# Rerank results\ncandidates = await vector_store.search(query, limit=50)\nreranked = await reranker.rerank(\n    query=\"What is machine learning?\",\n    documents=candidates,\n    top_k=5  # Return top 5\n)\n\nfor doc in reranked:\n    print(f\"Score: {doc.score:.4f}\")\n    print(f\"Text: {doc.text[:100]}...\")\n</code></pre> <p>Popular Models:</p> Model Speed Quality Use Case <code>ms-marco-MiniLM-L-6-v2</code> Fast Good General purpose <code>ms-marco-MiniLM-L-12-v2</code> Medium Better Higher quality <code>ms-marco-electra-base</code> Slower Best Maximum quality"},{"location":"guides/reranking/#2-llm-based-reranker","title":"2. LLM-Based Reranker","text":"<p>Uses LLM to judge relevance.</p> <p>Best for: Maximum quality, complex queries</p> Python<pre><code>from rag_toolkit.rag.rerankers import LLMReranker\n\n# Create LLM reranker\nreranker = LLMReranker(\n    llm_client=OpenAILLM(model=\"gpt-4-turbo\")\n)\n\n# Rerank with LLM\ncandidates = await vector_store.search(query, limit=20)\nreranked = await reranker.rerank(\n    query=\"Explain quantum computing for beginners\",\n    documents=candidates,\n    top_k=5\n)\n</code></pre> <p>Prompt: </p>Text Only<pre><code>Query: {query}\nDocument: {document}\n\nRate the relevance of this document to the query on a scale of 0-1.\nOnly return the numeric score.\n</code></pre><p></p>"},{"location":"guides/reranking/#3-reciprocal-rank-fusion-rrf","title":"3. Reciprocal Rank Fusion (RRF)","text":"<p>Combines rankings from multiple retrievers.</p> <p>Best for: Hybrid search, multi-source retrieval</p> Python<pre><code>from rag_toolkit.rag.rerankers import ReciprocalRankFusion\n\n# Create RRF reranker\nreranker = ReciprocalRankFusion(k=60)\n\n# Get results from multiple sources\nvector_results = await vector_store.search(query, limit=20)\nkeyword_results = await keyword_search(query, limit=20)\n\n# Fuse rankings\nfused = await reranker.fuse(\n    results_lists=[vector_results, keyword_results]\n)\n</code></pre> <p>Formula: </p>Text Only<pre><code>score(doc) = \u03a3 (1 / (k + rank_i))\n</code></pre><p></p>"},{"location":"guides/reranking/#4-similarity-reranker","title":"4. Similarity Reranker","text":"<p>Rerank by semantic similarity to query.</p> <p>Best for: Simple reranking, no external dependencies</p> Python<pre><code>from rag_toolkit.rag.rerankers import SimilarityReranker\n\n# Create similarity reranker\nreranker = SimilarityReranker(\n    embedding_client=OpenAIEmbedding()\n)\n\n# Rerank by similarity\nreranked = await reranker.rerank(\n    query=\"machine learning\",\n    documents=candidates,\n    top_k=5\n)\n</code></pre>"},{"location":"guides/reranking/#5-diversity-reranker","title":"5. Diversity Reranker","text":"<p>Maximize diversity in results.</p> <p>Best for: Exploratory search, varied perspectives</p> Python<pre><code>from rag_toolkit.rag.rerankers import DiversityReranker\n\n# Create diversity reranker\nreranker = DiversityReranker(\n    embedding_client=OpenAIEmbedding(),\n    lambda_param=0.5  # 0.5 relevance + 0.5 diversity\n)\n\n# Get diverse results\nreranked = await reranker.rerank(\n    query=\"AI applications\",\n    documents=candidates,\n    top_k=10\n)\n</code></pre>"},{"location":"guides/reranking/#integration-with-rag","title":"Integration with RAG","text":""},{"location":"guides/reranking/#basic-integration","title":"Basic Integration","text":"Python<pre><code>from rag_toolkit import RagPipeline\nfrom rag_toolkit.rag.rerankers import CrossEncoderReranker\nfrom rag_toolkit.rag.models import RagConfig\n\n# Create reranker\nreranker = CrossEncoderReranker()\n\n# Configure pipeline with reranking\nconfig = RagConfig(\n    retrieval_k=50,  # Retrieve 50 candidates\n    rerank=True,  # Enable reranking\n    rerank_k=5,  # Rerank to top 5\n)\n\npipeline = RagPipeline(\n    embedding_client=embedding,\n    vector_store=vector_store,\n    llm_client=llm,\n    config=config,\n    reranker=reranker,\n)\n\n# Queries automatically use two-stage retrieval\nresult = await pipeline.query(\"What is machine learning?\")\n# 1. Vector search: 50 candidates\n# 2. Rerank: top 5 most relevant\n# 3. LLM: generate answer from top 5\n</code></pre>"},{"location":"guides/reranking/#custom-reranking-strategy","title":"Custom Reranking Strategy","text":"Python<pre><code># Multi-stage reranking\nconfig = RagConfig(\n    retrieval_k=100,  # Stage 1: 100 candidates\n    rerank=True,\n    rerank_k=20,  # Stage 2: rerank to 20\n)\n\n# Then apply diversity\ndiversity_reranker = DiversityReranker()\n\n# In custom pipeline\ncandidates = await vector_store.search(query, limit=100)\nrelevant = await cross_encoder.rerank(query, candidates, top_k=20)\nfinal = await diversity_reranker.rerank(query, relevant, top_k=5)\n</code></pre>"},{"location":"guides/reranking/#advanced-reranking","title":"Advanced Reranking","text":""},{"location":"guides/reranking/#weighted-reranking","title":"Weighted Reranking","text":"<p>Combine multiple signals:</p> Python<pre><code>from rag_toolkit.rag.rerankers import WeightedReranker\n\n# Create weighted reranker\nreranker = WeightedReranker(\n    rerankers=[\n        (CrossEncoderReranker(), 0.6),  # 60% weight\n        (SimilarityReranker(), 0.3),  # 30% weight\n        (RecencyReranker(), 0.1),  # 10% weight\n    ]\n)\n\n# Combined scoring\nreranked = await reranker.rerank(\n    query=\"recent AI developments\",\n    documents=candidates,\n    top_k=5\n)\n</code></pre>"},{"location":"guides/reranking/#metadata-aware-reranking","title":"Metadata-Aware Reranking","text":"<p>Boost results based on metadata:</p> Python<pre><code>from rag_toolkit.rag.rerankers import MetadataReranker\n\n# Create metadata-aware reranker\nreranker = MetadataReranker(\n    base_reranker=CrossEncoderReranker(),\n    metadata_boosts={\n        \"source\": {\"research_paper\": 1.2, \"blog\": 0.8},\n        \"year\": lambda y: 1.0 + (int(y) - 2020) * 0.05,  # Boost recent\n    }\n)\n\n# Rerank with metadata consideration\nreranked = await reranker.rerank(\n    query=\"latest ML techniques\",\n    documents=candidates,\n    top_k=5\n)\n</code></pre>"},{"location":"guides/reranking/#query-specific-reranking","title":"Query-Specific Reranking","text":"<p>Adapt reranking to query type:</p> Python<pre><code>from rag_toolkit.rag.rerankers import AdaptiveReranker\n\n# Create adaptive reranker\nreranker = AdaptiveReranker(\n    query_classifier=QueryClassifier(),\n    rerankers={\n        \"factual\": CrossEncoderReranker(),\n        \"exploratory\": DiversityReranker(),\n        \"recent\": RecencyReranker(),\n    }\n)\n\n# Automatically selects best reranker\nreranked = await reranker.rerank(\n    query=query,  # Classified automatically\n    documents=candidates,\n    top_k=5\n)\n</code></pre>"},{"location":"guides/reranking/#reranking-configuration","title":"Reranking Configuration","text":""},{"location":"guides/reranking/#candidate-count","title":"Candidate Count","text":"Python<pre><code># More candidates = better recall, slower\nconfig = RagConfig(\n    retrieval_k=100,  # Retrieve 100\n    rerank_k=5,  # Select top 5\n)\n\n# Fewer candidates = faster, lower recall\nconfig = RagConfig(\n    retrieval_k=20,  # Retrieve 20\n    rerank_k=5,  # Select top 5\n)\n</code></pre> <p>Recommended: - Production: <code>retrieval_k=50</code>, <code>rerank_k=5</code> - High quality: <code>retrieval_k=100</code>, <code>rerank_k=10</code> - Fast: <code>retrieval_k=20</code>, <code>rerank_k=3</code></p>"},{"location":"guides/reranking/#score-threshold","title":"Score Threshold","text":"Python<pre><code># Only keep documents above threshold\nreranked = await reranker.rerank(\n    query=query,\n    documents=candidates,\n    top_k=10,\n    min_score=0.5,  # Minimum relevance score\n)\n\n# May return fewer than top_k if scores too low\n</code></pre>"},{"location":"guides/reranking/#batching","title":"Batching","text":"Python<pre><code># Rerank multiple queries in batch\nqueries = [\"Q1\", \"Q2\", \"Q3\"]\ncandidates_lists = [candidates1, candidates2, candidates3]\n\n# Batch reranking\nall_reranked = await reranker.rerank_batch(\n    queries=queries,\n    documents_lists=candidates_lists,\n    top_k=5\n)\n</code></pre>"},{"location":"guides/reranking/#model-selection","title":"Model Selection","text":""},{"location":"guides/reranking/#cross-encoder-models","title":"Cross-Encoder Models","text":"Python<pre><code># Fast and good quality (recommended)\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n)\n\n# Better quality, slower\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n)\n\n# Best quality, slowest\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-electra-base\"\n)\n</code></pre>"},{"location":"guides/reranking/#domain-specific-models","title":"Domain-Specific Models","text":"Python<pre><code># Biomedical\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/biomed-roberta-base\"\n)\n\n# Legal\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/legal-bert-base\"\n)\n\n# Multilingual\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\"\n)\n</code></pre>"},{"location":"guides/reranking/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/reranking/#caching-scores","title":"Caching Scores","text":"Python<pre><code>from functools import lru_cache\n\nclass CachedReranker:\n    \"\"\"Reranker with score caching.\"\"\"\n\n    def __init__(self, reranker):\n        self.reranker = reranker\n        self._cache = {}\n\n    async def rerank(self, query: str, documents, top_k: int):\n        \"\"\"Rerank with caching.\"\"\"\n        # Cache key: query + document IDs\n        cache_key = (query, tuple(d.id for d in documents))\n\n        if cache_key not in self._cache:\n            self._cache[cache_key] = await self.reranker.rerank(\n                query, documents, top_k\n            )\n\n        return self._cache[cache_key]\n\n# Usage\ncached_reranker = CachedReranker(reranker)\n</code></pre>"},{"location":"guides/reranking/#gpu-acceleration","title":"GPU Acceleration","text":"Python<pre><code># Use GPU for faster reranking\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    device=\"cuda\",  # Use GPU\n    batch_size=32,  # Larger batches on GPU\n)\n</code></pre>"},{"location":"guides/reranking/#batch-processing","title":"Batch Processing","text":"Python<pre><code># Process multiple queries in parallel\nimport asyncio\n\nasync def batch_rerank(\n    queries: list[str],\n    documents_lists: list,\n    reranker,\n    max_concurrent: int = 5\n):\n    \"\"\"Rerank multiple queries in parallel.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def rerank_one(query, documents):\n        async with semaphore:\n            return await reranker.rerank(query, documents, top_k=5)\n\n    tasks = [\n        rerank_one(q, docs)\n        for q, docs in zip(queries, documents_lists)\n    ]\n\n    return await asyncio.gather(*tasks)\n\n# Usage\nresults = await batch_rerank(queries, candidates_lists, reranker)\n</code></pre>"},{"location":"guides/reranking/#evaluation","title":"Evaluation","text":""},{"location":"guides/reranking/#reranking-quality","title":"Reranking Quality","text":"Python<pre><code>from rag_toolkit.rag.evaluation import RerankingEvaluator\n\n# Create evaluator\nevaluator = RerankingEvaluator()\n\n# Evaluate reranker\nmetrics = await evaluator.evaluate(\n    reranker=reranker,\n    test_queries=queries,\n    candidates_lists=all_candidates,\n    relevant_doc_ids=relevant_ids,\n)\n\nprint(f\"NDCG@5: {metrics.ndcg_at_5:.3f}\")\nprint(f\"MRR: {metrics.mrr:.3f}\")\nprint(f\"Precision@5: {metrics.precision_at_5:.3f}\")\n</code></pre>"},{"location":"guides/reranking/#ab-testing","title":"A/B Testing","text":"Python<pre><code># Compare rerankers\nreranker_a = CrossEncoderReranker()\nreranker_b = LLMReranker()\n\nmetrics_a = await evaluator.evaluate(reranker_a, test_data)\nmetrics_b = await evaluator.evaluate(reranker_b, test_data)\n\nprint(f\"Reranker A NDCG: {metrics_a.ndcg_at_5:.3f}\")\nprint(f\"Reranker B NDCG: {metrics_b.ndcg_at_5:.3f}\")\n\nif metrics_b.ndcg_at_5 &gt; metrics_a.ndcg_at_5:\n    print(\"Reranker B wins!\")\n</code></pre>"},{"location":"guides/reranking/#custom-rerankers","title":"Custom Rerankers","text":"<p>Implement your own reranker:</p> Python<pre><code>from typing import Protocol\n\nclass Reranker(Protocol):\n    \"\"\"Protocol for rerankers.\"\"\"\n\n    async def rerank(\n        self,\n        query: str,\n        documents: list,\n        top_k: int\n    ) -&gt; list:\n        \"\"\"Rerank documents.\"\"\"\n        ...\n</code></pre>"},{"location":"guides/reranking/#example-bm25-reranker","title":"Example: BM25 Reranker","text":"Python<pre><code>from rank_bm25 import BM25Okapi\n\nclass BM25Reranker:\n    \"\"\"BM25-based reranker.\"\"\"\n\n    def __init__(self):\n        self.bm25 = None\n\n    async def rerank(\n        self,\n        query: str,\n        documents: list,\n        top_k: int\n    ) -&gt; list:\n        \"\"\"Rerank using BM25.\"\"\"\n        # Tokenize\n        tokenized_docs = [doc.text.split() for doc in documents]\n        tokenized_query = query.split()\n\n        # Create BM25\n        self.bm25 = BM25Okapi(tokenized_docs)\n\n        # Score\n        scores = self.bm25.get_scores(tokenized_query)\n\n        # Sort by score\n        doc_scores = list(zip(documents, scores))\n        doc_scores.sort(key=lambda x: x[1], reverse=True)\n\n        # Return top k\n        return [doc for doc, score in doc_scores[:top_k]]\n\n# Usage\nbm25_reranker = BM25Reranker()\nreranked = await bm25_reranker.rerank(query, candidates, top_k=5)\n</code></pre>"},{"location":"guides/reranking/#monitoring","title":"Monitoring","text":""},{"location":"guides/reranking/#track-reranking-impact","title":"Track Reranking Impact","text":"Python<pre><code>class MonitoredReranker:\n    \"\"\"Reranker with monitoring.\"\"\"\n\n    def __init__(self, reranker):\n        self.reranker = reranker\n        self.improvements = []\n\n    async def rerank(self, query, documents, top_k):\n        \"\"\"Rerank with monitoring.\"\"\"\n        # Original order scores\n        original_scores = [d.score for d in documents]\n\n        # Rerank\n        reranked = await self.reranker.rerank(query, documents, top_k)\n\n        # New scores\n        new_scores = [d.score for d in reranked]\n\n        # Track improvement\n        improvement = sum(new_scores) - sum(original_scores[:top_k])\n        self.improvements.append(improvement)\n\n        return reranked\n\n    def get_avg_improvement(self):\n        \"\"\"Get average score improvement.\"\"\"\n        return sum(self.improvements) / len(self.improvements)\n\n# Usage\nmonitored = MonitoredReranker(reranker)\nawait monitored.rerank(query, candidates, top_k=5)\nprint(f\"Avg improvement: {monitored.get_avg_improvement():.3f}\")\n</code></pre>"},{"location":"guides/reranking/#best-practices","title":"Best Practices","text":"<ol> <li>Always Use Reranking in Production</li> <li>10-30% quality improvement</li> <li>Minimal latency cost</li> <li> <p>Worth the complexity</p> </li> <li> <p>Retrieve More, Rerank Less</p> </li> <li>Retrieve 50-100 candidates</li> <li>Rerank to top 5-10</li> <li> <p>Maximizes recall and precision</p> </li> <li> <p>Choose the Right Reranker</p> </li> <li>Cross-encoder: Default choice</li> <li>LLM: Maximum quality, higher cost</li> <li> <p>RRF: Combining multiple sources</p> </li> <li> <p>Optimize Configuration</p> </li> <li>Test different candidate counts</li> <li>Tune score thresholds</li> <li> <p>A/B test models</p> </li> <li> <p>Cache Aggressively</p> </li> <li>Cache reranking results</li> <li>Significant speed improvement</li> <li> <p>Low memory cost</p> </li> <li> <p>Monitor Quality</p> </li> <li>Track NDCG, MRR metrics</li> <li>Compare with/without reranking</li> <li>Iterate based on results</li> </ol>"},{"location":"guides/reranking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/reranking/#reranking-too-slow","title":"Reranking Too Slow","text":"Python<pre><code># Use faster model\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # Fast model\n)\n\n# Reduce candidates\nconfig = RagConfig(retrieval_k=20, rerank_k=5)\n\n# Use GPU\nreranker = CrossEncoderReranker(device=\"cuda\")\n</code></pre>"},{"location":"guides/reranking/#poor-reranking-quality","title":"Poor Reranking Quality","text":"Python<pre><code># Use better model\nreranker = CrossEncoderReranker(\n    model=\"cross-encoder/ms-marco-electra-base\"  # Better quality\n)\n\n# Retrieve more candidates\nconfig = RagConfig(retrieval_k=100, rerank_k=10)\n\n# Use LLM reranker\nreranker = LLMReranker(llm_client=OpenAILLM(model=\"gpt-4-turbo\"))\n</code></pre>"},{"location":"guides/reranking/#memory-issues","title":"Memory Issues","text":"Python<pre><code># Reduce batch size\nreranker = CrossEncoderReranker(batch_size=8)\n\n# Process in smaller batches\nfor i in range(0, len(candidates), 20):\n    batch = candidates[i:i+20]\n    reranked_batch = await reranker.rerank(query, batch, top_k=5)\n</code></pre>"},{"location":"guides/reranking/#next-steps","title":"Next Steps","text":"<ul> <li>RAG Pipeline - Full pipeline integration</li> <li>Advanced Pipeline Example</li> <li>Production Setup</li> <li>Vector Stores - Initial retrieval</li> </ul>"},{"location":"guides/reranking/#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Retrieval fundamentals</li> <li>Architecture - System design</li> <li>Cross-Encoder Models</li> </ul>"},{"location":"guides/vector_stores/","title":"Vector Stores","text":""},{"location":"guides/vector_stores/#vector-stores","title":"Vector Stores","text":"<p>Vector stores are the foundation of RAG systems, providing efficient storage and retrieval of high-dimensional embeddings.</p>"},{"location":"guides/vector_stores/#overview","title":"Overview","text":"<p>What is a Vector Store?</p> <p>A vector store (or vector database) stores embeddings along with metadata and provides lightning-fast similarity search capabilities.</p> <pre><code>graph LR\n    A[\ud83d\udcc4 Documents] --&gt; B[\ud83d\udd22 Embeddings]\n    B --&gt; C[\ud83d\udcbe Vector Store]\n    D[\ud83d\udcac Query] --&gt; E[\ud83d\udd22 Query Embedding]\n    E --&gt; C\n    C --&gt; F[\ud83d\udcda Similar Documents]\n\n    style A fill:#e3f2fd\n    style F fill:#c8e6c9</code></pre> <p>How it works:</p> <ol> <li>Documents are converted to embeddings and stored</li> <li>Query is converted to an embedding</li> <li>Vector store finds the most similar stored embeddings</li> <li>Returns the corresponding documents</li> </ol>"},{"location":"guides/vector_stores/#supported-vector-stores","title":"Supported Vector Stores","text":""},{"location":"guides/vector_stores/#milvus-primary-implementation","title":"Milvus (Primary Implementation)","text":"<p>Production-Ready Performance</p> <p>Milvus is the default vector store in RAG Toolkit, offering enterprise-grade features.</p> <p>Features:</p> <ul> <li> <p> Distributed Architecture</p> <p>Horizontal scalability for massive datasets</p> </li> <li> <p> Multiple Index Types</p> <p>HNSW, IVF_FLAT, IVF_PQ for different use cases</p> </li> <li> <p>:material-gpu: GPU Acceleration</p> <p>Optional GPU support for faster search</p> </li> <li> <p> Hybrid Search</p> <p>Vector + metadata filtering combined</p> </li> </ul> <p>Installation:</p> Bash<pre><code># Install with Milvus support (included by default)\npip install rag-toolkit\n\n# Start Milvus (Docker)\ndocker run -d --name milvus-standalone \\\n  -p 19530:19530 \\\n  -p 9091:9091 \\\n  milvusdb/milvus:latest standalone\n</code></pre> <p>Basic Usage:</p> Python<pre><code>from rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\nfrom rag_toolkit.infra.embedding import OpenAIEmbedding\n\n# Initialize vector store\nvector_store = MilvusVectorStore(\n    collection_name=\"my_documents\",\n    uri=\"http://localhost:19530\",\n    embedding_client=OpenAIEmbedding(),\n    dimension=1536,  # OpenAI ada-002 dimension\n)\n\n# Insert documents\nvector_store.upsert(\n    texts=[\"Document 1 content\", \"Document 2 content\"],\n    metadatas=[\n        {\"source\": \"doc1.pdf\", \"page\": 1},\n        {\"source\": \"doc2.pdf\", \"page\": 1}\n    ]\n)\n\n# Search\nresults = vector_store.search(\n    query=\"What is machine learning?\",\n    limit=5\n)\n\nfor result in results:\n    print(f\"Score: {result.score}\")\n    print(f\"Text: {result.text}\")\n    print(f\"Metadata: {result.metadata}\")\n</code></pre>"},{"location":"guides/vector_stores/#configuration","title":"Configuration","text":""},{"location":"guides/vector_stores/#connection-settings","title":"Connection Settings","text":"Python<pre><code>from rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\n\n# Local Milvus\nvector_store = MilvusVectorStore(\n    collection_name=\"documents\",\n    uri=\"http://localhost:19530\",\n    token=None,  # No authentication for local\n)\n\n# Milvus Cloud (Zilliz)\nvector_store = MilvusVectorStore(\n    collection_name=\"documents\",\n    uri=\"https://your-instance.zillizcloud.com:19530\",\n    token=\"your_api_key\",\n)\n\n# Milvus with authentication\nvector_store = MilvusVectorStore(\n    collection_name=\"documents\",\n    uri=\"http://production-milvus:19530\",\n    token=\"user:password\",\n)\n</code></pre>"},{"location":"guides/vector_stores/#index-configuration","title":"Index Configuration","text":"<p>Different index types offer trade-offs between speed, accuracy, and memory:</p> Python<pre><code>from rag_toolkit.infra.vectorstores.milvus.config import IndexConfig\n\n# HNSW - Best for high accuracy (default)\nindex_config = IndexConfig(\n    index_type=\"HNSW\",\n    metric_type=\"COSINE\",  # or \"L2\", \"IP\"\n    params={\"M\": 16, \"efConstruction\": 200}\n)\n\n# IVF_FLAT - Balanced speed/accuracy\nindex_config = IndexConfig(\n    index_type=\"IVF_FLAT\",\n    metric_type=\"COSINE\",\n    params={\"nlist\": 1024}\n)\n\n# IVF_PQ - Memory efficient for large datasets\nindex_config = IndexConfig(\n    index_type=\"IVF_PQ\",\n    metric_type=\"COSINE\",\n    params={\"nlist\": 1024, \"m\": 8, \"nbits\": 8}\n)\n\nvector_store = MilvusVectorStore(\n    collection_name=\"documents\",\n    index_config=index_config,\n)\n</code></pre>"},{"location":"guides/vector_stores/#metric-types","title":"Metric Types","text":"<p>Choose the right distance metric for your embeddings:</p> Metric Use Case Range Best For <code>COSINE</code> Default, most common [-1, 1] Text embeddings (OpenAI, Ollama) <code>L2</code> Euclidean distance [0, \u221e) Image embeddings <code>IP</code> Inner product [-\u221e, \u221e) Pre-normalized vectors Python<pre><code># Cosine similarity (recommended for most use cases)\nvector_store = MilvusVectorStore(\n    collection_name=\"documents\",\n    metric_type=\"COSINE\",\n)\n</code></pre>"},{"location":"guides/vector_stores/#collection-management","title":"Collection Management","text":""},{"location":"guides/vector_stores/#creating-collections","title":"Creating Collections","text":"Python<pre><code>from rag_toolkit.infra.vectorstores.milvus import MilvusVectorStore\n\n# Auto-create collection on first insert\nvector_store = MilvusVectorStore(\n    collection_name=\"my_docs\",\n    dimension=1536,\n    auto_create=True,  # Default\n)\n\n# Explicitly create collection\nawait vector_store.create_collection(\n    dimension=1536,\n    description=\"My document collection\",\n)\n\n# Check if collection exists\nexists = await vector_store.collection_exists()\nprint(f\"Collection exists: {exists}\")\n</code></pre>"},{"location":"guides/vector_stores/#listing-collections","title":"Listing Collections","text":"Python<pre><code># List all collections\ncollections = await vector_store.list_collections()\nfor collection in collections:\n    print(f\"Name: {collection.name}\")\n    print(f\"Count: {collection.count}\")\n    print(f\"Dimension: {collection.dimension}\")\n</code></pre>"},{"location":"guides/vector_stores/#dropping-collections","title":"Dropping Collections","text":"Python<pre><code># Delete collection and all data\nawait vector_store.drop_collection()\n</code></pre>"},{"location":"guides/vector_stores/#data-operations","title":"Data Operations","text":""},{"location":"guides/vector_stores/#inserting-documents","title":"Inserting Documents","text":"Python<pre><code># Simple insert\nids = vector_store.upsert(\n    texts=[\"First document\", \"Second document\"],\n    metadatas=[{\"tag\": \"intro\"}, {\"tag\": \"advanced\"}]\n)\n\n# Insert with explicit IDs\nids = vector_store.upsert(\n    ids=[\"doc-1\", \"doc-2\"],\n    texts=[\"Content 1\", \"Content 2\"],\n    metadatas=[{\"source\": \"pdf\"}, {\"source\": \"web\"}]\n)\n\n# Insert with pre-computed embeddings\nids = vector_store.upsert(\n    ids=[\"doc-1\"],\n    embeddings=[[0.1, 0.2, ..., 0.5]],  # 1536-dim vector\n    texts=[\"Document text\"],\n    metadatas=[{\"source\": \"custom\"}]\n)\n</code></pre>"},{"location":"guides/vector_stores/#updating-documents","title":"Updating Documents","text":"Python<pre><code># Update existing document (same ID)\nvector_store.upsert(\n    ids=[\"doc-1\"],\n    texts=[\"Updated content\"],\n    metadatas=[{\"source\": \"pdf\", \"updated\": True}]\n)\n</code></pre>"},{"location":"guides/vector_stores/#deleting-documents","title":"Deleting Documents","text":"Python<pre><code># Delete by ID\nvector_store.delete(ids=[\"doc-1\", \"doc-2\"])\n\n# Delete by filter\nvector_store.delete(filter={\"source\": \"old_data\"})\n\n# Delete all\nvector_store.delete(filter={})  # \u26a0\ufe0f Careful!\n</code></pre>"},{"location":"guides/vector_stores/#retrieving-documents","title":"Retrieving Documents","text":"Python<pre><code># Get by ID\ndocuments = vector_store.get(ids=[\"doc-1\", \"doc-2\"])\n\n# Get all documents\nall_docs = vector_store.get()\n\n# Get with metadata filter\nfiltered = vector_store.get(\n    filter={\"source\": \"research_papers\"}\n)\n</code></pre>"},{"location":"guides/vector_stores/#search","title":"Search","text":""},{"location":"guides/vector_stores/#basic-vector-search","title":"Basic Vector Search","text":"Python<pre><code># Semantic search\nresults = vector_store.search(\n    query=\"What is machine learning?\",\n    limit=5\n)\n\nfor result in results:\n    print(f\"Score: {result.score:.4f}\")\n    print(f\"Text: {result.text[:100]}...\")\n    print(f\"Metadata: {result.metadata}\\n\")\n</code></pre>"},{"location":"guides/vector_stores/#search-with-metadata-filtering","title":"Search with Metadata Filtering","text":"Python<pre><code># Filter by metadata\nresults = vector_store.search(\n    query=\"neural networks\",\n    limit=10,\n    filter={\n        \"source\": \"textbook.pdf\",\n        \"chapter\": 3\n    }\n)\n\n# Complex filters\nresults = vector_store.search(\n    query=\"deep learning\",\n    limit=5,\n    filter={\n        \"year\": {\"$gte\": 2020},  # &gt;= 2020\n        \"category\": {\"$in\": [\"AI\", \"ML\"]},  # In list\n        \"verified\": True\n    }\n)\n</code></pre>"},{"location":"guides/vector_stores/#search-with-custom-parameters","title":"Search with Custom Parameters","text":"Python<pre><code># Adjust search parameters for speed/accuracy trade-off\nresults = vector_store.search(\n    query=\"transformer models\",\n    limit=10,\n    search_params={\n        \"ef\": 64,  # HNSW: higher = more accurate but slower\n    }\n)\n</code></pre>"},{"location":"guides/vector_stores/#batch-search","title":"Batch Search","text":"Python<pre><code># Search multiple queries at once\nqueries = [\n    \"What is supervised learning?\",\n    \"Explain neural networks\",\n    \"Describe gradient descent\"\n]\n\nall_results = vector_store.batch_search(\n    queries=queries,\n    limit=5\n)\n\nfor i, results in enumerate(all_results):\n    print(f\"\\nQuery {i+1}: {queries[i]}\")\n    for result in results:\n        print(f\"  - {result.text[:50]}... (score: {result.score:.3f})\")\n</code></pre>"},{"location":"guides/vector_stores/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/vector_stores/#hybrid-search-vector-keyword","title":"Hybrid Search (Vector + Keyword)","text":"<p>Combine semantic search with keyword matching:</p> Python<pre><code># Vector search with keyword boost\nresults = vector_store.hybrid_search(\n    query=\"machine learning algorithms\",\n    keywords=[\"neural network\", \"deep learning\"],\n    limit=10,\n    alpha=0.7,  # 0.7 vector + 0.3 keyword\n)\n</code></pre>"},{"location":"guides/vector_stores/#reranking","title":"Reranking","text":"<p>Improve result quality with reranking:</p> Python<pre><code>from rag_toolkit.rag.rerankers import CrossEncoderReranker\n\n# Initial search with more results\nresults = vector_store.search(\n    query=\"transformer architecture\",\n    limit=50  # Retrieve more candidates\n)\n\n# Rerank to get best 10\nreranker = CrossEncoderReranker(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\nreranked = reranker.rerank(\n    query=\"transformer architecture\",\n    documents=results,\n    top_k=10\n)\n</code></pre>"},{"location":"guides/vector_stores/#dynamic-schema","title":"Dynamic Schema","text":"<p>Store arbitrary metadata without predefined schema:</p> Python<pre><code># Each document can have different metadata fields\nvector_store.upsert(\n    texts=[\n        \"Document 1\",\n        \"Document 2\",\n        \"Document 3\"\n    ],\n    metadatas=[\n        {\"source\": \"pdf\", \"pages\": 10, \"author\": \"Smith\"},\n        {\"source\": \"web\", \"url\": \"example.com\"},  # Different fields\n        {\"source\": \"book\", \"isbn\": \"123\", \"chapter\": 5}  # More fields\n    ]\n)\n\n# Filter by any field\nresults = vector_store.search(\n    query=\"important topic\",\n    filter={\"author\": \"Smith\"}\n)\n</code></pre>"},{"location":"guides/vector_stores/#partitions","title":"Partitions","text":"<p>Organize data into logical partitions:</p> Python<pre><code># Create partitions for different data types\nvector_store.create_partition(\"research_papers\")\nvector_store.create_partition(\"blog_posts\")\n\n# Insert into specific partition\nvector_store.upsert(\n    texts=[\"Research paper content\"],\n    partition=\"research_papers\"\n)\n\n# Search in specific partition\nresults = vector_store.search(\n    query=\"latest findings\",\n    partition=\"research_papers\",\n    limit=5\n)\n</code></pre>"},{"location":"guides/vector_stores/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/vector_stores/#batch-operations","title":"Batch Operations","text":"<p>Process documents in batches for better performance:</p> Python<pre><code># Batch insert (recommended for large datasets)\nbatch_size = 100\nfor i in range(0, len(documents), batch_size):\n    batch = documents[i:i+batch_size]\n    vector_store.upsert(\n        texts=[doc.text for doc in batch],\n        metadatas=[doc.metadata for doc in batch]\n    )\n</code></pre>"},{"location":"guides/vector_stores/#index-building","title":"Index Building","text":"<p>Build index after bulk inserts:</p> Python<pre><code># Insert without building index\nvector_store.upsert(\n    texts=large_document_list,\n    build_index=False  # Skip index building\n)\n\n# Build index once after all inserts\nawait vector_store.build_index()\n</code></pre>"},{"location":"guides/vector_stores/#connection-pooling","title":"Connection Pooling","text":"<p>Reuse connections for better performance:</p> Python<pre><code># Connection pool automatically managed\nvector_store = MilvusVectorStore(\n    collection_name=\"docs\",\n    pool_size=10,  # Connection pool size\n)\n</code></pre>"},{"location":"guides/vector_stores/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"guides/vector_stores/#collection-statistics","title":"Collection Statistics","text":"Python<pre><code># Get collection info\nstats = await vector_store.get_collection_stats()\nprint(f\"Document count: {stats.count}\")\nprint(f\"Dimension: {stats.dimension}\")\nprint(f\"Metric type: {stats.metric}\")\n\n# Get collection size\nsize_mb = await vector_store.get_collection_size()\nprint(f\"Collection size: {size_mb:.2f} MB\")\n</code></pre>"},{"location":"guides/vector_stores/#compaction","title":"Compaction","text":"<p>Optimize storage by compacting deleted data:</p> Python<pre><code># Compact collection (reclaim space from deleted documents)\nawait vector_store.compact()\n</code></pre>"},{"location":"guides/vector_stores/#index-statistics","title":"Index Statistics","text":"Python<pre><code># Get index information\nindex_info = await vector_store.get_index_info()\nprint(f\"Index type: {index_info.type}\")\nprint(f\"Index params: {index_info.params}\")\n</code></pre>"},{"location":"guides/vector_stores/#implementing-custom-vector-stores","title":"Implementing Custom Vector Stores","text":"<p>You can implement your own vector store by following the <code>VectorStoreClient</code> protocol:</p> Python<pre><code>from typing import Protocol, runtime_checkable\nfrom rag_toolkit.core.vectorstore import VectorStoreClient\nfrom rag_toolkit.core.types import SearchResult\n\n@runtime_checkable\nclass CustomVectorStore(VectorStoreClient):\n    \"\"\"Custom vector store implementation.\"\"\"\n\n    async def upsert(\n        self,\n        ids: list[str] | None = None,\n        texts: list[str] | None = None,\n        embeddings: list[list[float]] | None = None,\n        metadatas: list[dict] | None = None,\n    ) -&gt; list[str]:\n        \"\"\"Insert or update documents.\"\"\"\n        # Your implementation\n        pass\n\n    async def search(\n        self,\n        query: str | list[float],\n        limit: int = 5,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Search for similar documents.\"\"\"\n        # Your implementation\n        pass\n\n    async def delete(\n        self,\n        ids: list[str] | None = None,\n        filter: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Delete documents.\"\"\"\n        # Your implementation\n        pass\n\n    async def get(\n        self,\n        ids: list[str] | None = None,\n        filter: dict | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Retrieve documents.\"\"\"\n        # Your implementation\n        pass\n</code></pre> <p>See Custom Vector Store Example for a complete implementation.</p>"},{"location":"guides/vector_stores/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/vector_stores/#connection-issues","title":"Connection Issues","text":"Python<pre><code># Check connection\ntry:\n    await vector_store.health_check()\n    print(\"\u2705 Connected to Milvus\")\nexcept Exception as e:\n    print(f\"\u274c Connection failed: {e}\")\n</code></pre>"},{"location":"guides/vector_stores/#collection-not-found","title":"Collection Not Found","text":"Python<pre><code># Check if collection exists before operations\nif not await vector_store.collection_exists():\n    await vector_store.create_collection(dimension=1536)\n</code></pre>"},{"location":"guides/vector_stores/#dimension-mismatch","title":"Dimension Mismatch","text":"Python<pre><code># Ensure embedding dimension matches collection dimension\nembedding_dim = vector_store.embedding_client.dimension\ncollection_dim = vector_store.dimension\n\nif embedding_dim != collection_dim:\n    raise ValueError(\n        f\"Dimension mismatch: embeddings={embedding_dim}, \"\n        f\"collection={collection_dim}\"\n    )\n</code></pre>"},{"location":"guides/vector_stores/#out-of-memory","title":"Out of Memory","text":"Python<pre><code># For large datasets, use batch processing\nbatch_size = 100  # Reduce if still OOM\n\n# Or use memory-efficient index\nfrom rag_toolkit.infra.vectorstores.milvus.config import IndexConfig\n\nindex_config = IndexConfig(\n    index_type=\"IVF_PQ\",  # More memory efficient\n    params={\"nlist\": 1024, \"m\": 8, \"nbits\": 8}\n)\n</code></pre>"},{"location":"guides/vector_stores/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Metric</li> <li>Use <code>COSINE</code> for text embeddings (default)</li> <li>Use <code>L2</code> for image embeddings</li> <li> <p>Use <code>IP</code> if embeddings are pre-normalized</p> </li> <li> <p>Index Selection</p> </li> <li><code>HNSW</code>: Best accuracy, more memory</li> <li><code>IVF_FLAT</code>: Balanced</li> <li> <p><code>IVF_PQ</code>: Memory efficient for large datasets</p> </li> <li> <p>Batch Operations</p> </li> <li>Insert documents in batches (100-1000)</li> <li>Build index after bulk inserts</li> <li> <p>Use async operations for better performance</p> </li> <li> <p>Metadata Design</p> </li> <li>Keep metadata small and relevant</li> <li>Use consistent field names</li> <li> <p>Index frequently filtered fields</p> </li> <li> <p>Collection Management</p> </li> <li>One collection per logical dataset</li> <li>Use partitions for data organization</li> <li> <p>Regular compaction for space efficiency</p> </li> <li> <p>Monitoring</p> </li> <li>Check collection stats regularly</li> <li>Monitor query latency</li> <li>Set up alerts for errors</li> </ol>"},{"location":"guides/vector_stores/#next-steps","title":"Next Steps","text":"<ul> <li>Embeddings Guide - Learn about embedding clients</li> <li>RAG Pipeline - Build complete RAG systems</li> <li>Custom Vector Store Example - Implement your own</li> <li>Production Setup - Deploy to production</li> </ul>"},{"location":"guides/vector_stores/#see-also","title":"See Also","text":"<ul> <li>Milvus Documentation</li> <li>Vector Store Protocol</li> <li>Search Strategies</li> </ul>"},{"location":"tools/","title":"Tools","text":""},{"location":"tools/#tools","title":"Tools","text":"<p>Production-Ready Utilities</p> <p>Professional tools for migration, benchmarking, and performance optimization of your RAG applications.</p>"},{"location":"tools/#featured-tools","title":"Featured Tools","text":"<ul> <li> <p> Vector Store Migration</p> <p>Transfer vector data seamlessly between Milvus, Qdrant, and ChromaDB with validation and progress tracking.</p> <p> Start Migrating</p> </li> <li> <p> Performance Benchmarks</p> <p>Comprehensive benchmark suite to measure and compare vector store performance.</p> <p> Run Benchmarks</p> </li> <li> <p> Benchmark Results</p> <p>Real-world performance comparison with detailed metrics and insights.</p> <p> View Results</p> </li> </ul>"},{"location":"tools/#tool-categories","title":"Tool Categories","text":"\ud83d\udd04 Migration\ud83d\udcca Benchmarking\ud83c\udfaf Performance <ul> <li> <p> Data Migration</p> <p>Transfer vectors between different stores</p> </li> <li> <p> Filtered Migration</p> <p>Migrate subsets based on metadata</p> </li> <li> <p> Migration Estimation</p> <p>Plan migrations before execution</p> </li> <li> <p> Data Validation</p> <p>Ensure integrity during transfer</p> </li> </ul> <ul> <li> <p> Insert Benchmarks</p> <p>Measure vector insertion performance</p> </li> <li> <p> Search Benchmarks</p> <p>Test similarity search speed</p> </li> <li> <p> Scale Tests</p> <p>Performance at scale (10K+ vectors)</p> </li> <li> <p> HTML Reports</p> <p>Beautiful visual reports with charts</p> </li> </ul> <ul> <li> <p> Store Comparison</p> <p>Compare Milvus, Qdrant, and ChromaDB</p> </li> <li> <p> Best Practices</p> <p>Optimize batch sizes and retries</p> </li> <li> <p> Real Metrics</p> <p>Production-ready insights</p> </li> <li> <p> Configuration</p> <p>Fine-tune benchmark settings</p> </li> </ul>"},{"location":"tools/#quick-start","title":"Quick Start","text":""},{"location":"tools/#migration-tool","title":"Migration Tool","text":"<p>Migrate Between Vector Stores</p> <p>Transfer data with automatic validation and progress tracking.</p> InstallBasic MigrationWith Progress install.sh<pre><code># Install RAG Toolkit\npip install rag-toolkit\n\n# With all vector stores\npip install rag-toolkit[all]\n</code></pre> simple_migration.py<pre><code>from rag_toolkit.migration import VectorStoreMigrator\nfrom rag_toolkit.infra.vectorstores import (\n    get_chromadb_service,\n    get_qdrant_service,\n)\n\n# Initialize stores\nsource = get_chromadb_service(host=\"localhost\")\ntarget = get_qdrant_service(host=\"localhost\")\n\n# Create migrator\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    validate=True,\n)\n\n# Run migration\nresult = migrator.migrate(\n    source_collection=\"my_documents\",\n    target_collection=\"my_documents\",\n    batch_size=1000,\n)\n\nprint(f\"Success: {result.success}\")\nprint(f\"Migrated: {result.vectors_migrated}\")\n</code></pre> progress_migration.py<pre><code>def on_progress(progress):\n    print(\n        f\"Progress: {progress.percentage:.1f}% \"\n        f\"ETA: {progress.eta_seconds:.0f}s\"\n    )\n\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    on_progress=on_progress,\n)\n\nresult = migrator.migrate(\n    source_collection=\"docs\",\n    batch_size=500,\n)\n</code></pre>"},{"location":"tools/#benchmark-suite","title":"Benchmark Suite","text":"<p>Measure Performance</p> <p>Run comprehensive benchmarks across all vector stores.</p> Run All TestsSpecific CategoryView Results benchmark.sh<pre><code># Start vector stores\ndocker-compose up -d\n\n# Run complete benchmark suite\nmake benchmark\n\n# Generate HTML report\nmake benchmark-report\n</code></pre> category_benchmark.sh<pre><code># Run only insert benchmarks\npytest tests/benchmarks/test_insert_benchmark.py -v\n\n# Run only search benchmarks\npytest tests/benchmarks/test_search_benchmark.py -v\n\n# Run scale tests\npytest tests/benchmarks/test_scale_benchmark.py -v\n</code></pre> view_results.sh<pre><code># Open interactive HTML report\nopen benchmark_report.html\n\n# Or view in docs\nmkdocs serve\n# Navigate to Tools &gt; Benchmark Results\n</code></pre>"},{"location":"tools/#key-features","title":"Key Features","text":""},{"location":"tools/#vector-store-migration","title":"Vector Store Migration","text":"<ul> <li> <p> Validated Transfers</p> <p>Automatic data integrity verification</p> </li> <li> <p> Real-Time Progress</p> <p>Track migration with ETA calculation</p> </li> <li> <p> Retry Logic</p> <p>Exponential backoff for transient failures</p> </li> <li> <p> Dry-Run Mode</p> <p>Test migrations without writing data</p> </li> <li> <p> Metadata Filtering</p> <p>Migrate only relevant subsets</p> </li> <li> <p> Estimation</p> <p>Predict time and resource requirements</p> </li> </ul>"},{"location":"tools/#performance-benchmarks","title":"Performance Benchmarks","text":"<ul> <li> <p> 30 Benchmark Tests</p> <p>Comprehensive test coverage</p> </li> <li> <p> Automatic Batching</p> <p>Handle large datasets effortlessly</p> </li> <li> <p> Visual Reports</p> <p>Interactive HTML with Chart.js</p> </li> <li> <p> Store Comparison</p> <p>Side-by-side performance metrics</p> </li> <li> <p> Configurable</p> <p>Adjust iterations and parameters</p> </li> <li> <p> Docker Integration</p> <p>Easy setup with docker-compose</p> </li> </ul>"},{"location":"tools/#common-use-cases","title":"Common Use Cases","text":""},{"location":"tools/#development-to-production","title":"Development to Production","text":"<p>Migrate from local development to production:</p> Python<pre><code># Dev: ChromaDB\ndev_store = get_chromadb_service(host=\"localhost\")\n\n# Prod: Qdrant Cloud\nprod_store = get_qdrant_service(\n    host=\"production.qdrant.com\",\n    api_key=os.getenv(\"QDRANT_API_KEY\"),\n)\n\nmigrator = VectorStoreMigrator(\n    source=dev_store,\n    target=prod_store,\n    validate=True,\n)\n\nresult = migrator.migrate(\n    source_collection=\"dev_documents\",\n    target_collection=\"prod_documents\",\n)\n</code></pre>"},{"location":"tools/#performance-testing","title":"Performance Testing","text":"<p>Compare vector stores for your workload:</p> Bash<pre><code># Start all stores\ndocker-compose up -d\n\n# Run benchmarks\nmake benchmark\n\n# Generate report\nmake benchmark-report\n\n# View results\nopen benchmark_report.html\n</code></pre>"},{"location":"tools/#backup-restore","title":"Backup &amp; Restore","text":"<p>Create vector data backups:</p> Python<pre><code>from datetime import datetime\n\n# Backup\nbackup_migrator = VectorStoreMigrator(\n    source=qdrant_service,\n    target=chromadb_backup,\n)\n\nbackup_result = backup_migrator.migrate(\n    source_collection=\"critical_data\",\n    target_collection=f\"backup_{datetime.now().isoformat()}\",\n)\n\n# Restore later\nrestore_migrator = VectorStoreMigrator(\n    source=chromadb_backup,\n    target=qdrant_service,\n)\n\nrestore_result = restore_migrator.migrate(\n    source_collection=\"backup_2026_01_20\",\n    target_collection=\"critical_data_restored\",\n)\n</code></pre>"},{"location":"tools/#ab-testing","title":"A/B Testing","text":"<p>Test different vector stores:</p> Python<pre><code>stores = [\n    (\"Milvus\", get_milvus_service()),\n    (\"Qdrant\", get_qdrant_service()),\n    (\"ChromaDB\", get_chromadb_service()),\n]\n\nfor name, store in stores:\n    migrator = VectorStoreMigrator(source=baseline, target=store)\n\n    result = migrator.migrate(\n        source_collection=\"test_data\",\n        batch_size=1000,\n    )\n\n    print(f\"{name}: {result.duration_seconds:.2f}s\")\n</code></pre>"},{"location":"tools/#documentation","title":"Documentation","text":"<ul> <li> <p> Migration Guide</p> <p>Complete migration tool documentation</p> </li> <li> <p> Benchmark Guide</p> <p>Running and configuring benchmarks</p> </li> <li> <p> Results Analysis</p> <p>Understanding benchmark metrics</p> </li> </ul>"},{"location":"tools/#need-help","title":"Need Help?","text":"<ul> <li> <p> User Guide</p> <p>Comprehensive documentation and tutorials</p> </li> <li> <p> Examples</p> <p>Practical code examples</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation</p> </li> <li> <p> Discussions</p> <p>Ask questions and share ideas</p> </li> <li> <p> Report Issues</p> <p>Found a bug? Let us know!</p> </li> </ul>"},{"location":"tools/benchmark_results/","title":"Benchmark Results","text":""},{"location":"tools/benchmark_results/#benchmark-results","title":"Benchmark Results","text":"\ud83d\ude80 Vector Store Performance Benchmarks <p>Real-world performance comparison across Milvus, Qdrant, and ChromaDB</p>"},{"location":"tools/benchmark_results/#summary","title":"Summary","text":"Total Benchmarks 29 Fastest Operation 0.64 ms Slowest Operation 222.7 s Average Time 10.1 s"},{"location":"tools/benchmark_results/#key-findings","title":"Key Findings","text":""},{"location":"tools/benchmark_results/#insert-performance","title":"Insert Performance","text":"Single InsertBatch Insert (100 vectors)Large Batch (1K vectors) Store Mean Time Ops/Sec Qdrant \ud83e\udd47 1.19 ms 843 ops/s ChromaDB \ud83e\udd48 1.64 ms 609 ops/s Milvus 11,087 ms 0.09 ops/s Store Mean Time Ops/Sec ChromaDB \ud83e\udd47 6.46 ms 155 ops/s Qdrant \ud83e\udd48 25.53 ms 39 ops/s Milvus 11,082 ms 0.09 ops/s Store Mean Time Ops/Sec ChromaDB \ud83e\udd47 46.53 ms 21 ops/s Qdrant \ud83e\udd48 260.07 ms 3.85 ops/s Milvus 22,110 ms 0.05 ops/s"},{"location":"tools/benchmark_results/#search-performance","title":"Search Performance","text":"Top-1 SearchTop-10 SearchTop-100 Search Store Mean Time Ops/Sec ChromaDB \ud83e\udd47 0.64 ms 1551 ops/s Qdrant \ud83e\udd48 1.41 ms 711 ops/s Milvus 2.27 ms 440 ops/s Store Mean Time Ops/Sec ChromaDB \ud83e\udd47 0.90 ms 1108 ops/s Milvus \ud83e\udd48 2.30 ms 435 ops/s Qdrant 2.75 ms 364 ops/s Store Mean Time Ops/Sec Milvus \ud83e\udd47 2.16 ms 462 ops/s ChromaDB \ud83e\udd48 2.99 ms 334 ops/s Qdrant 8.78 ms 114 ops/s"},{"location":"tools/benchmark_results/#scale-performance-10k-vectors","title":"Scale Performance (10K vectors)","text":"Store Insert Time Search Time Total ChromaDB \ud83e\udd47 601 ms 1.02 ms 602 ms Qdrant \ud83e\udd48 3,178 ms 3.29 ms 3,181 ms Milvus 222,734 ms 2.40 ms 222,736 ms"},{"location":"tools/benchmark_results/#insights","title":"Insights","text":"<p>ChromaDB: Best for Rapid Prototyping</p> <ul> <li>Fastest single and batch inserts</li> <li>Lowest latency searches (0.64ms)</li> <li>Best scale performance (10K in 601ms)</li> <li>Ideal for: Development, testing, small to medium datasets</li> </ul> <p>Qdrant: Balanced Performance</p> <ul> <li>Good insert performance (1.19ms single)</li> <li>Consistent search latency</li> <li>Reliable at scale (3.2s for 10K)</li> <li>Ideal for: Production, medium to large datasets, distributed deployments</li> </ul> <p>Milvus: Specialized Use Cases</p> <ul> <li>Slow inserts due to flush requirements (11s)</li> <li>Competitive search performance (2.27ms)</li> <li>Not ideal for real-time applications</li> <li>Ideal for: Batch processing, large-scale analytics, read-heavy workloads</li> </ul>"},{"location":"tools/benchmark_results/#methodology","title":"Methodology","text":""},{"location":"tools/benchmark_results/#test-environment","title":"Test Environment","text":"<ul> <li>Hardware: MacBook Air M1</li> <li>Python: 3.12.12</li> <li>Benchmark Tool: pytest-benchmark 5.2.3</li> <li>Iterations: 5-10 rounds per test</li> <li>Date: December 25, 2025</li> </ul>"},{"location":"tools/benchmark_results/#benchmark-categories","title":"Benchmark Categories","text":"<ul> <li>Insert Benchmarks: Single, batch (100), large batch (1K)</li> <li>Search Benchmarks: Top-1, Top-10, Top-100</li> <li>Batch Operations: Bulk insert/delete cycles</li> <li>Scale Tests: 10K vector operations</li> </ul>"},{"location":"tools/benchmark_results/#vector-configuration","title":"Vector Configuration","text":"<ul> <li>Dimension: 384 (nomic-embed-text)</li> <li>Metric: Cosine similarity</li> <li>Batch Size: 500 (for large operations)</li> </ul>"},{"location":"tools/benchmark_results/#interactive-report","title":"Interactive Report","text":"<p>For detailed results with charts and full statistics, generate the benchmark report locally using <code>make benchmark-report</code>.</p> <p>The interactive report includes:</p> <ul> <li>\ud83d\udcca Bar charts for visual comparison</li> <li>\ud83d\udcc8 Detailed timing statistics</li> <li>\ud83d\udcc9 Min/Max/StdDev metrics</li> <li>\ud83d\udd0d Filterable results</li> </ul>"},{"location":"tools/benchmark_results/#running-benchmarks","title":"Running Benchmarks","text":"<p>To reproduce these results:</p> Bash<pre><code># Start vector stores\ndocker-compose up -d\n\n# Run benchmarks\nmake benchmark\n\n# Generate report\nmake benchmark-report\n</code></pre> <p>See Benchmarks Guide for detailed instructions.</p>"},{"location":"tools/benchmark_results/#recommendations","title":"Recommendations","text":""},{"location":"tools/benchmark_results/#for-development","title":"For Development","text":"<p>Use ChromaDB for fast iteration:</p> Python<pre><code>store = get_chromadb_service(host=\"localhost\")\n</code></pre>"},{"location":"tools/benchmark_results/#for-production","title":"For Production","text":"<p>Use Qdrant for reliability:</p> Python<pre><code>store = get_qdrant_service(\n    host=\"qdrant.example.com\",\n    port=6333,\n    api_key=os.getenv(\"QDRANT_API_KEY\")\n)\n</code></pre>"},{"location":"tools/benchmark_results/#for-analytics","title":"For Analytics","text":"<p>Use Milvus for batch processing:</p> Python<pre><code>store = get_milvus_service(\n    host=\"milvus.example.com\",\n    port=19530\n)\n</code></pre>"},{"location":"tools/benchmark_results/#version-information","title":"Version Information","text":"<ul> <li>RAG Toolkit: 0.1.0</li> <li>Milvus: 2.3+</li> <li>Qdrant: 1.7+</li> <li>ChromaDB: 0.4+</li> </ul> <p>Last updated: January 5, 2026</p>"},{"location":"tools/benchmarks/","title":"Benchmarks","text":""},{"location":"tools/benchmarks/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Comprehensive benchmark suite to measure and compare RAG Toolkit performance across different vector store implementations.</p>"},{"location":"tools/benchmarks/#overview","title":"Overview","text":"<p>Benchmark Suite Features</p> <ul> <li> <p> 30 Benchmark Tests</p> <p>Across 4 categories for comprehensive evaluation</p> </li> <li> <p> Unified API</p> <p>Through <code>VectorStoreWrapper</code> for consistent testing</p> </li> <li> <p> Automatic Batching</p> <p>For large datasets without manual chunking</p> </li> <li> <p> HTML Reports</p> <p>With Chart.js visualizations and metrics</p> </li> <li> <p> Comparative Analysis</p> <p>Across Milvus, Qdrant, and ChromaDB</p> </li> </ul> <p>View Latest Results</p> <p>Run <code>make benchmark-report</code> to generate the interactive HTML report with complete benchmark data and charts.</p> <p>Local Viewing</p> <p>After generating, open the <code>benchmark_report.html</code> file directly in your browser.</p>"},{"location":"tools/benchmarks/#benchmark-categories","title":"Benchmark Categories","text":"<p>Four Test Categories</p>"},{"location":"tools/benchmarks/#1-insert-benchmarks-9-tests","title":"1.  Insert Benchmarks (9 tests)","text":"<p>Vector Insertion Performance</p> <p>Measure how fast each store can insert vectors.</p> <p>Test Scenarios:</p> <ul> <li> Single insert (1 vector)</li> <li> Batch insert (100 vectors)</li> <li> Large batch insert (1,000 vectors)</li> </ul> <p>Tested on: Milvus, Qdrant, ChromaDB</p>"},{"location":"tools/benchmarks/#2-search-benchmarks-9-tests","title":"2.  Search Benchmarks (9 tests)","text":"<p>Similarity Search Performance</p> <p>Measure vector similarity search speed and accuracy.</p> <p>Test Scenarios:</p> <ul> <li> Top-1 search (find closest match)</li> <li> Top-10 search</li> <li> Top-100 search</li> </ul> <p>Tested on: Milvus, Qdrant, ChromaDB</p>"},{"location":"tools/benchmarks/#3-batch-operations-6-tests","title":"3.  Batch Operations (6 tests)","text":"<p>Complex Operation Performance</p> <p>Measure performance of compound operations.</p> <p>Test Scenarios:</p> <ul> <li> Bulk insert + delete cycles</li> <li> Insert + search cycles</li> </ul> <p>Tested on: Milvus, Qdrant, ChromaDB</p>"},{"location":"tools/benchmarks/#4-scale-benchmarks-6-tests","title":"4.  Scale Benchmarks (6 tests)","text":"<p>Performance at Scale</p> <p>Measure how stores handle large-scale operations.</p> <p>Test Scenarios:</p> <ul> <li> 10K vector insertion</li> <li> Search in large databases (10K+ vectors)</li> </ul> <p>Tested on: Milvus, Qdrant, ChromaDB</p>"},{"location":"tools/benchmarks/#running-benchmarks","title":"Running Benchmarks","text":""},{"location":"tools/benchmarks/#quick-start","title":"Quick Start","text":"All BenchmarksSpecific CategoryCustom Iterations Bash<pre><code># Run complete benchmark suite\nmake benchmark\n</code></pre> Bash<pre><code># Run only insert benchmarks\npytest tests/benchmarks/test_insert_benchmark.py -v\n\n# Run only search benchmarks\npytest tests/benchmarks/test_search_benchmark.py -v\n</code></pre> Bash<pre><code># Run with 10 iterations per test\npytest tests/benchmarks/ --benchmark-min-rounds=10\n\n# Run with custom warmup\npytest tests/benchmarks/ --benchmark-warmup=on\n</code></pre>"},{"location":"tools/benchmarks/#generate-html-report","title":"Generate HTML Report","text":"<p>Visual Reports</p> <p>Generate beautiful HTML reports with interactive charts.</p> Generate Report<pre><code>make benchmark-report\n</code></pre> <p>Report Contents:</p> <ul> <li> <p> Summary Statistics</p> <p>Min, max, mean, median for all tests</p> </li> <li> <p> Comparative Charts</p> <p>Bar charts comparing all vector stores</p> </li> <li> <p> Detailed Tables</p> <p>Timing breakdowns with color coding</p> </li> <li> <p> Color-Coded Stores</p> <p>Easy visual identification</p> </li> </ul> <p>Viewing the Report</p> <p>The report opens automatically in your browser, or manually open <code>benchmark_report.html</code>.</p> <p>Integration with Documentation:</p> Step 1: GenerateStep 2: Copy to DocsStep 3: Rebuild DocsStep 4: Access Bash<pre><code>make benchmark-report\n</code></pre> Bash<pre><code>cp benchmark_report.html docs/_static/\n</code></pre> Bash<pre><code>cd docs &amp;&amp; make html\n</code></pre> <p>Report available at: <code>&lt;your-docs-url&gt;/_static/benchmark_report.html</code></p> <p>Add Link to Documentation</p> Markdown<pre><code>View the latest [benchmark results](../_static/benchmark_report.html).\n</code></pre>"},{"location":"tools/benchmarks/#compare-results","title":"Compare Results","text":"Bash<pre><code># Compare current vs previous results\nmake benchmark-compare\n</code></pre>"},{"location":"tools/benchmarks/#clean-results","title":"Clean Results","text":"Bash<pre><code># Remove all benchmark data\nmake benchmark-clean\n</code></pre>"},{"location":"tools/benchmarks/#benchmark-configuration","title":"Benchmark Configuration","text":""},{"location":"tools/benchmarks/#environment-setup","title":"Environment Setup","text":"<p>Prerequisites</p> <p>Ensure all vector stores are running before benchmarking.</p> MilvusQdrantChromaDB Bash<pre><code># Start Milvus (port 19530)\ndocker run -d --name milvus \\\n  -p 19530:19530 \\\n  milvusdb/milvus:latest standalone\n</code></pre> <p>Check status: </p>Bash<pre><code>curl http://localhost:19530/healthz\n</code></pre><p></p> Bash<pre><code># Start Qdrant (port 6333)\ndocker run -d --name qdrant \\\n  -p 6333:6333 \\\n  qdrant/qdrant:latest\n</code></pre> <p>Check status: </p>Bash<pre><code>curl http://localhost:6333/health\n</code></pre><p></p> Bash<pre><code># ChromaDB runs in-process\n# No Docker setup required\npip install chromadb\n</code></pre> <p>Runs automatically with sufficient RAM</p>"},{"location":"tools/benchmarks/#python-environment","title":"Python Environment","text":"Install Dependencies<pre><code># Install benchmark dependencies\npip install pytest-benchmark&gt;=4.0.0\n\n# Verify installation\npytest --version\n</code></pre>"},{"location":"tools/benchmarks/#architecture","title":"Architecture","text":""},{"location":"tools/benchmarks/#vectorstorewrapper","title":"VectorStoreWrapper","text":"<p>Unified API</p> <p>Abstract API differences across vector stores.</p> wrapper_usage.py<pre><code>from tests.benchmarks.utils.wrapper import VectorStoreWrapper\n\n# Initialize wrapper\nwrapper = VectorStoreWrapper(\n    service=milvus_service,\n    collection_name=\"benchmark_collection\",\n    store_type=\"milvus\"\n)\n\n# Unified API (works for all stores)\nwrapper.add_vectors(data)\nwrapper.search(query_vector, top_k=10)\nwrapper.delete_vectors(ids)\nwrapper.count()\n</code></pre>"},{"location":"tools/benchmarks/#automatic-batching","title":"Automatic Batching","text":"<p>Smart Batching</p> <p>Large operations are automatically batched to avoid payload limits.</p> auto_batching.py<pre><code># 10K vectors automatically batched in chunks of 500\ndata = generator.generate_data(10000)\nwrapper.add_vectors(data)  # Internally: 20 batches of 500\n</code></pre>"},{"location":"tools/benchmarks/#data-generation","title":"Data Generation","text":"<p>Reproducible Test Data</p> <p>Generate test data with configurable dimensions.</p> data_generation.py<pre><code>from tests.benchmarks.utils.data_generator import VectorDataGenerator\n\ngenerator = VectorDataGenerator(dimension=384, seed=42)\n\n# Generate store-specific formats\nmilvus_data = generator.generate_milvus_data(100)\nqdrant_data = generator.generate_qdrant_points(100)\nchroma_data = generator.generate_chroma_data(100)\n</code></pre>"},{"location":"tools/benchmarks/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tools/benchmarks/#performance-metrics","title":"Performance Metrics","text":"<p>Benchmark Statistics</p> <p>Each benchmark reports comprehensive timing statistics.</p> Metric Description Min/Max/Mean Timing statistics StdDev Consistency measure Median Typical performance IQR Variability indicator OPS Operations per second Rounds/Iterations Test repetitions"},{"location":"tools/benchmarks/#example-output","title":"Example Output","text":"Benchmark Results<pre><code>------------------------ benchmark 'insert': 6 tests -------------------------\nName (time in ms)                     Min       Max      Mean    StdDev    Median\n----------------------------------------------------------------------------------\ntest_qdrant_single_insert          1.0323    4.4449    1.2977    0.4554    1.1683\ntest_qdrant_batch_insert_100      23.9068   68.0399   28.0110    8.4520   25.6845\ntest_milvus_single_insert      11067.0126 11151.9131 11094.9597   33.1737 11083.0722\n</code></pre>"},{"location":"tools/benchmarks/#performance-comparison","title":"Performance Comparison","text":"<p>Expected Relative Performance</p> <ul> <li> <p> Insert</p> <p>Qdrant &gt; ChromaDB &gt; Milvus</p> </li> <li> <p> Search</p> <p>Milvus \u2248 Qdrant &gt; ChromaDB</p> </li> <li> <p> Scale</p> <p>Qdrant &gt; Milvus &gt; ChromaDB</p> </li> </ul> <p>Hardware Dependent</p> <p>Actual performance depends on hardware, configuration, and data characteristics.</p>"},{"location":"tools/benchmarks/#known-limitations","title":"Known Limitations","text":""},{"location":"tools/benchmarks/#payload-size-limits","title":"Payload Size Limits","text":"Store Limit Benchmark Strategy Qdrant 33MB per request Automatic batching at 500 vectors Milvus No practical limit Direct batch operations ChromaDB In-memory Scales with available RAM"},{"location":"tools/benchmarks/#flush-requirements","title":"Flush Requirements","text":"MilvusQdrantChromaDB Python<pre><code># Requires explicit flush for immediate availability\nservice.flush(collection_name)\n</code></pre> Python<pre><code># Automatic background indexing\n# No manual flush needed\n</code></pre> Python<pre><code># Immediate consistency\n# Data available instantly\n</code></pre>"},{"location":"tools/benchmarks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tools/benchmarks/#tests-hanging","title":"Tests Hanging","text":"<p>Connectivity Issues</p> <p>Check vector store connectivity if tests hang.</p> MilvusQdrantChromaDB Bash<pre><code># Test Milvus health\ncurl http://localhost:19530/healthz\n</code></pre> Bash<pre><code># Test Qdrant health\ncurl http://localhost:6333/health\n</code></pre> Bash<pre><code># Check process memory\nps aux | grep python\n</code></pre>"},{"location":"tools/benchmarks/#import-errors","title":"Import Errors","text":"<p>Environment Check</p> <p>Ensure correct Python environment.</p> verify_environment.sh<pre><code># Check Python version (3.11-3.13)\npython --version\n\n# Verify dependencies\npip list | grep -E \"pytest-benchmark|milvus|qdrant|chromadb\"\n</code></pre>"},{"location":"tools/benchmarks/#performance-issues","title":"Performance Issues","text":"<p>Optimization Steps</p> <p>For slow benchmarks, try these solutions:</p> <ol> <li>Reduce Dataset Sizes - Modify test parameters</li> <li>Use Faster Storage - Switch to SSD</li> <li>Increase Docker Resources - Allocate more CPU/memory</li> <li>Close Applications - Free up system resources</li> </ol>"},{"location":"tools/benchmarks/#future-enhancements","title":"Future Enhancements","text":"<p>Planned Features</p> <p>Upcoming benchmark additions:</p> <ul> <li> <p> Hybrid Search</p> <p>Benchmark hybrid search performance</p> </li> <li> <p> Concurrent Operations</p> <p>Multi-threaded operation tests</p> </li> <li> <p> Memory Profiling</p> <p>Track memory usage patterns</p> </li> <li> <p> Network Simulation</p> <p>Test with latency/packet loss</p> </li> <li> <p> Scaling Tests</p> <p>Multi-node performance</p> </li> <li> <p> Cost Analysis</p> <p>Per-operation cost tracking</p> </li> </ul>"},{"location":"tools/benchmarks/#contributing","title":"Contributing","text":"<p>Add New Benchmarks</p> <p>Follow these steps to contribute:</p> <ol> <li>Create Test File - Add to <code>tests/benchmarks/</code></li> <li>Use Decorator - <code>@pytest.mark.benchmark(group=\"category\")</code></li> <li>Follow Patterns - Match existing test structure</li> <li>Update Documentation - Add to this guide</li> <li>Submit PR - Follow Contributing Guide</li> </ol>"},{"location":"tools/benchmarks/#references","title":"References","text":"<ul> <li> <p> pytest-benchmark</p> <p>Documentation</p> </li> <li> <p> Milvus</p> <p>Benchmark Guide</p> </li> <li> <p> Qdrant</p> <p>Performance Benchmarks</p> </li> <li> <p> ChromaDB</p> <p>Performance Guide</p> </li> </ul>"},{"location":"tools/migration/","title":"Migration","text":""},{"location":"tools/migration/#vector-store-migration","title":"Vector Store Migration","text":"<p>Transfer vector data seamlessly between different vector store implementations with validation, progress tracking, and intelligent error handling.</p>"},{"location":"tools/migration/#overview","title":"Overview","text":"<p>Migration Capabilities</p> <p>Professional-grade tools for moving data between vector stores.</p> <p>What You Can Do:</p> <ul> <li> <p> Migrate Data</p> <p>Transfer between Milvus, Qdrant, and ChromaDB</p> </li> <li> <p> Validate Integrity</p> <p>Automatic data integrity verification</p> </li> <li> <p> Track Progress</p> <p>Real-time progress with ETA calculation</p> </li> <li> <p> Handle Errors</p> <p>Retry logic with exponential backoff</p> </li> <li> <p> Estimate Resources</p> <p>Predict migration time and requirements</p> </li> </ul>"},{"location":"tools/migration/#quick-start","title":"Quick Start","text":""},{"location":"tools/migration/#basic-migration","title":"Basic Migration","text":"simple_migration.py<pre><code>from rag_toolkit.migration import VectorStoreMigrator\nfrom rag_toolkit.infra.vectorstores import (\n    get_chromadb_service,\n    get_qdrant_service,\n)\n\n# Initialize source and target stores\nsource = get_chromadb_service(\n    host=\"localhost\",\n    port=8000,\n)\n\ntarget = get_qdrant_service(\n    host=\"localhost\",\n    port=6333,\n)\n\n# Create migrator\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    validate=True,  # Enable validation\n)\n\n# Run migration\nresult = migrator.migrate(\n    source_collection=\"my_documents\",\n    target_collection=\"my_documents\",\n    batch_size=1000,\n)\n\n# Check results\nprint(f\"Success: {result.success}\")\nprint(f\"Migrated: {result.vectors_migrated}\")\nprint(f\"Failed: {result.vectors_failed}\")\nprint(f\"Duration: {result.duration_seconds}s\")\nprint(f\"Success Rate: {result.success_rate}%\")\n</code></pre> <p>Migration Complete</p> Text Only<pre><code>Success: True\nMigrated: 125,430 vectors\nFailed: 0\nDuration: 127.5s\nSuccess Rate: 100.0%\n</code></pre>"},{"location":"tools/migration/#migration-with-progress-tracking","title":"Migration with Progress Tracking","text":"<p>Real-Time Progress Monitoring</p> <p>Track migration progress with live updates and ETA.</p> progress_tracking.py<pre><code># Define progress callback\ndef on_progress(progress):\n    print(\n        f\"Progress: {progress.percentage:.1f}% \"\n        f\"({progress.vectors_processed}/{progress.total_vectors}) \"\n        f\"- ETA: {progress.eta_seconds:.0f}s\"\n    )\n\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    on_progress=on_progress,  # Add callback\n)\n\nresult = migrator.migrate(\n    source_collection=\"my_docs\",\n    batch_size=500,\n)\n</code></pre> <p>Progress Output</p> Text Only<pre><code>Progress: 12.5% (12,500/100,000) - ETA: 245s\nProgress: 25.0% (25,000/100,000) - ETA: 210s\nProgress: 37.5% (37,500/100,000) - ETA: 175s\nProgress: 50.0% (50,000/100,000) - ETA: 140s\n...\nProgress: 100.0% (100,000/100,000) - ETA: 0s\n</code></pre>"},{"location":"tools/migration/#features","title":"Features","text":""},{"location":"tools/migration/#estimation","title":"Estimation","text":"<p>Plan Before You Migrate</p> <p>Estimate time and resources before starting a large migration.</p> estimate_migration.py<pre><code># Get migration estimate\nestimate = migrator.estimate(\n    collection_name=\"large_collection\",\n    batch_size=1000,\n)\n\nprint(f\"Total vectors: {estimate.total_vectors:,}\")\nprint(f\"Estimated duration: {estimate.estimated_duration_seconds:.1f}s\")\nprint(f\"Estimated batches: {estimate.estimated_batches}\")\nprint(f\"Throughput: {estimate.vectors_per_second:.0f} vectors/sec\")\n</code></pre> <p>Estimate Output</p> Text Only<pre><code>Total vectors: 1,250,000\nEstimated duration: 625.0s (~10.4 minutes)\nEstimated batches: 1,250\nThroughput: 2,000 vectors/sec\n</code></pre>"},{"location":"tools/migration/#filtered-migration","title":"Filtered Migration","text":"<p>Selective Migration</p> <p>Migrate only vectors matching specific metadata criteria.</p> By Status &amp; YearComplex CriteriaUse Cases filter_by_status.py<pre><code># Migrate only published articles from 2025\nresult = migrator.migrate(\n    source_collection=\"articles\",\n    target_collection=\"articles_published_2025\",\n    filter={\n        \"status\": \"published\",\n        \"year\": 2025,\n    },\n)\n\nprint(f\"Migrated {result.vectors_migrated} filtered vectors\")\n</code></pre> complex_filter.py<pre><code># Multiple filter criteria\nresult = migrator.migrate(\n    source_collection=\"documents\",\n    target_collection=\"filtered_docs\",\n    filter={\n        \"document_type\": \"report\",\n        \"department\": \"engineering\",\n        \"confidential\": False,\n        \"version\": \"2.0\",\n    },\n)\n</code></pre> <ul> <li> <p> Migrate Subsets</p> <p>Migrate only production-ready documents</p> </li> <li> <p> Filtered Backups</p> <p>Create backups of non-sensitive data only</p> </li> <li> <p> Split Collections</p> <p>Organize by customer, region, or date</p> </li> <li> <p> Test Samples</p> <p>Test migrations with representative data</p> </li> </ul>"},{"location":"tools/migration/#dry-run-mode","title":"Dry-Run Mode","text":"<p>Test Before Executing</p> <p>Simulate migrations without writing to target store.</p> dry_run.py<pre><code># Test migration before executing\nresult = migrator.migrate(\n    source_collection=\"production_data\",\n    target_collection=\"production_backup\",\n    dry_run=True,  # No writes to target\n)\n\nprint(f\"Would migrate {result.vectors_migrated} vectors\")\nprint(f\"Estimated duration: {result.duration_seconds}s\")\nprint(f\"No data written to target store\")\n</code></pre> With FiltersBenefits dry_run_with_filter.py<pre><code># Test filtered migration\nresult = migrator.migrate(\n    source_collection=\"documents\",\n    filter={\"status\": \"active\"},\n    dry_run=True,\n)\n\nif result.vectors_migrated &gt; 1000000:\n    print(\"\u26a0 Warning: Large migration, consider splitting\")\nelse:\n    print(\"\u2713 Safe to proceed\")\n\n    # Execute real migration\n    real_result = migrator.migrate(\n        source_collection=\"documents\",\n        filter={\"status\": \"active\"},\n        dry_run=False,\n    )\n</code></pre> <ul> <li> <p> Zero Risk</p> <p>Test without modifying target</p> </li> <li> <p> Validation</p> <p>Verify filters and counts</p> </li> <li> <p> Performance</p> <p>Measure actual throughput</p> </li> <li> <p> Planning</p> <p>Calculate storage &amp; windows</p> </li> </ul> <p>Dry-Run Behavior</p> <p>Dry-run mode skips target writes and validation but still reads from source to provide accurate counts.</p>"},{"location":"tools/migration/#retry-logic","title":"Retry Logic","text":"<p>Automatic Error Recovery</p> <p>Built-in exponential backoff for transient failures.</p> retry_config.py<pre><code># Configure retry behavior\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    max_retries=5,         # Up to 5 retries (default: 3)\n    retry_delay=2.0,       # Initial delay 2s (default: 1.0s)\n    retry_backoff=2.0,     # Double delay each retry (default: 2.0)\n)\n\nresult = migrator.migrate(\n    source_collection=\"unreliable_network_migration\",\n)\n</code></pre> <p>Automatic Retry On</p> <ul> <li>\u26a1 Network timeouts</li> <li>\ud83d\udd0c Temporary connection failures</li> <li>\ud83d\udea6 Rate limit errors (429)</li> <li>\u23f1\ufe0f Transient store unavailability</li> </ul> Retry TimingConservative (Cloud)Aggressive (Local)Error Handling retry_timing.py<pre><code># Exponential backoff example\nAttempt 1: Immediate\nAttempt 2: Wait 2.0s\nAttempt 3: Wait 4.0s (2.0 * 2.0)\nAttempt 4: Wait 8.0s (4.0 * 2.0)\nAttempt 5: Wait 16.0s (8.0 * 2.0)\n</code></pre> conservative_retry.py<pre><code># Longer delays for rate-limited APIs\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    max_retries=10,\n    retry_delay=5.0,\n    retry_backoff=1.5,  # Slower backoff\n)\n</code></pre> aggressive_retry.py<pre><code># Fast retry for local migrations\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    max_retries=3,\n    retry_delay=0.5,\n    retry_backoff=2.0,\n)\n</code></pre> error_handling.py<pre><code>from rag_toolkit.migration import MigrationError\n\ntry:\n    result = migrator.migrate(source_collection=\"docs\")\nexcept MigrationError as e:\n    # All retries exhausted\n    print(f\"Failed after {migrator.max_retries} retries: {e}\")\n</code></pre>"},{"location":"tools/migration/#validation","title":"Validation","text":"<p>Data Integrity</p> <p>Automatic validation ensures successful migration.</p> validation.py<pre><code># Validation enabled by default\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    validate=True,  # Enable validation\n)\n\nresult = migrator.migrate(source_collection=\"docs\")\n\n# Check validation status\nif result.metadata.get(\"validated\"):\n    print(\"\u2713 Migration validated successfully\")\nelse:\n    print(\"\u26a0 Validation warnings:\", result.errors)\n</code></pre>"},{"location":"tools/migration/#error-handling","title":"Error Handling","text":"<p>Resilient Migration</p> <p>Migration continues even if individual batches fail.</p> error_handling_batch.py<pre><code>result = migrator.migrate(\n    source_collection=\"docs\",\n    batch_size=1000,\n)\n\nif not result.success:\n    print(f\"Completed with {result.vectors_failed} failures\")\n    for error in result.errors:\n        print(f\"  - {error}\")\n</code></pre>"},{"location":"tools/migration/#migration-models","title":"Migration Models","text":""},{"location":"tools/migration/#migrationresult","title":"MigrationResult","text":"<p>Complete Migration Information</p> <p>Detailed results from a migration operation.</p> migration_result.py<pre><code>@dataclass\nclass MigrationResult:\n    success: bool                    # Overall success\n    vectors_migrated: int            # Successfully migrated\n    vectors_failed: int              # Failed vectors\n    duration_seconds: float          # Total time\n    source_collection: str           # Source name\n    target_collection: str           # Target name\n    started_at: datetime             # Start timestamp\n    completed_at: datetime           # End timestamp\n    errors: List[str]                # Error messages\n    metadata: Dict[str, Any]         # Additional info\n\n    @property\n    def total_vectors(self) -&gt; int:\n        \"\"\"Total vectors processed\"\"\"\n\n    @property\n    def success_rate(self) -&gt; float:\n        \"\"\"Percentage successfully migrated\"\"\"\n</code></pre>"},{"location":"tools/migration/#migrationprogress","title":"MigrationProgress","text":"<p>Real-Time Progress</p> <p>Live progress information during migration.</p> migration_progress.py<pre><code>@dataclass\nclass MigrationProgress:\n    vectors_processed: int           # Processed so far\n    total_vectors: int               # Total to migrate\n    current_batch: int               # Current batch number\n    total_batches: int               # Total batches\n    elapsed_seconds: float           # Time elapsed\n    errors: int                      # Errors encountered\n\n    @property\n    def percentage(self) -&gt; float:\n        \"\"\"Completion percentage\"\"\"\n\n    @property\n    def eta_seconds(self) -&gt; float:\n        \"\"\"Estimated time remaining\"\"\"\n</code></pre>"},{"location":"tools/migration/#migrationestimate","title":"MigrationEstimate","text":"<p>Pre-Migration Planning</p> <p>Estimate migration requirements before execution.</p> migration_estimate.py<pre><code>@dataclass\nclass MigrationEstimate:\n    total_vectors: int               # Vectors to migrate\n    estimated_duration_seconds: float # Estimated time\n    estimated_batches: int           # Number of batches\n    source_dimension: int            # Vector dimension\n    target_dimension: Optional[int]  # If different\n    compatible: bool                 # Schema compatible\n    warnings: List[str]              # Potential issues\n\n    @property\n    def vectors_per_second(self) -&gt; float:\n        \"\"\"Estimated throughput\"\"\"\n</code></pre>"},{"location":"tools/migration/#best-practices","title":"Best Practices","text":""},{"location":"tools/migration/#batch-sizing","title":"Batch Sizing","text":"<p>Optimal Batch Sizes</p> <p>Choose batch size based on your dataset size and constraints.</p> <ul> <li> <p> Memory</p> <p>Larger batches use more memory</p> </li> <li> <p> Network</p> <p>Larger batches reduce overhead</p> </li> <li> <p> Store Limits</p> <p>Some stores have payload limits (Qdrant: 33MB)</p> </li> </ul> Small DatasetsMedium DatasetsLarge Datasets small_dataset.py<pre><code># &lt; 10K vectors\nbatch_size = 500\n</code></pre> medium_dataset.py<pre><code># 10K - 100K vectors\nbatch_size = 1000\n</code></pre> large_dataset.py<pre><code># &gt; 100K vectors\nbatch_size = 2000\n</code></pre>"},{"location":"tools/migration/#progress-monitoring","title":"Progress Monitoring","text":"<p>Robust Progress Tracking</p> <p>Implement detailed logging for large migrations.</p> progress_monitoring.py<pre><code>import logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\ndef detailed_progress(progress):\n    logger.info(\n        f\"[{datetime.now().isoformat()}] \"\n        f\"Batch {progress.current_batch}/{progress.total_batches} | \"\n        f\"{progress.vectors_processed:,}/{progress.total_vectors:,} vectors | \"\n        f\"{progress.percentage:.1f}% | \"\n        f\"ETA: {progress.eta_seconds:.0f}s | \"\n        f\"Errors: {progress.errors}\"\n    )\n\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    on_progress=detailed_progress,\n)\n</code></pre>"},{"location":"tools/migration/#error-recovery","title":"Error Recovery","text":"<p>Graceful Failure Handling</p> <p>Handle migration failures with proper logging and recovery.</p> error_recovery.py<pre><code>result = migrator.migrate(\n    source_collection=\"production_data\",\n    target_collection=\"production_data_v2\",\n    batch_size=1000,\n)\n\nif not result.success:\n    # Log errors\n    for error in result.errors:\n        logger.error(f\"Migration error: {error}\")\n\n    # Decide on recovery strategy\n    if result.success_rate &gt; 95:\n        logger.info(\"Migration mostly successful, proceeding\")\n    else:\n        logger.error(\"Migration failed, rolling back\")\n        # Implement rollback logic\n</code></pre>"},{"location":"tools/migration/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"tools/migration/#pre-production-validation","title":"Pre-Production Validation","text":"<p>Production-Grade Migration</p> <p>Full pipeline with dry-run, validation, and verification.</p> production_migration.py<pre><code>import logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\ndef safe_production_migration(\n    source, target, collection_name, filter_criteria=None\n):\n    \"\"\"Production-grade migration with validation and dry-run.\"\"\"\n\n    migrator = VectorStoreMigrator(\n        source=source,\n        target=target,\n        max_retries=5,\n        retry_delay=2.0,\n    )\n\n    # Step 1: Dry-run to estimate\n    logger.info(\"Running dry-run estimation...\")\n    dry_result = migrator.migrate(\n        source_collection=collection_name,\n        filter=filter_criteria,\n        dry_run=True,\n    )\n\n    logger.info(\n        f\"Dry-run: {dry_result.vectors_migrated} vectors, \"\n        f\"{dry_result.duration_seconds:.1f}s estimated\"\n    )\n\n    # Step 2: Validate counts\n    if dry_result.vectors_migrated == 0:\n        raise ValueError(\"No vectors to migrate, check filter criteria\")\n\n    if dry_result.vectors_migrated &gt; 10_000_000:\n        logger.warning(\"Large migration detected, consider incremental approach\")\n\n    # Step 3: Execute real migration\n    logger.info(\"Starting real migration...\")\n    result = migrator.migrate(\n        source_collection=collection_name,\n        target_collection=f\"{collection_name}_prod_{datetime.now().strftime('%Y%m%d')}\",\n        filter=filter_criteria,\n        validate=True,\n    )\n\n    # Step 4: Verify success\n    if result.success_rate &lt; 99.9:\n        raise MigrationError(\n            f\"Migration success rate too low: {result.success_rate}%\"\n        )\n\n    logger.info(\n        f\"\u2713 Migration successful: {result.vectors_migrated} vectors, \"\n        f\"{result.success_rate:.2f}% success rate\"\n    )\n\n    return result\n\n# Usage\nresult = safe_production_migration(\n    source=dev_store,\n    target=prod_store,\n    collection_name=\"user_documents\",\n    filter_criteria={\"verified\": True, \"active\": True},\n)\n</code></pre>"},{"location":"tools/migration/#selective-migration","title":"Selective Migration","text":"<p>Multi-Target Strategy</p> <p>Migrate different data subsets to optimized targets.</p> Fast Store <p>```python title=\"fast_store.py\" linenums=\"1\" hl_lines=\"1-4 6-10\"</p> <p>fast_migrator = VectorStoreMigrator(     source=chromadb_source,     target=qdrant_fast, )</p> <p>high_priority_result = fast_migrator.migrate(     source_collection=\"documents\",     target_collection=\"documents_priority\",     filter={\"priority\": \"high\", \"status\": \"active\"}, )</p>"},{"location":"tools/migration/#high-priority-to-fast-store","title":"High-priority to fast store","text":""},{"location":"tools/migration/#migrate-archived-documents-to-cold-storage","title":"Migrate archived documents to cold storage","text":"<p>archive_migrator = VectorStoreMigrator(     source=chromadb_source,     target=s3_backed_store, )</p> <p>archive_result = archive_migrator.migrate(     source_collection=\"documents\",     target_collection=\"documents_archive\",     filter={\"status\": \"archived\", \"year\": {\"$lt\": 2024}}, )</p> <p>print(f\"Fast store: {high_priority_result.vectors_migrated} vectors\") print(f\"Archive: {archive_result.vectors_migrated} vectors\") </p>Text Only<pre><code>### Multi-Environment Deployment\n\nDeploy validated data across environments:\n\n```python\ndef deploy_to_environment(env_name: str, target_store, filter_override=None):\n    \"\"\"Deploy vectors to specific environment with environment-specific filters.\"\"\"\n\n    base_filter = {\"validated\": True, \"status\": \"active\"}\n    if filter_override:\n        base_filter.update(filter_override)\n\n    migrator = VectorStoreMigrator(\n        source=staging_store,\n        target=target_store,\n        max_retries=5,\n    )\n\n    # Dry-run first\n    dry_result = migrator.migrate(\n        source_collection=\"product_embeddings\",\n        filter=base_filter,\n        dry_run=True,\n    )\n\n    print(f\"{env_name} dry-run: {dry_result.vectors_migrated} vectors\")\n\n    # Require approval for production\n    if env_name == \"production\":\n        approval = input(f\"Deploy {dry_result.vectors_migrated} vectors to prod? [y/N]: \")\n        if approval.lower() != 'y':\n            print(\"Deployment cancelled\")\n            return None\n\n    # Execute migration\n    result = migrator.migrate(\n        source_collection=\"product_embeddings\",\n        target_collection=f\"product_embeddings_{env_name}\",\n        filter=base_filter,\n        validate=True,\n    )\n\n    return result\n\n# Deploy pipeline\ndev_result = deploy_to_environment(\"dev\", dev_store)\nstaging_result = deploy_to_environment(\"staging\", staging_store)\nprod_result = deploy_to_environment(\"production\", prod_store)\n</code></pre><p></p>"},{"location":"tools/migration/#common-use-cases","title":"Common Use Cases","text":""},{"location":"tools/migration/#development-to-production","title":"Development to Production","text":"<p>ChromaDB to Qdrant</p> <p>Migrate from local dev to production cloud.</p> dev_to_prod.py<pre><code># Development setup\ndev_store = get_chromadb_service(host=\"localhost\")\n\n# Production setup\nprod_store = get_qdrant_service(\n    host=\"production.qdrant.com\",\n    port=6333,\n    api_key=os.getenv(\"QDRANT_API_KEY\"),\n)\n\nmigrator = VectorStoreMigrator(\n    source=dev_store,\n    target=prod_store,\n    validate=True,\n)\n\nresult = migrator.migrate(\n    source_collection=\"dev_documents\",\n    target_collection=\"prod_documents\",\n    batch_size=1000,\n)\n</code></pre>"},{"location":"tools/migration/#store-comparison","title":"Store Comparison","text":"<p>Migrate data to test different stores:</p> Python<pre><code>source = get_chromadb_service()\n\nfor target_name, target_store in [\n    (\"Milvus\", get_milvus_service()),\n    (\"Qdrant\", get_qdrant_service()),\n]:\n    migrator = VectorStoreMigrator(source=source, target=target_store)\n\n    print(f\"\\nMigrating to {target_name}...\")\n    result = migrator.migrate(\n        source_collection=\"test_data\",\n        target_collection=\"test_data\",\n    )\n\n    print(f\"  Duration: {result.duration_seconds:.2f}s\")\n    print(f\"  Success rate: {result.success_rate:.1f}%\")\n</code></pre>"},{"location":"tools/migration/#backup-and-restore","title":"Backup and Restore","text":"<p>Create backups of vector data:</p> Python<pre><code># Backup from Qdrant to ChromaDB\nbackup_migrator = VectorStoreMigrator(\n    source=qdrant_service,\n    target=chromadb_backup,\n)\n\nbackup_result = backup_migrator.migrate(\n    source_collection=\"critical_data\",\n    target_collection=f\"backup_{datetime.now().isoformat()}\",\n)\n\n# Later: Restore from backup\nrestore_migrator = VectorStoreMigrator(\n    source=chromadb_backup,\n    target=qdrant_service,\n)\n\nrestore_result = restore_migrator.migrate(\n    source_collection=\"backup_2025_12_27\",\n    target_collection=\"critical_data_restored\",\n)\n</code></pre>"},{"location":"tools/migration/#exceptions","title":"Exceptions","text":""},{"location":"tools/migration/#migrationerror","title":"MigrationError","text":"<p>Base exception for all migration errors:</p> Python<pre><code>from rag_toolkit.migration import MigrationError\n\ntry:\n    result = migrator.migrate(source_collection=\"docs\")\nexcept MigrationError as e:\n    print(f\"Migration failed: {e}\")\n</code></pre>"},{"location":"tools/migration/#collectionnotfounderror","title":"CollectionNotFoundError","text":"<p>Raised when source collection doesn't exist:</p> Python<pre><code>from rag_toolkit.migration import CollectionNotFoundError\n\ntry:\n    estimate = migrator.estimate(collection_name=\"missing\")\nexcept CollectionNotFoundError:\n    print(\"Collection doesn't exist in source store\")\n</code></pre>"},{"location":"tools/migration/#validationerror","title":"ValidationError","text":"<p>Raised when post-migration validation fails:</p> Python<pre><code>from rag_toolkit.migration import ValidationError\n\ntry:\n    result = migrator.migrate(\n        source_collection=\"docs\",\n        validate=True,\n    )\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre>"},{"location":"tools/migration/#performance-tips","title":"Performance Tips","text":""},{"location":"tools/migration/#optimize-batch-size","title":"Optimize Batch Size","text":"<p>Test different batch sizes to find optimal throughput:</p> Python<pre><code>for batch_size in [500, 1000, 2000, 5000]:\n    migrator = VectorStoreMigrator(source=source, target=target)\n\n    result = migrator.migrate(\n        source_collection=\"test\",\n        target_collection=f\"test_{batch_size}\",\n        batch_size=batch_size,\n    )\n\n    throughput = result.vectors_migrated / result.duration_seconds\n    print(f\"Batch {batch_size}: {throughput:.0f} vectors/sec\")\n</code></pre>"},{"location":"tools/migration/#parallel-migrations","title":"Parallel Migrations","text":"<p>For multiple collections, consider parallel processing:</p> Python<pre><code>from concurrent.futures import ThreadPoolExecutor\n\ncollections = [\"docs\", \"images\", \"audio\"]\n\ndef migrate_collection(collection_name):\n    migrator = VectorStoreMigrator(source=source, target=target)\n    return migrator.migrate(source_collection=collection_name)\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    results = executor.map(migrate_collection, collections)\n\nfor result in results:\n    print(f\"{result.source_collection}: {result.success_rate}% success\")\n</code></pre>"},{"location":"tools/migration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tools/migration/#slow-migration","title":"Slow Migration","text":"<p>If migration is slow:</p> <ol> <li>Increase batch size (if memory allows)</li> <li>Check network latency between stores</li> <li>Monitor resource usage (CPU, memory, disk)</li> <li>Verify store performance (check individual store metrics)</li> </ol>"},{"location":"tools/migration/#validation-failures","title":"Validation Failures","text":"<p>If validation consistently fails:</p> <ol> <li>Check store connectivity</li> <li>Verify write permissions</li> <li>Inspect error messages in <code>result.errors</code></li> <li>Try smaller batch size</li> </ol>"},{"location":"tools/migration/#memory-issues","title":"Memory Issues","text":"<p>If running out of memory:</p> <ol> <li>Reduce batch size</li> <li>Disable progress tracking (if not needed)</li> <li>Close other applications</li> <li>Consider streaming approaches for very large datasets</li> </ol>"},{"location":"tools/migration/#next-steps-roadmap","title":"Next Steps &amp; Roadmap","text":""},{"location":"tools/migration/#implemented-features-phase-1-2-priority-1","title":"Implemented Features (Phase 1 &amp; 2 Priority 1)","text":"<p>\u2705 Core Migration Engine - Batch processing with configurable sizes - Progress tracking with callbacks - Automatic validation - Comprehensive error handling - Migration estimation</p> <p>\u2705 Advanced Filtering &amp; Safety (Phase 2 Priority 1) - Metadata-based filtering for selective migration - Dry-run mode for zero-risk testing - Retry logic with exponential backoff - Production-grade error recovery</p>"},{"location":"tools/migration/#planned-enhancements","title":"Planned Enhancements","text":""},{"location":"tools/migration/#phase-2-priority-2-data-continuity-schema-evolution","title":"Phase 2 Priority 2: Data Continuity &amp; Schema Evolution","text":"<p>1. Incremental Migration</p> <p>Problem: Large migrations (millions of vectors) can take hours/days. If interrupted, must restart from scratch.</p> <p>Solution: Checkpoint-based incremental migration with resume capability.</p> Python<pre><code># Technical implementation:\n# - Maintain .migration_checkpoint.json with migrated IDs\n# - Support resume=True to skip already-migrated vectors\n# - Implement conflict resolution strategies (skip, overwrite, fail)\n# - Enable continuous sync workflows\n\nmigrator.migrate(\n    source_collection=\"large_dataset\",\n    incremental=True,\n    resume=True,  # Resume from last checkpoint\n    checkpoint_file=\".migration_state.json\",\n)\n</code></pre> <p>Engineering considerations: - Checkpoint persistence: Local file, Redis, or database? - Checkpoint granularity: Per-batch, per-vector, or hybrid? - Distributed migration: How to handle concurrent migrators? - Checkpoint cleanup: TTL, manual deletion, or auto-cleanup? - Error recovery: Partial batch failures in incremental mode</p> <p>2. Schema Mapping &amp; Transformation</p> <p>Problem: Different vector stores use different schemas. Migration between Qdrant, Pinecone, and Weaviate requires manual field mapping.</p> <p>Solution: Declarative schema mapping with transformation functions.</p> Python<pre><code># Technical implementation:\n# - Field name mapping (dict-based)\n# - Type conversion pipeline (callable transformers)\n# - Default value injection\n# - Field exclusion/inclusion lists\n# - Pre/post-migration hooks\n\nschema_mapping = {\n    \"field_mapping\": {\"doc_text\": \"text\", \"doc_id\": \"id\"},\n    \"transforms\": {\n        \"timestamp\": lambda x: int(x.timestamp()),\n        \"tags\": lambda x: \",\".join(x) if isinstance(x, list) else x,\n    },\n    \"exclude_fields\": [\"internal_cache\", \"temp_data\"],\n    \"defaults\": {\"version\": \"2.0\", \"migrated_at\": datetime.now()},\n}\n\nmigrator.migrate(\n    source_collection=\"docs\",\n    schema_mapping=schema_mapping,\n)\n</code></pre> <p>Engineering considerations: - Transform error handling: Skip vector, use default, or fail fast? - Type safety: Runtime validation vs. static type checking? - Performance: Transform overhead on large datasets (vectorization?) - Bidirectional mapping: Support reverse migrations? - Schema versioning: Track mapping versions for rollback?</p>"},{"location":"tools/migration/#phase-3-enterprise-features","title":"Phase 3: Enterprise Features","text":"<p>1. Command-Line Interface (CLI)</p> <p>Problem: Python API requires coding. DevOps teams need CLI for scripts/CI-CD.</p> <p>Solution: Rich CLI with YAML configuration support.</p> Bash<pre><code># Technical implementation:\n# - Click or Typer-based CLI\n# - YAML/JSON config file support\n# - Environment variable injection\n# - Progress bars (rich library)\n# - Exit codes for CI/CD integration\n\nrag-migrate \\\n  --source qdrant://localhost:6333/docs \\\n  --target pinecone://api-key@index-name \\\n  --filter 'status=active,year=2025' \\\n  --batch-size 1000 \\\n  --dry-run \\\n  --retry 5 \\\n  --config migration.yaml\n</code></pre> <p>Engineering considerations: - Connection string parsing: URL-based or explicit parameters? - Secret management: Env vars, keyring, or external vault? - Config file schema: YAML, TOML, JSON, or all three? - Output formats: JSON, YAML, table, or all three? - Logging: Structured logging (JSON) vs. human-readable?</p> <p>2. Parallel Migration (Multi-threaded/Multi-process)</p> <p>Problem: Single-threaded migration is slow for large datasets.</p> <p>Solution: Parallel batch processing with configurable workers.</p> Python<pre><code># Technical implementation:\n# - ThreadPoolExecutor for I/O-bound operations\n# - ProcessPoolExecutor for CPU-bound transforms\n# - Batch-level parallelism (not vector-level)\n# - Graceful degradation on errors\n# - Resource throttling\n\nmigrator.migrate(\n    source_collection=\"large_dataset\",\n    parallel=True,\n    max_workers=8,  # Auto-detect by default\n    worker_type=\"thread\",  # or \"process\"\n)\n</code></pre> <p>Engineering considerations: - GIL limitations: When to use threads vs. processes? - Memory management: Worker memory limits to prevent OOM? - Error propagation: How to handle worker failures? - Progress tracking: Aggregate progress from multiple workers? - Rate limiting: Coordinate workers to respect API limits? - Batch ordering: Preserve order or allow out-of-order completion?</p> <p>3. Metrics &amp; Observability</p> <p>Problem: Large migrations lack visibility. Need monitoring, alerting, and debugging.</p> <p>Solution: Prometheus metrics, OpenTelemetry tracing, and structured logging.</p> Python<pre><code># Technical implementation:\n# - Prometheus client for metrics export\n# - OpenTelemetry spans for distributed tracing\n# - Structured logging (JSON) with correlation IDs\n# - Pluggable exporters (Prometheus, Grafana, Datadog)\n\nfrom rag_toolkit.migration import MetricsExporter\n\nmigrator = VectorStoreMigrator(\n    source=source,\n    target=target,\n    metrics_exporter=MetricsExporter(\n        backend=\"prometheus\",\n        port=9090,\n    ),\n)\n\n# Metrics exposed:\n# - migration_vectors_total\n# - migration_duration_seconds\n# - migration_batch_size\n# - migration_errors_total\n# - migration_retry_attempts\n</code></pre> <p>Engineering considerations: - Metric cardinality: Avoid label explosion - Sampling: Full tracing vs. sampled for large migrations? - Overhead: Metrics/tracing impact on throughput? - Retention: How long to keep metrics/traces? - Alerting: Built-in alerts or external alertmanager?</p>"},{"location":"tools/migration/#phase-4-advanced-capabilities","title":"Phase 4: Advanced Capabilities","text":"<p>1. Schema Version Migration</p> <ul> <li>Migrate between different schema versions (v1 \u2192 v2)</li> <li>Automatic field deprecation handling</li> <li>Version compatibility checks</li> </ul> <p>2. Cross-Cloud Migration</p> <ul> <li>AWS \u2192 GCP \u2192 Azure vector store migrations</li> <li>Network optimization for inter-cloud transfers</li> <li>Cost estimation for data egress</li> </ul> <p>3. Zero-Downtime Migration</p> <ul> <li>Dual-write pattern during migration</li> <li>Automatic cutover with validation</li> <li>Rollback support</li> </ul> <p>4. Data Quality Validation</p> <ul> <li>Embedding drift detection</li> <li>Outlier detection in migrated vectors</li> <li>Semantic similarity preservation checks</li> </ul>"},{"location":"tools/migration/#contributing","title":"Contributing","text":"<p>Interested in implementing these features? See the Contributing Guide for guidelines.</p> <p>Priority feature requests: 1. Incremental migration (high demand) 2. Schema mapping (cross-store compatibility) 3. CLI tool (DevOps workflows) 4. Parallel migration (performance)</p>"},{"location":"tools/migration/#architecture-decisions","title":"Architecture Decisions","text":"<p>Retry logic choice: Exponential backoff chosen over fixed delay for: - Better handling of transient failures (network, rate limits) - Reduced load on struggling services - Industry standard pattern (AWS SDK, Google APIs)</p> <p>Dry-run implementation: Read-only approach (no mock writes) because: - True performance measurement - Accurate filter validation - Simpler implementation (no mocking layer) - Predictable behavior</p> <p>Filter design: Metadata-based (not content-based) because: - Performance: No vector similarity computation needed - Clarity: Explicit metadata fields are self-documenting - Scalability: Metadata filtering supported by all stores - Future: Can extend to vector-based filtering later</p> <p>Checkpoint format (planned): JSON file (not database) because: - Zero dependencies - Human-readable for debugging - Easy to version control - Simple to backup/restore - Trade-off: Not suitable for distributed scenarios (Phase 4)</p>"},{"location":"tools/migration/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see the API Reference.</p>"}]}